intent_id,intent,code
0,import $libname,import $libname
1,import all libraries,"import pandas as pd
import numpy as np
import os
import plotly.express as px
import matplotlib.pyplot as plt
pd.options.plotting.backend = 'plotly'"
2,load $fname,$varname = pd.read_csv('$fname')
3,show $cardinal rows from $varname,$varname.head($cardinal)
4,plot histogram of $colname column in $varname,$varname.plot.hist(x='$colname')
5,get correlation matrix of $varname,$varname.corr()
6,print $varname shape,$varname.shape
7,barplot $colname and $colname columns of $varname,"px.bar(x='$colname', y='$colname', data_frame=$varname, title='CustomTitle', labels={'$colname':'$colname', '$colname':'$colname'})"
8,piechart of $colname column in $varname grouped by $colname column,"tmp = $varname['$colname'].value_counts(dropna=False)
px.pie(tmp,values=tmp.values,names=tmp.index,title='CustomTitle')"
9,install $libname,!pip install $libname
10,list columns of $varname,$varname.columns
11,describe $varname,$varname.describe()
12,group $varname by $colname and get $function of $colname,# Not supported in the current release :(
13,display a line plot showing $colname vs $colname in $varname,"$varname.plot.line(x='$colname', y='$colname', color=None, title='CustomTitle', labels={'$colname':'$colname', '$colname':'$colname'})"
14,show a scatter plot of $colname over $colname in $varname,"$varname.plot.scatter(x='$colname', y='$colname', color=None, size=None, title='CustomTitle', labels={'$colname':'$colname', '$colname':'$colname'})"
15,show a heatmap with $colname on x axis and $colname on y axis in $varname,"$varname.plot(kind='density_heatmap', x='$colname', y='$colname', title='CustomTitle', labels={'$colname':'$colname', '$colname':'$colname'})"
16,list all files in current directory,!ls .
17,switch to dark theme,"import plotly.io as pio
pio.templates.default = 'plotly_dark'"
10000,FTP - Send file,"from naas_drivers import ftp
path = ""/path/to/file/in/ftp""
dest_path = ""/path/to/file/in/ftp""
user = ""my user""
passwd = ""my passwd""
ftp = ftp.connect(user, passwd)
ftp = naas_drivers.ftp.connect(user, passwd)
ftp.send(path, dest_path)"
10001,FTP - Get file,"from naas_drivers import ftp
path = ""/path/to/file/in/ftp""
user = ""my user""
passwd = ""my passwd""
ftp = ftp.connect(user, passwd)
ftp = ftp.connect(user, passwd)
ftp.get(path)"
10002,FTP - S Connect,"from naas_drivers import ftp
path = ""/path/to/file/in/ftp""
user = ""my user""
passwd = ""my passwd""
ftp = ftp.connect(user, passwd)
ftps = ftp.connect(user, passwd)
ftps.get(path)"
10003,FTP - Connect,"from naas_drivers import ftp
user = ""my user""
passwd = ""my passwd""
ftp_get = ftp.connect(user, passwd, port=990, secure=True)
ftp_get"
10004,Twitter - Get tweets stats from profile,"import os
import re
import pandas as pd
#install developer snscrape package via command line
os.system(""pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git"")
#criteria for searching by username
username = ""JupyterNaas""
tweet_count = 500
#search by username using command line
os.system(""snscrape --jsonl --max-results {} twitter-search from:{} > user-tweets.json"".format(tweet_count, username))
# Reads the json generated from the CLI command above and creates a pandas dataframe
df = pd.read_json('user-tweets.json', lines=True, convert_dates=True, keep_default_dates=True)
df
#copy dataframe 
df1 = df.copy()

#keep only the columns needed
df1 = df1[['url','content','hashtags','date','likeCount','retweetCount']]

#convert columns to upper case to follow naas df convention
df1.columns = df1.columns.str.upper()

#convert time to ISO format to follow naas date convention
df1.DATE = pd.to_datetime(df1.DATE).dt.strftime(""%Y-%m-%d"")

#clean HASHTAGS column to provide searchable items in columns
df1.HASHTAGS = df1.HASHTAGS.fillna(""[]"")
df1.HASHTAGS = df1.apply(lambda row: "", "".join(list(row.HASHTAGS)) if row.HASHTAGS != '[]' else """", axis=1)

#display results
df1
df1.to_csv(""tweets_from_URL.csv"", index=False)"
10005,Twitter - Get user data,"import tweepy
import pandas as pd
# API Credentials
consumer_key = ""XXXXXXXXXXXXXXXXXX""
consumer_secret = ""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""
user_list = [""JupyterNaas"", ""Spotify"", ""ProjectJupyter""]
try:
    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)
    api = tweepy.API(auth)
except BaseException as e:
    print(f""Authentication has been failed due to -{str(e)}"")
def getUserInfo(user_id):
    
    # Define a pandas dataframe to store the date:
    user_info_df = pd.DataFrame(columns = ['twitter_id', 'name', 'screen_name', 'description', 'tweet_count', 'friends_count',
                        'followers_count', 'favourites_count', 'verified', 'created_at']
                                )

    # Collect userinformation using get_user
    for user in user_id:
        info = api.get_user(user) # Get user information request
        
        twitter_id = info.id
        name = info.name
        screen_name = info.screen_name
        description = info.description
        tweet_count = info.statuses_count
        friends_count = info.friends_count
        followers_count = info.followers_count
        favourites_count = info.favourites_count
        verified = info.verified
        created_at = info.created_at
        
       
        user_info = [twitter_id, name, screen_name, description, tweet_count, friends_count,
                        followers_count, favourites_count, verified, created_at]
        
        user_info_df.loc[len(user_info_df)] = user_info
        
    
    return user_info_df
df = getUserInfo(user_list)"
10006,Twitter - Get posts stats,"!pip install --user tweepy
from naas_drivers import linkedin, hubspot
import pandas as pd
import numpy as np
import naas
from datetime import datetime, timedelta
import requests
import json
import tweepy
#Twitter cookies
bearer_token = 'AAAAAAAAAAAAAAAAAAAAAFGZZgEAXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'

client = tweepy.Client(bearer_token=bearer_token)
user_name = ""<YOUR_PREFERRED_USERNAME>""
users = client.get_users(usernames=[user_name])
user_id = users.data[0][""id""]

#set application type. This is used to distinguish data for application in master data model
APP_TYPE = ""Twitter""

BASE_URL = ""https://twitter.com/""
SEND_EMAIL_TO = ""<YOUR_EMAIL_ID>""
#Get user tweets
tweets = client.get_users_tweets(id=user_id)
tweetIdList = []
for tweet in tweets.data:
    tweetIdList.append(str(tweet[""id""]))
tweetIdList
def get_post_stats(tweetIdList):
    twitterDF = pd.DataFrame()
    for tweetId in tweetIdList:

        #print(""*********************************"")
        #print(""Tweet ID : "", tweetId)
        parent_tweet = client.get_tweet(id=tweetId, tweet_fields=[""author_id"",""created_at"",""entities"",""in_reply_to_user_id"",
                                                                        ""referenced_tweets,source,public_metrics""])
        parent_user_name = user_name
        parent_Tweet_Title = """"
        #parent_tweet_id = reply_tweet.data[""data""][""referenced_tweets""][0][""id""]
        parent_tweet_text = parent_tweet.data[""text""]
        parent_created_at = parent_tweet.data[""created_at""].strftime(""%d-%m-%Y %H:%M:%S"")
        parent_tweet_URL = BASE_URL+user_name+""/status/""+tweetId
        parent_public_metrics = parent_tweet.data[""data""][""public_metrics""]
        parent_retweet_count = parent_public_metrics[""retweet_count""]
        parent_reply_count = parent_public_metrics[""reply_count""]
        parent_like_count = parent_public_metrics[""like_count""]
        parent_quote_count = parent_public_metrics[""quote_count""]
        parent_view_count = 0 #currently twitter has no ways to find out who actually viewed your post hence kept value = 0 to map the columns
        parent_comments_mentions = []

        retweeted_by_users = client.get_retweeters(id=tweetId)
        
        retweeted_by_username = []
        if retweeted_by_users.data != None:
            for retweet_user in retweeted_by_users.data:
                retweeted_by_username.append(retweet_user[""username""])
        
        like_by_users = client.get_liking_users(id=tweetId)        
        liked_by_username = []
        if like_by_users.data != None:
            for like_user in like_by_users.data:
                liked_by_username.append(like_user[""username""])
        
        mentions = client.get_users_mentions(id=user_id)
        
        comments = []
        if mentions.data != None:
            for mention in mentions.data:
                child_tweet = client.get_tweet(id=mention[""id""], tweet_fields=[""author_id"", ""in_reply_to_user_id"",""referenced_tweets,source,public_metrics,text""])
                in_reply_to_tweet_id = child_tweet.data[""data""][""referenced_tweets""][0][""id""]

                if str(in_reply_to_tweet_id) == tweetId:
                    username = ""@""+user_name
                    temp = str(mention[""text""])
                    temp = temp.replace(username,"""")
                    comments.append(temp)
        
        data=[[tweetId,parent_created_at,parent_user_name,parent_Tweet_Title,parent_tweet_text,parent_tweet_URL,parent_view_count,parent_reply_count,parent_like_count,parent_retweet_count,APP_TYPE]]
        df = pd.DataFrame(data,columns=[""ACTIVITY_ID"",""PUBLISHED_DATE"",""AUTHOR_NAME"",""TITLE"",""TEXT"",""POST_URL"",""VIEWS"",""COMMENTS"",""LIKES"",""SHARES"",""APPLICATION_TYPE""
])
        twitterDF = twitterDF.append(df)

    return twitterDF
get_post_stats(tweetIdList)"
10007,Twitter - Schedule posts,"from datetime import datetime
import naas_drivers
import naas
spreadsheet_id = ""1rFzw8eeVNXyD5CEUjnpxVn_iA2hzabYKU6pRdmnZ3FQ""
df = naas_drivers.gsheet.connect(spreadsheet_id).get(
    sheet_name=""Sheet1""
)
df
now = datetime.now().replace(second=0, microsecond=0) #the Naas scheduler only allow minutes
now
# dd/mm/YY H:M:S
dt_string = now.strftime(""%d/%m/%Y %H:%M:%S"") # To mach date time format in google sheet
dt_string
#naas.scheduler.delete()
naas.scheduler.add(recurrence=""* * * * *"") # to run every minute
for i in range(len(df)):
    print(dt_string, df['BROADCAST_DATE'][i])
    if dt_string == df['BROADCAST_DATE'][i]:
        twitter_post = df['TEXT'][i]
        event = ""naas_demo""
        key = ""ke4AigvXI5-EABaowdLt4fju1aOUxeMxSXQoN8FVyA""
        data = { ""value1"": twitter_post }
        result = naas_drivers.ifttt.connect(key).send(event, data)   "
10008,Twitter - Get tweets from search,"import tweepy
import pandas as pd
consumer_key = ""XXXXXXXXXXXXXXXXXX""
consumer_secret = ""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""

try:
    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)
    api = tweepy.API(auth)
except BaseException as e:
    print(f""Authentication has been failed due to -{str(e)}"")
def getTweets(search_words, date_since, numTweets):
    
    # Define a pandas dataframe to store the date:
    tweets_df = pd.DataFrame(columns = ['username', 'desc', 'location', 'following',
                                        'followers', 'totaltweets', 'usercreated', 'tweetcreated',
                                        'retweetcount', 'text', 'hashtags']
                                )

    # Collect tweets using the Cursor object
    # .Cursor() returns an object that you can iterate or loop over to access the data collected.
    tweets = tweepy.Cursor(api.search, q=search_words, lang=""en"", since=date_since, tweet_mode='extended').items(numTweets)
    # Store tweets into a python list
    tweet_list = [tweet for tweet in tweets]
    for tweet in tweet_list:
        username = tweet.user.screen_name
        desc = tweet.user.description
        location = tweet.user.location
        following = tweet.user.friends_count
        followers = tweet.user.followers_count
        totaltweets = tweet.user.statuses_count
        usercreated = tweet.user.created_at
        tweetcreated = tweet.created_at
        retweetcount = tweet.retweet_count
        hashtags = tweet.entities['hashtags']
        try:
            text = tweet.retweeted_status.full_text
        except AttributeError:
            text = tweet.full_text
        
        tweet_data = [username, desc, location, following, followers, totaltweets,
                        usercreated, tweetcreated, retweetcount, text, hashtags]
        
        tweets_df.loc[len(tweets_df)] = tweet_data
        
    
    return tweets_df
search_words = ""#jupyterlab OR #python OR #naas OR #naasai""
date_since = ""2021-09-21""
numTweets = 50
df = getTweets(search_words, date_since, numTweets)"
10009,Twitter - Post text and image,"import naas
import naas_drivers
data = naas_drivers.yahoofinance.get('^FCHI',date_from=-200, moving_averages=[50,20])
chart = naas_drivers.plotly.stock(data)
chart.show()
name = ""chart.png""
naas_drivers.plotly.export(chart, name)
url = naas.assets.add(name, params={""inline"":True})
twitter_post = """"""üìàüöÄ Every day at 9AM PST / 6PM CET<br>
CAC40 index value with #MovingAverages 20 and 50
<br>
Don't miss an #opportunity to #invest<br>
<br>
PS : this post has been generated automatically with https://www.naas.ai/ üòé 
#automation #trading #data #analysis @Nasdaq
""""""
event = ""twitter-post""
key = ""ke4AigvXI5-EABaowdLt4fju1aOUxeMxSXQoN8FVyA""
data = { ""value1"" : twitter_post,  ""value2"" : url}
result = naas_drivers.ifttt.webhook(event, key, data)"
10010,Data.gouv.fr - R√©cup√©ration donn√©es l√©gales entreprise,"import requests
from pprint import pprint
# Input
SIREN = ""877941724""
def get_info(siren):
    req_url = f""https://entreprise.data.gouv.fr/api/sirene/v3/unites_legales/{siren}""
    res = requests.get(req_url)
    res.raise_for_status
    return res.json()
    
data = get_info(SIREN)
pprint(data)"
10011,Data.gouv.fr - COVID19 -  FR - Entr√©es et sorties par r√©gion pour 1 million d'hab.,"import requests
import pandas as pd
from datetime import datetime, timedelta
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
# URLs
BASE_URL_ENTREE = 'https://geodes.santepubliquefrance.fr/GC_indic.php?lang=fr&prodhash=3c0e7522&indic=incid_rea&dataset=covid_hospit_incid&view=map2&filters=jour='
BASE_URL_TOTAL = 'https://geodes.santepubliquefrance.fr/GC_indic.php?lang=fr&prodhash=3c0e7522&indic=rea&dataset=covid_hospit&view=map2&filters=sexe=0,jour='

# Liste des d√©partements
DEPARTMENTS = ['Ain', 'Aisne', 'Allier', 'Alpes-de-Haute-Provence', 'Hautes-Alpes', 'Alpes-Maritimes', 'Ard√®che', 'Ardennes', 'Ari√®ge', 'Aube', 'Aude', 'Aveyron',
               'Bouches-du-Rh√¥ne', 'Calvados', 'Cantal', 'Charente', 'Charente-Maritime', 'Cher', 'Corr√®ze', 'C√¥te-d\'Or', 'C√¥tes-d\'Armor', 'Creuse', 'Dordogne',
               'Doubs', 'Dr√¥me', 'Eure', 'Eure-et-Loir', 'Finist√®re', 'Corse-du-Sud', 'Haute-Corse', 'Gard','Haute-Garonne','Gers','Gironde','H√©rault',
               'Ille-et-Vilaine','Indre','Indre-et-Loire','Is√®re','Jura','Landes','Loir-et-Cher','Loire','Haute-Loire','Loire-Atlantique','Loiret','Lot',
               'Lot-et-Garonne','Loz√®re','Maine-et-Loire','Manche','Marne','Haute-Marne','Mayenne','Meurthe-et-Moselle','Meuse','Morbihan','Moselle','Ni√®vre',
               'Nord','Oise','Orne','Pas-de-Calais','Puy-de-D√¥me','Pyr√©n√©es-Atlantiques','Hautes-Pyr√©n√©es','Pyr√©n√©es-Orientales','Bas-Rhin','Haut-Rhin','Rh√¥ne',
               'Haute-Sa√¥ne','Sa√¥ne-et-Loire','Sarthe','Savoie','Haute-Savoie','Paris','Seine-Maritime','Seine-et-Marne','Yvelines','Deux-S√®vres','Somme','Tarn',
               'Tarn-et-Garonne','Var','Vaucluse','Vend√©e','Vienne','Haute-Vienne','Vosges','Yonne','Territoire de Belfort','Essonne','Hauts-de-Seine',
               'Seine-Saint-Denis','Val-de-Marne','Val-d\'Oise','Guadeloupe','Martinique','Guyane','La R√©union','Mayotte', 'France Enti√®re']

# Nombre de jours
LISSAGE_JOURS = 7

# Pour contenir les x derniers jours, x √©tant la variable ""LISSAGE_JOURS""
DATES = []

# Les indices contiennent x tableaux ordonn√©s en fonction de ""DATES"" contenant les donn√©es des d√©partements ordonn√© comme ""DEPARTEMENTS""
INDICES_TEMP_ENTREES = []
INDICES_TEMP_REANIMATION = []
INDICES_TEMP_COURANT = []
INDICES_ENTREES = []
INDICES_COURANT = []
INDICES_SORTIES = []
for i in range(LISSAGE_JOURS + 1):
  # G√©n√©ration des dates
  DAY = (datetime.today() - timedelta(days = (LISSAGE_JOURS - i))).isoformat().split(""T"")[0]
  DATES.append(DAY)

  # R√©cup√©ration des entr√©es en r√©animation
  URL = (BASE_URL_ENTREE + DAY)
  RESPONSE = requests.get(URL)
  JSON = RESPONSE.json()
  INDICES_TEMP_ENTREES.append(JSON['content']['distribution']['values'])
  TOTAL_ENTREES = 0
  for value in JSON['content']['distribution']['values']:
    TOTAL_ENTREES += value
    INDICES_ENTREES.append(value)
  INDICES_ENTREES.append(TOTAL_ENTREES)
  
  # R√©cup√©ration des personnes actuellement en r√©animation
  URL = (BASE_URL_TOTAL + DAY)
  RESPONSE = requests.get(URL)
  JSON = RESPONSE.json()
  INDICES_TEMP_REANIMATION.append(JSON['content']['distribution']['values'])
for i in range(1, LISSAGE_JOURS + 1):
  TOTAL_SORTIES = 0
  for j in range(len(DEPARTMENTS) - 1):
    INDICES_TEMP_COURANT.append([])
    INDICES_TEMP_COURANT[i-1].append(INDICES_TEMP_REANIMATION[i][j] - INDICES_TEMP_REANIMATION[i - 1][j])
    TOTAL_SORTIES += INDICES_TEMP_ENTREES[i][j] - INDICES_TEMP_COURANT[i - 1][j]
    INDICES_SORTIES.append(INDICES_TEMP_ENTREES[i][j] - INDICES_TEMP_COURANT[i - 1][j])
  INDICES_SORTIES.append(TOTAL_SORTIES)

INDICES_ENTREES = INDICES_ENTREES[len(DEPARTMENTS) : len(INDICES_ENTREES)]
DATES.pop(0)

for value in INDICES_TEMP_COURANT:
  TOTAL_COURANT = 0
  for v in value:
    TOTAL_COURANT += v
    INDICES_COURANT.append(v)
  INDICES_COURANT.append(TOTAL_COURANT)
iterables  = []
iterables.append(DATES)
iterables.append(DEPARTMENTS)
idx = pd.MultiIndex.from_product(iterables, names=['DATE', 'ZONE'])

datas = []
for i in range(len(iterables[1]) * len(iterables[0])) :
    datas.append(np.array([INDICES_ENTREES[i], INDICES_SORTIES[i],INDICES_COURANT[i], datetime.today()]))

df = pd.DataFrame(datas, index=idx, columns=['ENTREES', 'SORTIES', 'SOLDES', 'LAST UPDATE'])
df
# Pr√©pare la figure pour deux graphes
fig = make_subplots(rows=2, cols=1)
#fig = go.Figure()

# Application d'un filtre pour le graphe
# df = df.filter(like='France Enti√®re', axis=0)

# Cr√©ation des √©l√©ments du dropdown pour appliquer les filtres
buttons = []
buttons.append(dict(method='restyle', label=""Entire France"",
                      args=[{'y':[df.filter(like=""France Enti√®re"", axis=0).ENTREES, df.filter(like=""France Enti√®re"", axis=0).SORTIES, df.filter(like=""France Enti√®re"", axis=0).SOLDES]}]))
for i in range(len(DEPARTMENTS) - 1):
  dep = DEPARTMENTS[i]
  buttons.append(dict(method='restyle', label=dep,
                      args=[{'y':[df.filter(like=dep, axis=0).ENTREES, df.filter(like=dep, axis=0).SORTIES, df.filter(like=dep, axis=0).SOLDES]}]))

# Affichage des lignes dans le graphe
fig.add_trace(go.Scatter(x=df.filter(like='France Enti√®re', axis=0).index.get_level_values('DATE'), y=df.filter(like='France Enti√®re', axis=0).ENTREES, fill='tozeroy',name=""Admissions"",line=dict(width=0.5,color=""rgb(160,0,0)""),line_shape='spline'), row = 1, col = 1)
fig.add_trace(go.Scatter(x=df.filter(like='France Enti√®re', axis=0).index.get_level_values('DATE'), y=df.filter(like='France Enti√®re', axis=0).SORTIES, fill='tozeroy',name=""Releases"",line=dict(width=0.5,color=""rgb(0,160,0)""),line_shape='spline'), row = 1, col = 1)
fig.add_trace(go.Scatter(x=df.filter(like='France Enti√®re', axis=0).index.get_level_values('DATE'), y=df.filter(like='France Enti√®re', axis=0).SOLDES, fill='tozeroy',name=""Balance"",line=dict(width=0.5,color=""rgb(0,0,160)""),line_shape='spline'), row = 2, col = 1)

# Redimensionnement et couleur de fond du graphe
fig.update_layout(width=1400, height=400, plot_bgcolor='rgb(255,255,255)', title_text=""Admissions, releases and balance for COVID-19 reanimation services in France (last update : "" + str(df['LAST UPDATE'][0]) + "")"")

# Mise en place du dropdown
fig.update_layout(updatemenus=[dict(buttons=buttons, direction=""down"", pad={""r"": 1, ""t"": 1}, showactive=True, x=0.05, xanchor=""left"", y=1.22, yanchor=""top"")])

fig.update_layout(annotations=[dict(text=""Zone"", x=0, xref=""paper"", y=1.18, yref=""paper"", align=""left"", showarrow=False)])

# Affichage du graphe
fig.show()
#df"
10012,AlphaVantage - Get cashflow statement,"import requests
import pandas as pd
API_KEY = ""demo""
COMPANY = ""IBM""
response = requests.get(f'https://www.alphavantage.co/query?function=CASH_FLOW&symbol={COMPANY}&apikey={API_KEY}')
data = response.json()
df = pd.DataFrame(data.get(""annualReports""))
df"
10013,AlphaVantage - Get balance sheet,"import requests
import pandas as pd
API_KEY = ""demo""
COMPANY = ""IBM""
response = requests.get(f'https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol={COMPANY}&apikey={API_KEY}')
data = response.json()
df = pd.DataFrame(data.get(""annualReports""))
df"
10014,AlphaVantage - Get income statement,"import requests
import pandas as pd
API_KEY = ""demo""
COMPANY = ""IBM""
response = requests.get(f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={COMPANY}&apikey={API_KEY}')
data = response.json()
df = pd.DataFrame(data.get(""annualReports""))
df"
10015,AlphaVantage - Get company overview,"import requests
import pandas as pd
API_KEY = ""demo""
COMPANY = ""IBM""
response = requests.get(f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={COMPANY}&apikey={API_KEY}')
data = response.json()
data"
10016,WorldBank - World employment by sector,"import math
import pandas as pd
from datetime import datetime
from plotly.offline import iplot, plot, download_plotlyjs, init_notebook_mode
import plotly.graph_objects as go
from plotly.subplots import make_subplots
URL = ""https://www.ilo.org/ilostat-files/Documents/Excel/MBI_33_EN.xlsx""

SELECTED_YEAR = 2019
retrieved_data = pd.read_excel(URL, sep='\t', parse_dates=[0],
                     names=['COUNTRY','b','GENDER','YEAR','e','AGRICULTURE - FORESTRY - FISHING','MINING - QUARRYING','MANUFACTURING','UTILITIES','CONSTRUCTION',
                            'WHOLESALE - RETAIL - REPAIR VEHICLES','TRANSPORT - STORAGE - COMMUNICATION','ACCOMODATION - FOOD SERVICES','FINANCE - INSURANCE',
                            'REAL ESTATE - BUSINESS - ADMINISTRATION','PUBLIC ADMINISTRATION - DEFENCE - SOCIAL SECURITY','EDUCATION','HUMAN HEALTH - SOCIAL WORK',
                            'OTHER SERVICES','t','u','v','w','x','y','z','aa','ab','ac','ad','ae','af','ag','ah'])


# Select only rows with data
data = retrieved_data.drop([0, 1, 2, 3, 4], axis = 0)

# Select rows we want (""Total"" of gender and no male nor female)
data = data[data['GENDER'] == ""Total""]

# Select by year
data = data[data['YEAR'] == SELECTED_YEAR]

# Select only columns with interest
df = data[['COUNTRY', 'AGRICULTURE - FORESTRY - FISHING','MINING - QUARRYING','MANUFACTURING','UTILITIES','CONSTRUCTION',
              'WHOLESALE - RETAIL - REPAIR VEHICLES','TRANSPORT - STORAGE - COMMUNICATION','ACCOMODATION - FOOD SERVICES','FINANCE - INSURANCE',
              'REAL ESTATE - BUSINESS - ADMINISTRATION','PUBLIC ADMINISTRATION - DEFENCE - SOCIAL SECURITY','EDUCATION','HUMAN HEALTH - SOCIAL WORK',
              'OTHER SERVICES']]
               
df = df.set_index('COUNTRY')
df
# Initialization of variables and functions

SECTORS = []
for col in df.columns :
  SECTORS.append(col)

indexstock = ['World', ""France""]

#***********************************************************************************
# Initialization of graphs

fig = make_subplots(rows=2, cols=2, 
                    specs=[[{'type': 'polar'}, {'type': 'polar'}],[{""colspan"": 2}, None]],
                    shared_xaxes=True, shared_yaxes=False, row_heights=[0.3, 0.7],
                    vertical_spacing=0.11, horizontal_spacing=0.15)

#***********************************************************************************
# Add polar graphs

fig.add_trace(go.Scatterpolar(r = df.loc[indexstock[0]], theta = SECTORS, fill = 'toself', name = indexstock[0], 
                                  marker_color='mediumpurple'), row = 1, col = 1)
fig.add_trace(go.Scatterpolar(r = df.loc[indexstock[1]], theta = SECTORS, fill = 'toself', name = indexstock[1], 
                                  marker_color='indianred') ,row = 1, col = 2)

#***********************************************************************************
# Horizontal group bar graph 

# print(df.loc['World'])
fig.add_trace(go.Bar(x=df.loc[indexstock[0]], y=SECTORS, orientation='h', name=indexstock[0], text=df.loc[indexstock[0]], 
                     textposition='auto', marker_color='mediumpurple'), row = 2, col = 1)
fig.add_trace(go.Bar(x=df.loc[indexstock[1]], y=SECTORS, orientation='h', name=indexstock[1], text=df.loc[indexstock[1]], 
                     textposition='auto', marker_color='indianred'), row = 2, col = 1)

#***********************************************************************************
# Setting layout

fig.update_layout(title_text=""Differences of repartition of employement by country and sector au "" + str(datetime.today()) + "" (en %)"",
                  title_x=0.5, width=1600, height=1000, 
                  showlegend=True, legend=dict(x=-.2, y=0.8),
                  polar=dict(radialaxis=dict(visible=True)),
                  barmode='group')

#***********************************************************************************
# Creationg buttons

buttons_country_1 = []
for index in df.index:
  buttons_country_1.append(dict(method='restyle', label=index, args=[{'r':[df.loc[index]], 'x':[df.loc[index]], 'name':[index, index], 'text':[df.loc[index]]}, [0, 2]]))
buttons_country_2 = []
for index in df.index:
  buttons_country_2.append(dict(method='restyle', label=index, args=[{'r':[df.loc[index]], 'x':[df.loc[index]], 'name':[index, index], 'text':[df.loc[index]]}, [1, 3]]))

fig.update_layout(updatemenus=[dict(buttons=buttons_country_1, direction=""down"", pad={""r"": 1, ""t"": 1}, showactive=True, x=0.04, xanchor=""left"", y=0.69, yanchor=""top""),
                              dict(buttons=buttons_country_2, direction=""down"", pad={""r"": 1, ""t"": 1}, showactive=True, x=0.6, xanchor=""left"", y=0.69, yanchor=""top"")])

#***********************************************************************************
# Display graph

fig.show()

#***********************************************************************************
# Export as HTML file

Tickets_plot = fig
plot(Tickets_plot, filename=""/content/gdrive/My Drive/datamining/employement_by_sector_and_country.html"", auto_open=False)

"
10017,WorldBank - GDP per capita and growth,"import pandas as pd
import numpy as np
import plotly.graph_objects as go
from pandas_datareader import wb
from naas_drivers import plotly
indicators = wb.download(indicator=['NY.GDP.PCAP.CD', 'NY.GDP.PCAP.KD.ZG'], country='all', start=2013, end=2021)

indicators = indicators.reset_index()
indicators = indicators[['country', 'year', 'NY.GDP.PCAP.CD', 'NY.GDP.PCAP.KD.ZG']]
indicators.columns = ['country', 'year', 'GDP_PER_CAPITAL', 'GDP_GROWTH_PER_CAPITAL']

indicators = indicators.fillna(0)

countries = wb.get_countries()
countries = countries[['name', 'region', 'iso3c']]

master_table = pd.merge(indicators, countries, left_on='country', right_on='name')

master_table = master_table[master_table['region'] != 'Aggregates']

master_table = master_table.drop(columns=['name'])

master_table = master_table.dropna()

# Cr√©ation de l'ensemble final
xls_formatted = pd.DataFrame(columns=['COUNTRY', 'YEAR', 'GDP_PER_CAPITAL', 'GDP_GROWTH_PER_CAPITAL', 'REGION', 'ISO3C'])

for index, line in master_table.iterrows():
  xls_formatted = xls_formatted.append(
    {
        'COUNTRY': line['country'],
        'YEAR': line['year'],
        'GDP_PER_CAPITAL': line['GDP_PER_CAPITAL'],
        'GDP_GROWTH_PER_CAPITAL': line['GDP_GROWTH_PER_CAPITAL'],
        'REGION': line['region'],
        'ISO3C': line['iso3c'],
    }, ignore_index=True
  )

master_table = xls_formatted

master_table
year = ""2019""
master_year_table = master_table[master_table['YEAR'] == year]

GDP_GROWTH_PER_CAPITAL = ""GDP growth per capita""
GDP_PER_CAPITAL = ""GDP per capita""

fig = go.Figure()

fig.layout = go.Layout(
    #width=500,
    #height=300,
    annotations=[
        go.layout.Annotation(
            showarrow=False,
            text='Source: World Bank',
            xanchor='right',
            x=1,
            yanchor='top',
            y=-0.05
        )])

fig.add_trace(go.Choropleth(
    locations=master_year_table['ISO3C'],
    z = master_year_table['GDP_PER_CAPITAL'],
    colorscale = [(0,""#053D8B""),(0.25,""#1164B0""),(0.5,""#298BC8""),(0.75,""#3FB7DB""), (1,""#5CD5E8"")],
    colorbar_title = ""Value"",
    customdata = master_year_table['COUNTRY'],
    hovertemplate = '<b>%{customdata}: %{z:,.0f}</b><extra></extra>'
))

fig.add_trace(go.Choropleth(
    locations=master_year_table['ISO3C'],
    visible= False,
    z = master_year_table['GDP_GROWTH_PER_CAPITAL'],
    colorscale = [(0,""#053D8B""),(0.25,""#1164B0""),(0.5,""#298BC8""),(0.75,""#3FB7DB""), (1,""#5CD5E8"")],
    colorbar_title = ""Growth "",
    customdata = master_year_table['COUNTRY'],
    hovertemplate = '<b>%{customdata}: %{z:0.2f}%</b><extra></extra>'
))

fig.update_layout(
    autosize=True,
    width= 900,
    #height= 900,
    title=f""GDP per capital in {year}"",
    title_x=0.5,
    title_y=0.95,
    updatemenus=[
        dict(
            type = ""buttons"",
            active=0,
            direction = ""left"",
            buttons=list([
                dict(
                    args=[{""visible"": [True, False]}, {""title"": f""{GDP_PER_CAPITAL} in {year}""}],
                    label=GDP_PER_CAPITAL,
                    method=""update""
                ),
                dict(
                    args=[{""visible"": [False, True]}, {""title"": f""{GDP_GROWTH_PER_CAPITAL} in {year}""}],
                    label=GDP_GROWTH_PER_CAPITAL,
                    method=""update""
                )
            ]),
            showactive=True,
            x=0.32,
            xanchor=""left"",
            y=1.2,
            yanchor=""top""
        ),
    ]
)

fig.show()
plotly.export(fig, ""GDP.png"", css=None)
plotly.export(fig, ""GDP.html"", css=None)"
10018,WorldBank - Richest countries top10,"import pandas as pd
from pandas_datareader import wb
import plotly.graph_objects as go
pd.options.display.float_format = '{: .0f}'.format
countries = wb.get_countries()
countries = countries[['name', 'region']]
countries
indicators = wb.download(indicator=['NY.GDP.PCAP.CD', 'NY.GDP.MKTP.CD'], country='all', start=2018, end=2018)
indicators = indicators.reset_index()
indicators = indicators[['country', 'NY.GDP.PCAP.CD', 'NY.GDP.MKTP.CD']]
indicators.columns = ['country', 'GDP_PER_CAPITA', 'CURRENT_GDP']
indicators
master_table = pd.merge(indicators, countries, left_on='country', right_on='name')

master_table = master_table[master_table['region'] != 'Aggregates']
master_table = master_table[(master_table['GDP_PER_CAPITA'] > 0) | (master_table['CURRENT_GDP'] > 0)]
master_table = master_table.fillna(0)

master_table = pd.melt(master_table, id_vars=['region', 'country'], value_vars=['GDP_PER_CAPITA', 'CURRENT_GDP'], var_name='INDICATOR', value_name='VALUE')
master_table = master_table.set_index(['region', 'country', 'INDICATOR'])
master_table = master_table.sort_index()

master_table
table = master_table.reset_index()
gdp_per_capita_per_region = table[table['INDICATOR'] == 'GDP_PER_CAPITA'][['region', 'VALUE']].groupby('region').mean().sort_values('VALUE', ascending=False)
current_gdp_per_region = table[table['INDICATOR'] == 'CURRENT_GDP'][['region', 'VALUE']].groupby('region').mean().sort_values('VALUE', ascending=False)
gdp_per_capita_per_country = table[table['INDICATOR'] == 'GDP_PER_CAPITA'][['country', 'VALUE']].sort_values('VALUE', ascending=False).head(10)
current_gdp_per_country = table[table['INDICATOR'] == 'CURRENT_GDP'][['country', 'VALUE']].sort_values('VALUE', ascending=False).head(10)

data = [
  go.Bar(x=gdp_per_capita_per_region.index, y=gdp_per_capita_per_region['VALUE'], text=gdp_per_capita_per_region['VALUE'], textposition='outside'),
  go.Bar(x=current_gdp_per_region.index, y=current_gdp_per_region['VALUE'], text=current_gdp_per_region['VALUE'], textposition='outside', visible=False),
  go.Bar(x=gdp_per_capita_per_country['country'], y=gdp_per_capita_per_country['VALUE'], text=gdp_per_capita_per_country['VALUE'], textposition='outside', visible=False),
  go.Bar(x=current_gdp_per_country['country'], y=current_gdp_per_country['VALUE'], text=current_gdp_per_country['VALUE'], textposition='outside', visible=False),
]

layout = go.Layout(
  title='Top 10 richest regions & countries',
  margin = dict(t = 60, b = 150),
  updatemenus=list([
    dict(showactive=True, type=""buttons"", active=0, buttons=[
      {'label': 'GDP / Capita per region', 'method': 'update', 'args': [{'visible': [True, False, False, False]}]},
      {'label': 'Current GDP per region', 'method': 'update', 'args': [{'visible': [False, True, False, False]}]},
      {'label': 'GDP / Capita per country', 'method': 'update', 'args': [{'visible': [False, False, True, False]}]},
      {'label': 'Current GDP per country', 'method': 'update', 'args': [{'visible': [False, False, False, True]}]}
    ])
  ]),
  annotations=[dict(
    text = 'Updated in 2018 from The World Bank',
    showarrow = False,
    xref = 'paper', x = 1,
    yref = 'paper', y = -0.4)]
)

go.Figure(data, layout)"
10019,WorldBank - GDP contributors,"import pandas as pd
from pandas_datareader import wb
import plotly.graph_objects as go
country = wb.get_countries()[""iso2c""]
startYear = 2017
endYear = startYear + 1

#Recup√©ration des valeurs pour World (Initial - Final) GDP/PPL
data_gdp_world_2017 = wb.download(indicator=['NY.GDP.PCAP.KD.ZG'], country=['WLD'], start=startYear, end=startYear)
data_gdp_world_2018 = wb.download(indicator=['NY.GDP.PCAP.KD.ZG'], country=['WLD'], start=endYear, end=endYear)

gdp_growth_2017 = data_gdp_world_2017.iloc[0][0]
gdp_growth_2018 = data_gdp_world_2018.iloc[0][0]

data_gdp_current_year = wb.download(indicator=['NY.GDP.PCAP.KD.ZG'], country=country, start=endYear, end=endYear)

data_gdp_current_year
data_gdp_biggest = data_gdp_current_year.sort_values('NY.GDP.PCAP.KD.ZG', ascending=False)
data_gdp_biggest = data_gdp_biggest.head(5)

data_gdp_lowest = data_gdp_current_year.sort_values('NY.GDP.PCAP.KD.ZG', ascending=True)
data_gdp_lowest = data_gdp_lowest.head(5)
#Data formating

measure = []
text = []
x = []
y = []


#Push initial
measure.append(""absolute"")
text.append(str(round(gdp_growth_2017,4)))
y.append(gdp_growth_2017)
x.append(str(startYear) + "" Growth"")


#Push Beetween
# measure.append(""relative"")
# text.append(str(round(gdp_growth_2018 - gdp_growth_2017,4)))
# y.append(gdp_growth_2018 - gdp_growth_2017)
# x.append(""Between"")

totalBiggest = 0

for index, row in data_gdp_biggest.iterrows() :
    measure.append(""relative"")
    text.append(index[0])
    totalBiggest += row[0]
    y.append(row[0])
    x.append(index[0])


totalLowest = 0

for index, row in data_gdp_lowest.iterrows() :
    measure.append(""relative"")
    text.append(index[0])
    totalLowest += row[0]
    y.append(row[0])
    x.append(index[0])


#Other Country total
otherCountryTotal =   gdp_growth_2018 - (gdp_growth_2017 + totalLowest + totalBiggest)
measure.append(""relative"")
text.append(str(round(otherCountryTotal,4)))
y.append(otherCountryTotal)
x.append(""Other Country"")



#Push result
measure.append(""total"")
text.append(str(round(gdp_growth_2018,4)))
y.append(gdp_growth_2018)
x.append(str(endYear) + "" Growth"")


fig = go.Figure(go.Waterfall(
    name = ""Growth between Year"", orientation = ""v"",
    measure = measure,
    x = x,
    text = text,
    y = y,
    textposition = ""outside"",
    texttemplate=""%{y:.2f}%"",
    connector = {""line"":{""color"":""rgb(63, 63, 63)""}},
))

fig.update_layout(title = ""GDP growth with 5 top and lowest contributors (per capita per country) "", showlegend = True, margin=dict(l=0, r=0, t=50, b=0),
    height=700)

fig.show()
#Recup√©ration des valeurs pour World (Initial - Final) GDP/country
data_gdp_world_2017 = wb.download(indicator=['NY.GDP.MKTP.KD.ZG'], country=['WLD'], start=startYear, end=startYear)
data_gdp_world_2018 = wb.download(indicator=['NY.GDP.MKTP.KD.ZG'], country=['WLD'], start=endYear, end=endYear)

gdp_growth_pll_2017 = data_gdp_world_2017.iloc[0][0]
gdp_growth_2018 = data_gdp_world_2018.iloc[0][0]

data_gdp_current_year = wb.download(indicator=['NY.GDP.MKTP.KD.ZG'], country=country, start=endYear, end=endYear)


data_gdp_current_year
data_gdp_biggest = data_gdp_current_year.sort_values('NY.GDP.MKTP.KD.ZG', ascending=False)
data_gdp_biggest = data_gdp_biggest.head(5)

data_gdp_lowest = data_gdp_current_year.sort_values('NY.GDP.MKTP.KD.ZG', ascending=True)
data_gdp_lowest = data_gdp_lowest.head(5)

#Data formating

measure = []
text = []
x = []
y = []


#Push initial
measure.append(""absolute"")
text.append(str(round(gdp_growth_2017,4)))
y.append(gdp_growth_2017)
x.append(str(startYear) + "" Growth"")


#Push Beetween
# measure.append(""relative"")
# text.append(str(round(gdp_growth_2018 - gdp_growth_2017,4)))
# y.append(gdp_growth_2018 - gdp_growth_2017)
# x.append(""Between"")

totalBiggest = 0

for index, row in data_gdp_biggest.iterrows() :
    measure.append(""relative"")
    text.append(index[0])
    totalBiggest += row[0]
    y.append(row[0])
    x.append(index[0])


totalLowest = 0

for index, row in data_gdp_lowest.iterrows() :
    measure.append(""relative"")
    text.append(index[0])
    totalLowest += row[0]
    y.append(row[0])
    x.append(index[0])


#Other Country total
otherCountryTotal =   gdp_growth_2018 - (gdp_growth_2017 + totalLowest + totalBiggest)
measure.append(""relative"")
text.append(str(round(otherCountryTotal,4)))
y.append(otherCountryTotal)
x.append(""Other Country"")



#Push result
measure.append(""total"")
text.append(str(round(gdp_growth_2018,4)))
y.append(gdp_growth_2018)
x.append(str(endYear) + "" Growth"")
fig = go.Figure(go.Waterfall(
    name = ""Growth between Year"", orientation = ""v"",
    measure = measure,
    x = x,
    text = text,
    y = y,
    textposition = ""outside"",
    texttemplate=""%{y:.2f}%"",
    connector = {""line"":{""color"":""rgb(63, 63, 63)""}},
))

fig.update_layout(title = ""GDP growth with 5 top and lowest contributors (per country)"", showlegend = True,margin=dict(l=0, r=0, t=50, b=0), height=700)

fig.show()"
10020,WorldBank - Most populated countries,"import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
import requests
import io
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from pandas import DataFrame
import plotly.graph_objects as go
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
downloaded = drive.CreateFile({'id':""1FjX4NTIq1z3zS9vCdAdpddtj9mKa0wIW""})   # replace the id with id of file you want to access
downloaded.GetContentFile('POP_PROJ_20042020112713800.csv')
data = pd.read_csv(""POP_PROJ_20042020112713800.csv"", usecols=[""Country"", ""Time"", ""Value""])
data.rename(columns = {'Country':'COUNTRY', 'Time':'TIME', 
                              'Value':'VALUE'}, inplace = True) 
data
firstOccur = []
secondOccur = []
firstYear = 2000
secondYear = 2030
def tambouille_first(number1):
  first = []
  for index, row in data.iterrows():
    if(row[""TIME""] == number1):
      first.append(row)

  first = DataFrame(first)
  first = first.sort_values(by =""VALUE"",ascending=True)
  first = first.tail(10)
  return first

def tambouille_second(number2):
  second = []
  for index, row in data.iterrows():
    if(row[""TIME""] == number2):
      second.append(row)

  second = DataFrame(second)
  second =second.sort_values(by =""VALUE"",ascending=True)
  second = second.tail(10)
  return second

firstOccur = tambouille_first(firstYear)
secondOccur = tambouille_second(secondYear)

firstOccur

fig = go.Figure(data=[
  go.Bar(name=str(firstYear), y=firstOccur[""COUNTRY""], x=firstOccur[""VALUE""],orientation='h'),
  go.Bar(name=str(secondYear), y=secondOccur[""COUNTRY""], x=secondOccur[""VALUE""],orientation='h'),
])
fig.update_layout(title_text=""TOP 10 des pays les plus peupl√©s en 2000 avec pr√©vision 2030"", annotations=[
        dict(
            x=1,
            y=-0.15,
            showarrow=False,
            text=""Source : OECD -> 2019"",
            xref=""paper"",
            yref=""paper""
        )])
fig.show()"
10021,WorldBank - GDP per country and evolution,"import pandas as pd
import numpy as np
import plotly.graph_objects as go
from pandas_datareader import wb

indicators = wb.download(indicator=['NY.GDP.PCAP.CD', 'NY.GDP.PCAP.KD.ZG'], country='all', start=2013, end=2018)

indicators = indicators.reset_index()
indicators = indicators[['country', 'year', 'NY.GDP.PCAP.CD', 'NY.GDP.PCAP.KD.ZG']]
indicators.columns = ['country', 'year', 'GDP_PER_CAPITAL', 'GDP_GROWTH_PER_CAPITAL']

indicators = indicators.fillna(0)

countries = wb.get_countries()
countries = countries[['name', 'region', 'iso3c']]

master_table = pd.merge(indicators, countries, left_on='country', right_on='name')

master_table = master_table[master_table['region'] != 'Aggregates']

master_table = master_table.drop(columns=['name'])

master_table = master_table.dropna()

# Cr√©ation de l'ensemble final
xls_formatted = pd.DataFrame(columns=['COUNTRY', 'YEAR', 'GDP_PER_CAPITAL', 'GDP_GROWTH_PER_CAPITAL', 'REGION', 'ISO3C'])

for index, line in master_table.iterrows():
  xls_formatted = xls_formatted.append(
    {
        'COUNTRY': line['country'],
        'YEAR': line['year'],
        'GDP_PER_CAPITAL': line['GDP_PER_CAPITAL'],
        'GDP_GROWTH_PER_CAPITAL': line['GDP_GROWTH_PER_CAPITAL'],
        'REGION': line['region'],
        'ISO3C': line['iso3c'],
    }, ignore_index=True
  )

master_table = xls_formatted

master_table
# Variable √† changer pour avoir les autres ann√©es
year = ""2018""
master_year_table = master_table[master_table['YEAR'] == year]

GDP_GROWTH_PER_CAPITAL = ""GDP GROWTH PER CAPITAL""
GDP_PER_CAPITAL = ""GDP PER CAPITAL""

fig = go.Figure()

fig.add_trace(go.Choropleth(
    locations=master_year_table['ISO3C'],
    z = master_year_table['GDP_PER_CAPITAL'],
    colorscale = [(0,""black""), (0.01,""red""),(0.1,""yellow""),(0.3,""green""),(1,""green"")],
    colorbar_title = ""GDP PER CAPITAL"",
    customdata = master_year_table['COUNTRY'],
    hovertemplate = '<b>%{customdata}: %{z:,.0f}</b><extra></extra>'
))

fig.add_trace(go.Choropleth(
    locations=master_year_table['ISO3C'],
    visible= False,
    z = master_year_table['GDP_GROWTH_PER_CAPITAL'],
    colorscale = [(0,""red""),(0.5,""red""),(0.75,""rgb(240,230,140)""), (1,""green"")],
    colorbar_title = ""GDP GROWTH PER CAPITAL"",
    customdata = master_year_table['COUNTRY'],
    hovertemplate = '<b>%{customdata}: %{z:0.2f}%</b><extra></extra>'
))

fig.update_layout(
    autosize=False,
    width= 1600,
    height= 900,
    title=f""GDP per capital in {year}"",
    title_x=0.5,
    updatemenus=[
        dict(
            type = ""buttons"",
            active=0,
            buttons=list([
                dict(
                    args=[{""visible"": [True, False]}, {""title"": f""{GDP_PER_CAPITAL} in {year}""}],
                    label=GDP_PER_CAPITAL,
                    method=""update""
                ),
                dict(
                    args=[{""visible"": [False, True]}, {""title"": f""{GDP_GROWTH_PER_CAPITAL} in {year}""}],
                    label=GDP_GROWTH_PER_CAPITAL,
                    method=""update""
                )
            ]),
            showactive=True,
            x=1,
            xanchor=""right"",
            y=1.1,
            yanchor=""top""
        ),
    ]
)

fig.show()"
10022,WorldBank - Gini index,"import pandas as pd
from pandas_datareader import wb
import plotly.graph_objects as go
import plotly.express as px 
countries = wb.get_countries()
countries = countries[['name', 'iso3c']]
countries.columns = ['country', 'iso3c']
countries
indicators = wb.download(indicator=['SI.POV.GINI'], country='all', start=1967, end=2018)
indicators.columns = ['GINI_INDEX']
indicators
master_table = pd.merge(indicators.reset_index(), countries, left_on='country', right_on='country')
master_table = master_table.set_index(['country', 'iso3c', 'year'])
master_table
pivoted_table = pd.pivot_table(master_table, index=['country', 'iso3c'], columns='year', values='GINI_INDEX')
pivoted_table = pivoted_table.ffill(axis=1)
pivoted_table
pivoted_table = pd.pivot_table(master_table, index=['country', 'iso3c'], columns='year', values='GINI_INDEX')
pivoted_table = pivoted_table.ffill(axis=1)
countries = list(pivoted_table.index.get_level_values(0))
locations = list(pivoted_table.index.get_level_values(1))

data = []
steps = []
i = 0
for year in pivoted_table.columns:
  data.append(dict(
    type='choropleth',
    name='',
    locations=locations,
    z=pivoted_table[year],
    hovertext=countries,
    colorscale=px.colors.sequential.Reds,
    visible=year=='2018'
  ))
  
  step = dict(
    method='restyle',
    args=['visible', [False] * len(pivoted_table.columns)],
    label=year)
  step['args'][1][i] = True
  steps.append(step)

  i = i + 1

layout = go.Layout(
  title=dict(
    text='Evolution of the gini index from 1969 to 2018', 
    x=0.5,
    font=dict(
      size=21,
    )
  ),
  sliders=[dict(steps=steps, active=len(data) - 1)],
  annotations=[dict(
    text='Updated in 2018 from The World Bank',
    showarrow=False,
    x=1,
    y=-0.05
  )],
  autosize=True,
  height=800
)

fig = go.Figure(data, layout)
fig
fig.write_html(""file.html"")"
10023,WorldBank - World population and density,"import pandas as pd
import numpy as np
import plotly.express as px

# Options pour afficher plus de donn√©es sur le retour console
# pd.set_option(""display.max_rows"", 10)
# pd.set_option(""display.max_columns"", 10)
years = list(map(lambda a : str(a), range(1950, 2020, 1)))
usecols = [""Region, subregion, country or area *"", ""Country code"", ""Type"", *years]
renamed_population_columns = {}
renamed_density_columns = {}

xls_populations = pd.read_excel('https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/1_Population/WPP2019_POP_F01_1_TOTAL_POPULATION_BOTH_SEXES.xlsx',
                    header=16,
                    encoding=""utf-8"",
                    usecols=usecols)

# Pour chaque ann√©e on vient cr√©er une colonne ""population_{ann√©e}"" dans notre dataset
for year in years:
  xls_populations[year] = pd.to_numeric(xls_populations[year], errors='coerce')
  renamed_population_columns[year] = f""population_{year}""
xls_populations = xls_populations.rename(columns=renamed_population_columns)

# On r√©cup√®re seulement les valeurs du type ""Country/Area""
xls_populations = xls_populations[xls_populations['Type'] == ""Country/Area""]

xls_populations
xls_density = pd.read_excel('https://population.un.org/wpp/Download/Files/1_Indicators%20(Standard)/EXCEL_FILES/1_Population/WPP2019_POP_F06_POPULATION_DENSITY.xlsx',
                    header=16,
                    encoding=""utf-8"",
                    usecols=[""Region, subregion, country or area *"", ""Country code"", ""Type"", *years])

# Pour chaque ann√©e on vient cr√©er une colonne ""density_{ann√©e}"" dans notre dataset
for year in years:
  xls_density[year] = pd.to_numeric(xls_density[year], errors='coerce')
  renamed_density_columns[year] = f""density_{year}""
xls_density = xls_density.rename(columns=renamed_density_columns)

# On r√©cup√®re seulement les valeurs du type ""Country/Area""
xls_density = xls_density[xls_density['Type'] == ""Country/Area""]

xls_density

# On vient concatener le dataset ""Population"" avec le dataset ""Densit√©""
result = pd.concat([xls_populations,xls_density], sort=False)
n = result.index.nlevels
xls_global = result.groupby(level=range(n)).first()

xls_global
# Pour chaque ann√©e on vient comparer la population total d'un pays avec celle de l'ann√©e N-1 pour en d√©duire son √©volution sur une ann√©e
for index, year in enumerate(years):
  # Suppression des bruits (donn√©es non-traitables)
  if index is 0:
    continue
  try:
    past_year = str(int(year) - 1)
    xls_global[f'population_growth_{year}'] = (xls_global[f'population_{year}'] - xls_global[f'population_{past_year}']) / xls_global[f'population_{past_year}'] * 100
  except KeyError:
    xls_global[f'population_growth_{year}'] = np.nan

xls_global
# R√©cup√©ration des continents via l'API RestCountries
countries = pd.read_json('https://restcountries.eu/rest/v2/all?fields=region;numericCode', dtype = {""numericCode"": int})
countries = countries.rename(columns={""region"": ""Region"", ""numericCode"" : ""Country code""})
# Suppression du bruit (donn√©es non-traitables)
countries= countries.dropna()
# On format les donn√©es pour qu'elles correspondent au format du dataset global
countries['Country code'] = countries['Country code'].replace(regex=r""^0+"", value='')
countries[""Country code""] = countries[""Country code""].astype(int)

countries
xls_global = xls_global.join(countries.set_index('Country code'), on='Country code')

xls_global
# Cr√©ation de l'ensemble final
xls_formatted = pd.DataFrame(columns=['COUNTRY', 'YEAR', 'POPULATION', 'POPULATION GROWTH', 'DENSITY', 'REGION'])


for index, line in xls_global.iterrows():
  for year in years:
    # On ignore 1950 car il n'est pas possible de calculer l'√©volution sans les donn√©es de 1949
    if year == ""1950"":
      continue
    xls_formatted = xls_formatted.append(
        {
            'COUNTRY': line['Region, subregion, country or area *'],
            'YEAR': year,
            'POPULATION': line[f""population_{year}""],
            'POPULATION GROWTH': line[f""population_growth_{year}""],
            'DENSITY': line[f""density_{year}""],
            'REGION': line['Region'],
        }, ignore_index=True)

# Suppression du bruit (donn√©es non-traitables)
xls_formatted = xls_formatted.dropna()

xls_formatted
fig = px.scatter(xls_formatted, x=""DENSITY"", y=""POPULATION GROWTH"", animation_frame=""YEAR"", animation_group=""COUNTRY"",
           size=""POPULATION"", color=""REGION"", hover_name=""COUNTRY"",
           log_x=True, size_max=60)
fig.show()"
10024,PyPI - - Get number of downloads any package,"try:
    import pypistats
except:
    !pip install -U pypistats --user
    !pip install --upgrade pypistats
    import pypistats
# from pprint import pprint
from datetime import datetime
import plotly.graph_objects as go
import naas
# Inputs
package = ""pandas""

# Outputs
name_output = f""{package}_downloads""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df = pypistats.overall(package, total=False, format=""pandas"")
df.head()
# Gives us the cumulative number of downloads over a period of 180 days
def get_cumulative_dataframe(df):
    
    data = df.groupby('category').get_group('with_mirrors').sort_values(
        'date').reset_index(drop='index').groupby(
        'date').agg({'downloads':'sum'}).reset_index()
    
    cum_sum = 0
    for idx, num in enumerate(data['downloads']):
        cum_sum+=num
        data.loc[idx, 'cumulative_downloads'] = cum_sum

    data['cumulative_downloads'] = data.cumulative_downloads.astype('int')
    data.drop(columns = 'downloads', inplace=True)
    
    return data

df_downloads = get_cumulative_dataframe(df)
df_downloads.tail(5)
def create_linechart(df, package, date, value):
    # Get last value
    last_value = ""{:,.0f}"".format(df.loc[df.index[-1], value]).replace("","", "" "")
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date].to_list(),
            y=df[value].to_list(),
            mode=""lines+text"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""‚≠ê<b> Number of downloads for {package} </b><br><span style='font-size: 13px;'> Total Downloads as of today: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        yaxis_title='No. of downloads',
        yaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_downloads, package, ""date"", ""cumulative_downloads"")
# Save your dataframe in CSV
df_downloads.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10025,Streamlit - Create prediction app,"from naas_drivers import streamlit
import streamlit as st
from naas_drivers.tools.prediction import Prediction
from naas_drivers.tools.yahoofinance import Yahoofinance
from naas_drivers.tools.plotly import Plotly
%%writefile streamlit_app.py

yf = Yahoofinance()
pre = Prediction()
plotly = Plotly()

stock = ""TSLA""

dataset = yf.get(stock_companies = stock)
pr = pre.get(dataset=dataset)
plt = plotly.stock(pr,""linechart_close"")

st.write(""# Prediction for {}"".format(stock))
st.plotly_chart(plt)
streamlit.add(""streamlit_app.py"", port=9999, debug=False)"
10026,PostgresSQL - Get data from database,"try:
    import psycopg2
except:
    !pip install psycopg2-binary --user
    import psycopg2
import pandas as pd
import naas
# Credentials
PG_USER = ""YOUR_PG_USER""
PG_PASSWORD = ""YOUR_PG_PASSWORD""
PG_HOST = ""YOUR_PG_HOST""
PG_DBNAME = ""YOUR_PG_DBNAME""

# Database
DATABASE = ""YOUR_DATABASE""
SELECTED_FIELD = ""*"" # ""*"" or list of columns

# SQL Requests
SQL_REQUEST = f""SELECT {SELECTED_FIELD} FROM {DATABASE}""
PG = psycopg2.connect(f""user={PG_USER} password={PG_PASSWORD} dbname={PG_DBNAME} host={PG_HOST}"")
def get_data():
    cur = PG.cursor()
    cur.execute(SQL_REQUEST)
    res = cur.fetchall()
    cur.close()
    df = pd.DataFrame(res)
    return df

df = get_data()
df"
10027,Microsoft Teams - Send message,"from naas_drivers import teams
webhook = ""https://forgr.webhook.office.com/webhookb2/83bcbd92-7095-48e9-bc59-60fb9f6dcf2e@da744f16-111c-41b2-899f-6332dfe22d16/IncomingWebhook/b3cf540a51d7465ca10bd85a61c10d92/c97dc097-a7ff-4123-865d-ac6482bcdac2""
message = ""second test""
image_url = ""http://i.imgur.com/c4jt321l.png"" # Set to None if you don't need it
tm = teams.connect(webhook)
tm.send(message, image=image_url)"
10028,Johns Hopkins - Covid19 Active Cases,"import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
# URLs of the raw csv dataset
urls = [
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'
]

confirmed_df, deaths_df, recovered_df = tuple(pd.read_csv(url) for url in urls)
#Wide to Long DataFrame conversion
dates = confirmed_df.columns[4:]
confirmed_df_long = confirmed_df.melt(
    id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], 
    value_vars=dates, 
    var_name='Date', 
    value_name='Confirmed'
)
deaths_df_long = deaths_df.melt(
    id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], 
    value_vars=dates, 
    var_name='Date', 
    value_name='Deaths'
)
recovered_df_long = recovered_df.melt(
    id_vars=['Province/State', 'Country/Region', 'Lat', 'Long'], 
    value_vars=dates, 
    var_name='Date', 
    value_name='Recovered'
)

# Adjust for Canada
recovered_df_long = recovered_df_long[(recovered_df_long['Country/Region']!='Canada')]
# Join into one single dataframe/table
# Merging confirmed_df_long and deaths_df_long
full_table = confirmed_df_long.merge(
  right=deaths_df_long, 
  how='left',
  on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long']
)
# Merging full_table and recovered_df_long
full_table = full_table.merge(
  right=recovered_df_long, 
  how='left',
  on=['Province/State', 'Country/Region', 'Date', 'Lat', 'Long']
)

# Convert date strings to actual dates
full_table['Date'] = pd.to_datetime(full_table['Date'])
# Handle some missing values / NaNs
full_table['Recovered'] = full_table['Recovered'].fillna(0).astype('int64')

full_table.isna().sum()
# full_table.dtypes
# Adjust for Canada and 3 cruise ships
ship_rows = full_table['Province/State'].str.contains('Grand Princess') | full_table['Province/State'].str.contains('Diamond Princess') | full_table['Country/Region'].str.contains('Diamond Princess') | full_table['Country/Region'].str.contains('MS Zaandam')
full_ship = full_table[ship_rows]
full_table = full_table[~(ship_rows)]

# Add one more entry for each day to get the entire world's counts/totals
world_dict = {""Country/Region"": ""World"", ""Confirmed"": pd.Series(full_table.groupby(['Date'])['Confirmed'].sum()), ""Deaths"": pd.Series(full_table.groupby(['Date'])['Deaths'].sum()),""Recovered"": pd.Series(full_table.groupby(['Date'])['Recovered'].sum())}
world_df = pd.DataFrame.from_dict(world_dict).reset_index()
print(world_df.columns)
full_table = pd.concat([full_table, world_df], ignore_index=True)
# Active Cases = Confirmed - Deaths - Recovered
full_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']

full_grouped = full_table.groupby(['Date', 'Country/Region'])['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()
len(full_grouped[""Country/Region""].unique())
# Go back from long to wide for viz purposes
df = full_grouped
df.rename(columns={""Country/Region"": ""Country""}, inplace=True)
df_confirmed = df[[""Date"", ""Country"", ""Confirmed""]]
df_deaths = df[[""Date"", ""Country"", ""Deaths""]]
df_active = df[[""Date"", ""Country"", ""Active""]]
df_recovered = df[[""Date"", ""Country"", ""Recovered""]]

df_confirmed = df_confirmed.pivot(index=""Date"", columns=""Country"", values=""Confirmed"")
df_deaths = df_deaths.pivot(index=""Date"", columns=""Country"", values=""Deaths"")
df_recovered = df_recovered.pivot(index=""Date"", columns=""Country"", values=""Recovered"")
df_active = df_active.pivot(index=""Date"", columns=""Country"", values=""Active"")
def create_layout_button(df, column):
    first, latest = df.index.values[0], df.index.values[-1]
    return dict(label = column,
                method = 'update',
                args = [{'visible': df.columns.isin([column]),
                         'title': column,
                         'xaxis.range': [first, latest],
                         'showlegend': True
                        }])

def multi_plot(df, title, addAll = True):
    first, latest = df.index.values[0], df.index.values[-1]
    fig = go.Figure()

    for column in df.columns.to_list():
        fig.add_trace(
            go.Scatter(
                x = df.index,
                y = df[column],
                name = column
            )
        )

    button_all = dict(label = 'All',
                  method = 'update',
                  args = [{'visible': df.columns.isin(df.columns),
                           'title': 'All',
                           'xaxis.range': [first, latest],
                           'showlegend':True}])
    
    # Need ""World"" to be the default choice if ""All"" is not shown
    button_world = create_layout_button(df, ""World"")

    fig.update_layout(
        updatemenus=[{
            ""active"": 0,
            ""buttons"": ([button_all] * addAll) + [button_world] + [create_layout_button(df, column) for column in df.columns if column != ""World""],
            ""showactive"": True
            }
        ],
        yaxis_type=""log""
    )
    
    # Update remaining layout properties
    fig.update_layout(
        title_text=title,
#         annotations=[dict(
#             text=""Country:"",
#             x=0, y=0
#         )]
    )
   
    fig.show()
# test_df_active = df_active.swapaxes(""index"", ""columns"")
test_df_active = df_active
latest = test_df_active.index.values[-1]
print(latest)
test_df_active = test_df_active.T.sort_values(by=latest, ascending=False).head(11).T
test_df_active
multi_plot(test_df_active, title=""Logarithmic COVID-19 time series Active Cases by Country (Top 10)"")
multi_plot(df_active, title=""Logarithmic COVID-19 time series Active Cases by Country"", addAll=False)
# Uncomment to get a 30 day Moving Average Statistics and a health indicator based on that

# df_active[""MonthlyAverage""] = df_active[""World""].rolling('30D').mean().astype('int64')
# curr_30d = df_active.loc[latest, ""MonthlyAverage""]
# max_30d = df_active[""MonthlyAverage""].max()
# min_30d = df_active[""MonthlyAverage""].min()
# WHI_30d = 10 - 10 * ((curr_30d - min_30d) / (max_30d - min_30d))
#print(f""World Health Indicator (30 day Moving Average): {round(WHI_30d, 2)}"")
WHI = 10 - 10 * ((df_active.loc[latest, ""World""] - df_active[""World""].min()) / (df_active[""World""].max() - df_active[""World""].min()))

print(f""World Health Indicator (Raw values): {round(WHI, 2)}"")
WHI_data = pd.DataFrame.from_dict({""DATE_PROCESSED"": pd.to_datetime(""today"").date(), ""INDICATOR"": ""COVID-19 Active Cases"", ""VALUE"": [round(WHI, 2)]})
WHI_data"
10029,Johns Hopkins - Get Covid19 data,"import pandas as pd
import naas
# Input URLs of the raw csv dataset
urls = [
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'
]

# Output paths
title = ""DB_COVID19_JHU""
output_csv = f""{title}.csv""
def get_data_url(urls):
    df = pd.DataFrame()
    for url in urls:
        tmp_df = pd.read_csv(url)
        tmp_df[""Indicator""] = url.split(""/time_series_covid19_"")[-1].split(""_global.csv"")[0].capitalize()
        df = pd.concat([df, tmp_df])
    return df

df_init = get_data_url(urls)
df_init
def get_all_data(df_init):
    df = df_init.copy()
    # Cleaning
    df = df.drop(""Province/State"", axis=1)
    
    # Melt data
    df = pd.melt(df,
                 id_vars=[""Country/Region"", ""Lat"", ""Long"", ""Indicator""],
                 var_name=""Date"",
                 value_name=""Value"").fillna(0)
    df[""Date""] = pd.to_datetime(df[""Date""])
    
    # Calc active cases
    df_active = df.copy()
    df_active.loc[df_active[""Indicator""].isin([""Deaths"", ""Recovered""]), ""Value""] = df_active[""Value""] * (-1)
    df_active[""Indicator""] = ""Active cases""
    
    # Concat data
    df = pd.concat([df, df_active])
    
    # Group by country/region
    to_group = [""Country/Region"", ""Lat"", ""Long"", ""Indicator"", ""Date""]
    df = df.groupby(to_group, as_index=False).agg({""Value"": ""sum""})
    
    # Cleaning
    df = df.rename(columns={""Country/Region"": ""COUNTRY""})
    df.columns = df.columns.str.upper()
    return df.reset_index(drop=True)

df_clean = get_all_data(df_init)
df_clean
df_clean.to_csv(output_csv, index=False)"
10030,Neo - Get currencies live prices,"!pip install requests
from pathlib import Path
import pandas as pd
import requests
ccy_pair = 'EUR/CHF'
AUTH_HOST = ""https://auth.getneo.com""
DATA_HOST = ""https://data.getneo.com""
login_id = 'LOGIN'
api_key = 'PASSWORD'
bearer_token = None
output_dir = Path(""../data_output/"")
output_file = f""{ccy_pair.replace('/', '')}.csv""
if not bearer_token:
    response = requests.get(f""{AUTH_HOST}/api/v1/auth/login/"", params={""login_id"": login_id, ""api_key"": api_key})
    if response.status_code != 200:
        raise PermissionError(f""Failed to authenticate as {login_id}"")
    print(f""Authenticated as {login_id}"")

    bearer_token = response.headers[""Authorization""]
response = requests.get(f""{DATA_HOST}/api/v1/prices/{ccy_pair}"", headers={""Authorization"": bearer_token})
if response.status_code != 200:
    bearer_token = None
    raise ValueError(f""Failed to retrieve {ccy_pair} prices"")
prices_dict = response.json()
prices = pd.DataFrame(data=list(prices_dict.items()), columns=[""DATE"", ""VALUE""])
oldest_date = prices[""DATE""].min()
print(f""Closing price for {ccy_pair} on {oldest_date}: {prices_dict[oldest_date]}"")
newest_date = prices[""DATE""].max()
print(f""Closing price for {ccy_pair} on {newest_date}: {prices_dict[newest_date]}"")
prices.to_csv(output_dir / output_file, index=False)"
10031,Healthchecks - Perfom basic actions,"from naas_drivers import healthcheck
url = ""https://google.com""
key = ""123456-123456-12455""
healthcheck.connect(key)
healthcheck.send(""start"")
healthcheck.send()
healthcheck.send(""fail"")
healthcheck.check_up(url)"
10032,Google Sheets - Send LinkedIn invitations from spreadsheet,"import naas
from naas_drivers import linkedin, gsheet
spreadsheet_id = ""YOUR_SPREADSHEET_ID""
sheet_name = ""YOUR_SHEET_NAME""
profile_col_name = ""url""
# LinkedIn cookies
LI_AT = 'YOUR_COOKIE_LI_AT'
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'

# LinkedIn limit invitations up to 100 per week (Becareful !)
add_per_launch = 4 
# Scheduler your invitation everyday at 8:00 AM
naas.scheduler.add(cron=""0 8 * * *"")

# Uncomment the line below to delete your scheduler
# naas.scheduler.delete()
df = gsheet.connect(spreadsheet_id).get(sheet_name=sheet_name)
# Alert when last than 20 urls remains in the gsheet
if len(df) < 20:
    email_to = ""YOUR_EMAIL""
    subject = ""Invite LinkedIn alert : "" + str(len(df)) + "" lines left in the Linkedin's url database""
    content = ""You can add more lines to the gsheet or update the Notebook to set a new spreadsheet !""
    naas.notification.send(email_to=email_to, subject=subject, html=content)

df = df.head(add_per_launch)
print(""Invits will be send to :"")
df
for index, row in df.iterrows():
    profile = row[profile_col_name]
    result = linkedin.connect(LI_AT, JSESSIONID).invitation.send(recipient_url=profile)
    print(f""Invitation sent to : {profile}"")
    # Suppression de la ligne du gsheet
    gsheet.connect(spreadsheet_id).delete(sheet_name=sheet_name, rows=[2])"
10033,Google Sheets - Add items to Notion databases from new rows in,"import pandas as pd
import naas
from naas_drivers import notion, gsheet
# Enter spreadsheet_id and sheet name
spreadsheet_id = ""****""
sheet_name = ""YOUR_SHEET""

#Unique column# for gsheets
col_unique_gsheet = 'Name'
# Enter Notion Token API and Database URL
notion_token = ""****""
notion_database_url = ""https://www.notion.so/YOURDB""

#Unique column name for notion
col_unique_notion = 'Name'
#Schedule the notebook to run every 15 minutes
naas.scheduler.add(cron=""*/15 * * * *"")

# To delete your scheduler, uncomment the line below and execute the cell 
# naas.scheduler.delete()
#Connect with Gsheet and get all data in a dataframe
gsheet.connect(spreadsheet_id)
df_gsheet = gsheet.get(sheet_name=sheet_name)
#Connect with Notion db and get all pages in a dataframe
database = notion.connect(notion_token).database.get(notion_database_url)
df_notion = database.df()
#Iterate through all rows in Gsheet and find match in Notion db
#If no match is found then add data to df_difference dataframe

df_difference = pd.DataFrame()
for index,row in df_gsheet.iterrows():
    x = row[col_unique_gsheet]
    if not (x == df_notion[col_unique_notion]).any():
        df_difference = df_difference.append(df_gsheet.loc[index])
#Create a new page in notion db for each row in df_difference dataframe
if(df_difference.empty == False):    
    for index, row in df_difference.iterrows():
        page = notion.connect(notion_token).page.create(database_id=notion_database_url, title=row[col_unique_gsheet])
        #Add all properties here and map with respective column in row
        page.select('Type', row['Type'])
        page.select('Status', row['Status'])
        page.rich_text('Summary', row['Summary'])
        page.update()
    print(""The gsheets rows synced successfuly to Notion DB"")
else:     
    print(""No new rows in Gsheet to sync to Notion DB"")"
10034,Google Sheets - Send data to MongoDB,"from naas_drivers import mongo, gsheet
import pandas as pd
import naas
spreadsheet_id = ""------""
sheet_name = ""Sheet1""
user = ""your user""
passwd = ""your password""
host = ""Your Connection URL""
port = 9090

collection_name = ""COLLECTION NAME""
db_name = ""DATABASE NAME""
naas.scheduler.add(cron='0 9 * * *') #Send in production this notebook and run it, every day at 9:00.

# use this to delete your automation
#naas.scheduler.delete()
df = gsheet.connect(spreadsheet_id).get(sheet_name=sheet_name)
mongo.connect(host, port, user, passwd).send(df,
                                             collection_name,
                                             db_name,
                                             replace=True)"
10035,Google Sheets - Send data,"from naas_drivers import gsheet
data = [{ ""name"": ""Jean"", ""email"": ""jean@appleseed.com"" }, { ""name"": ""Bunny"", ""email"": ""bunny@appleseed.com"" }]
spreadsheet_id = ""id""
gsheet.connect(spreadsheet_id)
gsheet.send(
    sheet_name=""TSLA"",
    data=data
)"
10036,Google Sheets - Send emails from sheet,"from naas_drivers import gsheet
from naas_drivers import email
username = ""USERNAME""
password = ""PASSWORD""
email_from = ""***@cashstory.com"",
smtp_server = ""smtp.gmail.com"",
smtp_port = 465,
smtp_type = ""SSL"",
gmail = emails.connect(username,
                       password,
                       email_from,
                       smtp_server,
                       smtp_port,
                       smtp_type)
spreadsheet_id = ""1s-TQZrevbmveFKlx2H49fgvr_nZPEY_ffoi0iWal**E""
sheet_name = ""********""

df = gsheet.connect(spreadsheet_id).get(sheet_name)
df
emails = df['EMAIL'].drop_duplicates().values
print(emails)
subject = ""The tesla action is going up""
content = ""check in the link the chart data maide from fresh dataset : [LINK]""

for email in emails:
    print(email)
#     gmail.send(email_to=email, subject=subject, html=content)"
10037,Google Sheets - Get data,"from naas_drivers import gsheet
spreadsheet_id = ""---------""
gsheet.connect(spreadsheet_id)
gsheet.get(sheet_name=""TSLA"")"
10038,CCXT - Predict Bitcoin from Binance,"!pip install ccxt --user
import naas
import ccxt
import pandas as pd
from datetime import datetime
from naas_drivers import plotly, prediction
binance_api = """"
binance_secret = """"
symbol = 'BTC/USDT'
limit = 200
timeframe = '1d'
binance = ccxt.binance({
    'apiKey': binance_api,
    'secret': binance_secret
}) 

data = binance.fetch_ohlcv(symbol=symbol,
                           limit=limit,
                           timeframe=timeframe)
df = pd.DataFrame(data, columns=[""Date"",""Open"",""High"",""Low"",""Close"",""Volume""])
df['Date'] = [datetime.fromtimestamp(float(time)/1000) for time in df['Date']]
chart_candlestick = plotly.candlestick(df,
    label_x=""Date"", 
    label_open=""Open"", 
    label_high=""High"",
    label_low=""Low"",
    label_close=""Close""
)
df[f""MA{20}""] = df.Close.rolling(
                    20
                ).mean()
df[f""MA{50}""] = df.Close.rolling(
                    50
                ).mean()
pr = prediction.get(dataset=df)
chart_stock = plotly.stock(pr, kind=""linechart"")
chart_stock.update_layout(
    autosize=True,
    width=1300,
    height=800,
)"
10039,CCXT - Calculate Support and Resistance,"!pip install trendln matplotlib==3.1.3 --user
import naas
import ccxt
import pandas as pd
from datetime import datetime
import naas_drivers
import trendln
import plotly.tools as tls
import plotly.graph_objects as go
binance_api = """"
binance_secret = """"
symbol = 'BTC/USDT'
limit = 180
timeframe = '4h'
binance = ccxt.binance({
    'apiKey': binance_api,
    'secret': binance_secret
}) 

data = binance.fetch_ohlcv(symbol=symbol,
                           limit=limit,
                           timeframe=timeframe)
df = pd.DataFrame(data, columns=[""Date"", ""Open"", ""High"", ""Low"", ""Close"", ""Volume""])
df['Date'] = [datetime.fromtimestamp(float(time)/1000) for time in df['Date']]
df
fig = trendln.plot_support_resistance(
    df[-1000:].Close, #as per h for calc_support_resistance
    xformatter = None, #x-axis data formatter turning numeric indexes to display output
      # e.g. ticker.FuncFormatter(func) otherwise just display numeric indexes
    numbest = 1, #number of best support and best resistance lines to display
    fromwindows = True, #draw numbest best from each window, otherwise draw numbest across whole range
    pctbound = 0.1, # bound trend line based on this maximum percentage of the data range above the high or below the low
    extmethod = trendln.METHOD_NUMDIFF,
    method=trendln.METHOD_PROBHOUGH,
    window=125,
    errpct = 0.005,
    hough_prob_iter=50,
    sortError=False,
    accuracy=1)
plotly_fig = tls.mpl_to_plotly(fig)
layout = dict(
    dragmode=""pan"",
    xaxis_rangeslider_visible=False,
    showlegend=True,
)
new_data = list(plotly_fig.data)
new_data.pop(2)
new_data.pop(2)
new_data.pop(1)
new_data.pop(1)
fig = go.Figure(data=new_data, layout=layout)
fig"
10040,Dask - Parallelize operations on multiple csvs,"import os
try:
    import graphviz
except:
    !pip install --user graphviz
    import graphviz
try:
    import dask.dataframe as dd
except:
    !python -m pip install ""dask[complete]""
    import dask.dataframe as dd
folder_path = ""nycflights""

%env FOLDER_PATH=$folder_path
%%bash

[[ -f ""$FOLDER_PATH/nycflights.csv"" ]] || (mkdir -p $FOLDER_PATH && wget -O $FOLDER_PATH/nycflights.csv  https://github.com/vaibhavwalvekar/NYC-Flights-2013-Dataset-Analysis/raw/master/flights.csv )
# when the actual data types of given columns cannot be inferred from the first few examples
# they need to be specified manually
# this is where the dtype parameters comes in
df = dd.read_csv(os.path.join(folder_path, '*.csv'), 
                 parse_dates={'Date': [0, 1, 2]},
                 dtype={'TailNum': str,
                        'CRSElapsedTime': float,
                        'Cancelled': bool,
                        'dep_delay': float})
# no operation is actually performed until the .compute() function is called
df['dep_delay'].max().compute()
# the underlying task graph can be viewed to understand how the parallel execution takes place
df.dep_delay.max().visualize(rankdir=""LR"", size=""12, 12!"")
import pandas as pd
import glob
%%time
# the equivalent operation performed using Pandas
all_files = glob.glob(os.path.join(folder_path,'*.csv'))
dfs = []
for file in all_files:
    dfs.append(pd.read_csv(file, parse_dates={'Date': [0, 1, 2]}))
df = pd.concat(dfs, axis=0)
df.dep_delay.max()
%%time
# the entire operation again performed using Dask
df = dd.read_csv(os.path.join(folder_path,'*.csv'), 
                 parse_dates={'Date': [0, 1, 2]},
                 dtype={'TailNum': str,
                        'CRSElapsedTime': float,
                        'Cancelled': bool,
                       'dep_delay': float})
df['dep_delay'].max().compute()

# Dask clearly performs better in comparison to Pandas
# the performance benefits are more apparent when working on larger datasets
# especially when the size of the data exceeds available memory
"
10041,IFTTT - Post on Twitter,"from naas_drivers import ifttt
twitter_post = """"""Hello world, this is my first automated post 
                with @JupyterNaas @IFTTT driverüî•
                """"""
event = ""naas_demo""
key = ""ke4AigvXI5-EABaowdLt4fju1aOUxeMxSXQoN***""
data = { ""value1"": twitter_post }
result = ifttt.connect(key)
result = ifttt.send(event, data)"
10042,IFTTT - Trigger workflow,"from naas_drivers import ifttt
event = ""myevent""
key = ""cl9U-VaeBu1**********""
data = { ""value1"": ""Bryan"", ""value2"": ""Helmig"", ""value3"": 27 }
result = ifttt.connect(key)
result = ifttt.send(event, data)"
10043,Python - Consolidate Excel files,"import os
import pandas as pd
# Output
excel_output = 'concatenate.xlsx'
files = os.listdir()
my_list = []
for file in files:
    if file.endswith('.xlsx'):
        df = pd.read_excel(file)
        my_list.append(df)

df_concat = pd.concat(my_list, axis=0)
df_concat.to_excel(excel_output, index=False)"
10044,Python - Create dict from lists,"keys = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]
value = [219, 146, 112, 127, 124, 180, 236, 207, 236, 263, 350, 430, 474, 526, 488, 537, 500, 439]
# Call zip(iter1, iter2) with one list as iter1 and another list as iter2 to create a zip iterator containing pairs of elements from the two lists.
zip_iterators = zip(keys, value)
# Use dict() to convert this zip iterator to a dictionary of key-value pairs.
dict(zip_iterators)"
10045,Python - Looping Over Dataframe,"import pandas as pd
import numpy as np
dict1 = {
    ""student_id"": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    ""student_name"": [""Peter"", ""Dolly"", ""Maggie"", ""David"", ""Isabelle"", ""Harry"", ""Akin"", ""Abbey"", ""Victoria"", ""Sam""],
    ""student_course"": np.random.choice([""Biology"", ""Physics"", ""Chemistry""], size=10)
}
data = pd.DataFrame(dict1)
data
for column in data:
    print(column)
for k, v in data.iteritems():
    print(k)
    print(v)
for k, v in data.iterrows():
    print(k)
    print(v)
for row in data.itertuples():
    print(row)"
10046,Python - Using datetime library,"from datetime import datetime
# Date string format
date_string = ""2022-02-25""

# Your date string format
current_format = ""%Y-%m-%d""

# New date format you want to use
new_format = ""%Y-W%U""
date_datetime = datetime.strptime(date_string, current_format)
date_datetime
new_date = date_datetime.strftime(new_format)
new_date
new_date = datetime.strptime(date_string, current_format).strftime(new_format)
new_date
new_date"
10047,Python - Create dataframe from lists,"import pandas as pd
# Setup your columns name
col_key = ""KEYS""
col_value = ""VALUE""
keys = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]
value = [219, 146, 112, 127, 124, 180, 236, 207, 236, 263, 350, 430, 474, 526, 488, 537, 500, 439]
# Call zip(iter1, iter2) with one list as iter1 and another list as iter2 to create a zip iterator containing pairs of elements from the two lists.
zip_iterators = zip(keys, value)
df = pd.DataFrame(zip_iterators, columns=[col_key, col_value])
df
"
10048,Python - Download PDF from URL,"import urllib
# Input
pdf_path = ""https://s22.q4cdn.com/959853165/files/doc_financials/2021/q4/da27d24b-9358-4b5c-a424-6da061d91836.pdf""

# Output
pdf_output = ""2021 Annual Report.pdf""
def download_pdf(url, filepath):
    response = urllib.request.urlopen(url)    
    file = open(filepath, 'wb')
    file.write(response.read())
    file.close()
    print(""File saved:"", filepath) 
download_pdf(pdf_path, pdf_output)"
10049,Newsapi - Send emails briefs,"from naas_drivers import newsapi, emailbuilder
import naas
# Input
query = ""data, automation, AI"" #newsapi query

# Outputs
your_email = ""*********""
email_subject = ""News scheduled from Naas dev""
email_from = 'notifications@naas.ai'
naas.scheduler.add(recurrence=""0 8 * * *"")

# Uncomment the line below and run the cell to delete your scheduler
# naas.scheduler.delete()
table_news = newsapi.connect().get(q=query)

# rename columns match the field required by Naas emailbuilder drivers
table_news.rename(columns={'title':'text'}, inplace=True)
table_news.rename(columns={'link':'row_link'}, inplace=True)
table_news_email = table_news[:10]
table_news_email = table_news_email[['text','row_link']]
table_news_email
links = []
ht_str = ""<ul>""
for i in range(len(table_news_email)):
    val = ""<li>""+""<a href=""+'""'+table_news_email['row_link'][i]+'""'+"">""+table_news_email['text'][i]+""</a>""+""</li>""
    ht_str = ht_str+'\n'+val
ht_str = ht_str+""\n""+""</ul>"" 
email_content = emailbuilder.generate( 
        display='iframe',
        title=f'üåè NewsAPI brief', 
        subtitle=f'<b>Topics</b>: {query}',         
        table_1= ht_str,
        text=""Source: <a>https://newsapi.org/</a>""
        )
naas.notification.send(email_to=your_email,
                       subject=email_subject,
                       html=email_content,
                       email_from=email_from)"
10050,Newsapi - Get data,"from naas_drivers import newsapi
fields = [""image"", ""title""]
COMPANY = ""TSLA""
newsapi.get(COMPANY, fields=fields)"
10051,Newsapi - Run sentiment analysis,"from naas_drivers import newsapi, sentiment
data = newsapi.connect().get(""bitcoin"", fields=[""title"", ""image"", ""link"", ""description""])
sentiment = sentiment.get(data, column_name=""title"")
data
sentiment"
10052,YahooFinance - Candlestick chart,"import datetime as dt
import pandas_datareader as pdr
try:
    import mplfinance as mpf
except:
    !pip install mplfinance
    import mplfinance as mpf
try:
    import yfinance as yfin
except:
    !pip install yfinance
    import yfinance as yfin
# update date range ""start"" as desired
start = dt.datetime(2021, 1, 1)
end = dt.datetime.now()
ticker = ""ETH-USD""
# insert cryptoasset (in Yahoo format ""NNN-CCC"" es BTC-EUR or ETH-USD) here
yfin.pdr_override()
data = yfin.download(ticker, start, end)
data
mpf.plot(data, title=f""{ticker} trend"",
         type=""candle"",
         volume=True,
         style=""yahoo"")
"
10053,YahooFinance - Display chart from ticker,"from naas_drivers import yahoofinance, plotly
ticker = ""TSLA""
date_from = -365
date_to = ""today""
interval = '1d'
moving_averages = [20, 50]
df_yahoo = yahoofinance.get(ticker,
                            date_from=date_from,
                            date_to=date_to,
                            interval=interval,
                            moving_averages=moving_averages)
chart = plotly.linechart(df_yahoo,
                         x=""Date"",
                         y=[""Close"", ""MA20"", ""MA50""],
                         showlegend=True,
                         title=f""{ticker} stock as of today"")"
10054,YahooFinance - Send daily prediction to Notion,"import naas
from naas_drivers import prediction, yahoofinance, plotly, notion
from datetime import datetime
from naas_drivers.tools.notion import Link, BlockEmbed
import pytz
TICKER = ""TSLA""
date_from = -100 # 1OO days max to feed the naas_driver for prediction
date_to = ""today""
DATA_POINT = 20
# Credentials
NOTION_TOKEN = ""YOUR_NOTION_TOKEN""
DATABASE_URL = ""https://www.notion.so/naas-official/f42d6592949axxxxxxxxxxxxx""

# Setup your page title, this will be the key to update your data
PAGE_TITLE = ""Tesla stock""
NOW = datetime.now().strftime(""%Y-%m-%d"")
excel_output = f""{TICKER}_{NOW}.xlsx""
image_output = f""{TICKER}.png""
html_output = f""{TICKER}.html""
naas.scheduler.add(cron=""0 9 * * *"")

# if you want to delete the scheduler, uncoment the line below and execute the cell
# naas.scheduler.delete() 
df_yahoo = yahoofinance.get(tickers=TICKER,
                            date_from=date_from,
                            date_to=date_to).dropna().reset_index(drop=True)

# Display dataframe
df_yahoo.tail(5)
df_predict = prediction.get(dataset=df_yahoo,
                            date_column='Date',
                            column=""Close"",
                            data_points=DATA_POINT,
                            prediction_type=""all"").sort_values(""Date"", ascending=False).reset_index(drop=True)
# Display dataframe
df_predict.head(int(DATA_POINT)+5)
fig = plotly.linechart(df_predict,
                       x=""Date"",
                       y=[""Close"", ""ARIMA"", ""SVR"", ""LINEAR"", ""COMPOUND""],
                       showlegend=True,
                       title=f""{TICKER} predictions as of today, for next {str(DATA_POINT)} days."")
def get_variation(df):
    df = df.sort_values(""Date"", ascending=False).reset_index(drop=True)
    
    # Get value and value comp
    datanow = df.loc[0, ""Close""]
    datayesterday = df.loc[1, ""Close""]
    
    # Calc variation en value and %
    varv = datanow - datayesterday
    varp = (varv / datanow)
    return datanow, datayesterday, varv, varp

DATANOW, DATAYESTERDAY, VARV, VARP = get_variation(df_yahoo)
print(""Value today:"", DATANOW)
print(""Value yesterday:"", DATAYESTERDAY)
print(""Var. in value:"", VARV)
print(""Var. in %:"", VARP)
df_predict.to_excel(excel_output)

# Share output with naas
link_excel = naas.asset.add(excel_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(excel_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
link_image = naas.asset.add(image_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
link_html = naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
def update_notion_db(database_url, title, value=0, varv=0, varp=0, image_link=None, html_link=None, excel_link=None):
    # Decode database id
    database_id = database_url.split(""/"")[-1].split(""?v="")[0]
    
    # Get pages from notion database
    pages = notion.connect(NOTION_TOKEN).database.query(database_id, query={})
    
    # Create or update page
    page_new = True
    for page in pages:
        page_temp = page.df()
        page_id = page_temp.loc[page_temp.Name == ""Name"", ""Value""].values[0]
        if page_id == title:
            page_new = False
            break
    try:
        if page_new:
            page = notion.connect(NOTION_TOKEN).Page.new(database_id=database_id).create()
            page.title(""Name"", title)
            
        # Check if image already exists
        blocks = page.get_blocks()
        for block in blocks:
            content_block = getattr(block, block.type)
            if block.type == ""image"":
                image_url = block.image.external.url
                if image_url == image_link:
                    image_link = None
            if block.type == ""paragraph"":
                if len(block.paragraph.text) > 0:
                    text = block.paragraph.text[0].text.content
                    if text == ""Open dynamic graph"":
                        html_link = None
                    if text == ""Download Excel"":
                        excel_link = None
        if image_link:
            page.image(image_link)
        if html_link:
            res = page.paragraph(""Open dynamic graph"")
            res.paragraph.text[0].href = html_link
            res.paragraph.text[0].text.link = Link(html_link)
        if excel_link:
            res = page.paragraph(""Download Excel"")
            res.paragraph.text[0].href = excel_link
            res.paragraph.text[0].text.link = Link(excel_link)
                                    
        # Update dynamic properties
        page.select(""Status"", ""OK"")
        page.number(""Value"", round(float(value), 0))
        page.number(""Var (value)"", round(float(varv), 0))
        page.number(""Var (%)"", round(float(varp), 4))
        page.date(""Updated at"", datetime.now(pytz.timezone(""Europe/Paris"")).strftime(""%Y-%m-%d %H:%M:%S%z""))

        # Create page in Notion
        page.update()
        print(f""‚úÖ Page '{title}' updated in Notion."")
    except Exception as e:
        print(f""‚ùå Error updating {title}"")
        return e
        
update_notion_db(DATABASE_URL, PAGE_TITLE, DATANOW, VARV, VARP, link_image, link_html, link_excel)
"
10055,YahooFinance - Send daily prediction to Email,"import naas
from naas_drivers import prediction, yahoofinance, plotly
import markdown2
from datetime import datetime
from IPython.core.display import display, HTML
TICKER = ""TSLA""
date_from = -100 # 1OO days max to feed the naas_driver for prediction
date_to = ""today""
DATA_POINT = 20
# Naas notification parameters
EMAIL_TO = ""YOUR_EMAIL""
EMAIL_FROM = None
EMAIL_SUBJECT = f""üìà {TICKER} predictions as of today""

# Markdown template created on your local env
EMAIL_CONTENT_MD = ""email_content.md""
NOW = datetime.now().strftime(""%Y-%m-%d"")
excel_output = f""{TICKER}_{NOW}.xlsx""
image_output = f""{TICKER}.png""
html_output = f""{TICKER}.html""
naas.scheduler.add(cron=""0 9 * * *"")

# if you want to delete the scheduler, uncoment the line below and execute the cell
#naas.scheduler.delete() 
df_yahoo = yahoofinance.get(tickers=TICKER,
                            date_from=date_from,
                            date_to=date_to).dropna().reset_index(drop=True)

# Display dataframe
df_yahoo.tail(5)
df_predict = prediction.get(dataset=df_yahoo,
                            date_column='Date',
                            column=""Close"",
                            data_points=DATA_POINT,
                            prediction_type=""all"").sort_values(""Date"", ascending=False).reset_index(drop=True)
# Display dataframe
df_predict.head(int(DATA_POINT)+5)
fig = plotly.linechart(df_predict,
                       x=""Date"",
                       y=[""Close"", ""ARIMA"", ""SVR"", ""LINEAR"", ""COMPOUND""],
                       showlegend=True,
                       title=f""{TICKER} predictions as of today, for next {str(DATA_POINT)} days."")
def get_variation(df):
    df = df.sort_values(""Date"", ascending=False).reset_index(drop=True)
    
    # Get value and value comp
    datanow = df.loc[0, ""Close""]
    datayesterday = df.loc[1, ""Close""]
    
    # Calc variation en value and %
    varv = datanow - datayesterday
    varp = (varv / datanow)
    
    # Format result
    datanow = ""${:,.2f}"".format(round(datanow, 1))
    datayesterday = ""${:,.2f}"".format(round(datayesterday, 1))
    varv = ""{:+,.2f}"".format(varv)
    varp = ""{:+,.2%}"".format(varp)
    return datanow, datayesterday, varv, varp

DATANOW, DATAYESTERDAY, VARV, VARP = get_variation(df_yahoo)
print(""Value today:"", DATANOW)
print(""Value yesterday:"", DATAYESTERDAY)
print(""Var. in value:"", VARV)
print(""Var. in %:"", VARP)
def get_prediction(df, prediction):
    data = df.loc[0, prediction]
    
    # Format result
    data = ""${:,.2f}"".format(round(data, 1))
    return data

ARIMA = get_prediction(df_predict, ""ARIMA"")
print(""Value ARIMA:"", ARIMA)
SVR = get_prediction(df_predict, ""SVR"")
print(""Value SVR:"", SVR)
LINEAR = get_prediction(df_predict, ""LINEAR"")
print(""Value LINEAR:"", LINEAR)
COMPOUND = get_prediction(df_predict, ""COMPOUND"")
print(""Value COMPOUND:"", COMPOUND)
df_predict.to_excel(excel_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
link_html = naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
link_image = naas.asset.add(image_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)
link_webhook = naas.webhook.add()
%%writefile $EMAIL_CONTENT_MD
Hello world,

The **TICKER** price is **DATANOW** right now, VARV vs yesterday (VARP).<br>
Yesterday close : DATAYESTERDAY

In +DATA_POINT days, basic ML models predict the following prices: 

- **arima**: ARIMA
- **svr**: SVR
- **linear**: LINEAR
- **compound**: COMPOUND
    
<img href=link_html target=""_blank"" src=link_image style=""width:640px; height:360px;"" /><br>
[Open dynamic chart](link_html)<br>

Please find attached the data in Excel.<br>

Have a nice day.
<br>

PS: You can [send the email again](link_webhook) if you need a fresh update.<br>
<div><strong>Full Name</strong></div>
<div>Open source lover | <a href=""http://www.naas.ai/"" target=""_blank"">Naas</a></div>
<div>+ 33 1 23 45 67 89</div>
<div><small>This is an automated email from my Naas account</small></div>
def replace_value(md):
    post = md.replace(""DATANOW"", str(DATANOW))
    post = post.replace(""TICKER"", str(TICKER))
    post = post.replace(""DATAYESTERDAY"", str(DATAYESTERDAY))
    post = post.replace(""VARV"", str(VARV))
    post = post.replace(""VARP"", str(VARP))
    post = post.replace(""LINEAR"", str(LINEAR))
    post = post.replace(""SVR"", str(SVR))
    post = post.replace(""COMPOUND"", str(COMPOUND))
    post = post.replace(""ARIMA"", str(ARIMA))
    post = post.replace(""DATA_POINT"", str(DATA_POINT))
    post = post.replace(""link_image"", str(link_image))
    post = post.replace(""link_html"", str(link_html))
    post = post.replace(""link_webhook"", str(link_webhook))
    return post
content = open(EMAIL_CONTENT_MD, ""r"").read()
md = markdown2.markdown(content)
email_content = replace_value(md)
display(HTML(email_content))
naas.notification.send(email_to=EMAIL_TO,
                       subject=EMAIL_SUBJECT,
                       html=email_content,
                       files=[excel_output],
                       email_from=EMAIL_FROM)"
10056,YahooFinance - Cryptocurrencies heatmap correlation graph,"import datetime as dt
import matplotlib.pyplot as plt
try:
    import seaborn as sns
except:
    !pip install seaborn
    import seaborn as sns
try:
    import yfinance as yfin
except:
    !pip install yfinance
    import yfinance as yfin
# user settings (modify accordingly to Yahoo Finance parameters)
currency = ""USD""
metric = ""Close""

# Date
start = dt.datetime(2018,1,1)
end = dt.datetime.now()
# pick your favorite list of cryptocurrencies
crypto = ['BTC', 'ETH', 'LTC', 'XRP', 'DASH', 'SC']
yfin.pdr_override()

colnames = []

first = True

for ticker in crypto:
    data = yfin.download(f""{ticker}-{currency}"", start, end)
    if first:
        combined = data[[metric]].copy()
        colnames.append(ticker)
        combined.columns = colnames
        first = False
    else:
        combined = combined.join(data[metric])
        colnames.append(ticker)
        combined.columns = colnames
        
combined
plt.yscale('log') # first show linear
for ticker in crypto:
    plt.plot(combined[ticker], label=ticker)
    
plt.tick_params(axis=""x"", width = 2)
plt.xticks(rotation = ""vertical"", )
plt.margins(0.01)
plt.subplots_adjust(bottom = 0.15)
plt.legend(loc='lower center', bbox_to_anchor=(0.5, 1.05),
          ncol=6, fancybox=True, shadow=False)
plt.show()

# Correlation Heat Map
combined = combined.pct_change().corr(method='pearson')

sns.heatmap(combined, annot=True, cmap=""coolwarm"")
plt.show()
print(combined)"
10057,YahooFinance - Get data from ticker,"from naas_drivers import yahoofinance
ticker = ""TSLA""
date_from = -100
date_to = 'today'
interval = '1d'
moving_averages = [20, 50]
df_yahoo = yahoofinance.get(ticker,
                            date_from=date_from,
                            date_to=date_to,
                            interval=interval,
                            moving_averages=moving_averages)
df_yahoo"
10058,YahooFinance - Send daily prediction to Slack,"import naas
from naas_drivers import prediction, yahoofinance, plotly, slack
import markdown2
from datetime import datetime
import naas
TICKER = ""TSLA""
date_from = -100 # 1OO days max to feed the naas_driver for prediction
date_to = ""today""
DATA_POINT = 20
SLACK_TOKEN = ""xoxb-XXXXXXX""
SLACK_CHANNEL = ""demo-naas""

# Markdown template created on your local env
SLACK_CONTENT_MD = ""slack_content.md""
NOW = datetime.now().strftime(""%Y-%m-%d"")
excel_output = f""{TICKER}_{NOW}.xlsx""
image_output = f""{TICKER}.png""
html_output = f""{TICKER}.html""
naas.scheduler.add(cron=""0 9 * * *"")

# if you want to delete the scheduler, uncoment the line below and execute the cell
# naas.scheduler.delete() 
df_yahoo = yahoofinance.get(tickers=TICKER,
                            date_from=date_from,
                            date_to=date_to).dropna().reset_index(drop=True)

# Display dataframe
df_yahoo.tail(5)
df_predict = prediction.get(dataset=df_yahoo,
                            date_column='Date',
                            column=""Close"",
                            data_points=DATA_POINT,
                            prediction_type=""all"").sort_values(""Date"", ascending=False).reset_index(drop=True)
# Display dataframe
df_predict.head(int(DATA_POINT)+5)
fig = plotly.linechart(df_predict,
                       x=""Date"",
                       y=[""Close"", ""ARIMA"", ""SVR"", ""LINEAR"", ""COMPOUND""],
                       showlegend=True,
                       title=f""{TICKER} predictions as of today, for next {str(DATA_POINT)} days."")
def get_variation(df):
    df = df.sort_values(""Date"", ascending=False).reset_index(drop=True)
    
    # Get value and value comp
    datanow = df.loc[0, ""Close""]
    datayesterday = df.loc[1, ""Close""]
    
    # Calc variation en value and %
    varv = datanow - datayesterday
    varp = (varv / datanow)
    
    # Format result
    datanow = ""${:,.2f}"".format(round(datanow, 1))
    datayesterday = ""${:,.2f}"".format(round(datayesterday, 1))
    varv = ""{:+,.2f}"".format(varv)
    varp = ""{:+,.2%}"".format(varp)
    return datanow, datayesterday, varv, varp

DATANOW, DATAYESTERDAY, VARV, VARP = get_variation(df_yahoo)
print(""Value today:"", DATANOW)
print(""Value yesterday:"", DATAYESTERDAY)
print(""Var. in value:"", VARV)
print(""Var. in %:"", VARP)
def get_prediction(df, prediction):
    data = df.loc[0, prediction]
    
    # Format result
    data = ""${:,.2f}"".format(round(data, 1))
    return data

ARIMA = get_prediction(df_predict, ""ARIMA"")
print(""Value ARIMA:"", ARIMA)
SVR = get_prediction(df_predict, ""SVR"")
print(""Value SVR:"", SVR)
LINEAR = get_prediction(df_predict, ""LINEAR"")
print(""Value LINEAR:"", LINEAR)
COMPOUND = get_prediction(df_predict, ""COMPOUND"")
print(""Value COMPOUND:"", COMPOUND)
df_predict.to_excel(excel_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
link_html = naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
link_image = naas.asset.add(image_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)
%%writefile $SLACK_CONTENT_MD
Hey <!here>

The *TICKER* price is *DATANOW* right now, VARV vs yesterday (VARP).
Yesterday close : DATAYESTERDAY

In +DATA_POINT days, basic ML models predict the following prices: 

- *arima*: ARIMA
- *svr*: SVR
- *linear*: LINEAR
- *compound*: COMPOUND

<link_html|Open dynamic chart>
def replace_value(md):
    post = md.replace(""DATANOW"", str(DATANOW))
    post = post.replace(""TICKER"", str(TICKER))
    post = post.replace(""DATAYESTERDAY"", str(DATAYESTERDAY))
    post = post.replace(""VARV"", str(VARV))
    post = post.replace(""VARP"", str(VARP))
    post = post.replace(""LINEAR"", str(LINEAR))
    post = post.replace(""SVR"", str(SVR))
    post = post.replace(""COMPOUND"", str(COMPOUND))
    post = post.replace(""ARIMA"", str(ARIMA))
    post = post.replace(""DATA_POINT"", str(DATA_POINT))
    post = post.replace(""link_image"", str(link_image))
    post = post.replace(""link_html"", str(link_html))
    return post
content = open(SLACK_CONTENT_MD, ""r"").read()
slack_message = replace_value(content)
slack_message
slack.connect(SLACK_TOKEN).send(
    SLACK_CHANNEL,
    slack_message,
)"
10059,YahooFinance - Get USDEUR data and chart,"from naas_drivers import yahoofinance, plotly
ticker = ""EURUSD=X""
date_from = -365
date_to = ""today""
interval = '1d'
moving_averages = [20, 50]
df_yahoo = yahoofinance.get(ticker,
                            date_from=date_from,
                            date_to=date_to,
                            interval=interval,
                            moving_averages=moving_averages)
df_yahoo
# Get last value
last_date = df_yahoo.loc[df_yahoo.index[-1], ""Date""].strftime(""%Y-%m-%d"")
last_value = df_yahoo.loc[df_yahoo.index[-1], ""Close""]

# Create chart
chart = plotly.linechart(df_yahoo,
                         x=""Date"",
                         y=[""Close"", ""MA20"", ""MA50""],
                         showlegend=True,
                         title=f""<b>{ticker} rate as of {last_date}</b><br><span style='font-size: 13px;'>Last value: {last_value}</span>"")

chart.update_layout(
    title_font=dict(family=""Arial"", size=18, color=""black""),
    legend_font=dict(family=""Arial"", size=11, color=""black""),
    margin_pad=10,
)"
10060,YahooFinance - Get Stock Update,"import naas 
from naas_drivers import yahoofinance, plotly
import markdown2
from IPython.display import Markdown as md
TICKER = 'INR=X'
date_from = -30
date_to = 'today'
email_to = [""template@naas.ai""]
email_from = None
#data cleaning
df = yahoofinance.get(TICKER, date_from=date_from, date_to = date_to)
df = df.dropna()# drop the na values from the dataframe
df.reset_index(drop=True)
df = df.sort_values(""Date"", ascending=False).reset_index(drop=True)
df.head()
LASTOPEN = round(df.loc[0, ""Open""], 2)
LASTCLOSE = round(df.loc[0, ""Close""], 2)
YESTERDAYOPEN = round(df.loc[1, ""Open""], 2)
YESTERDAYCLOSE = round(df.loc[1, ""Close""], 2)
MAXRATE = round(df['Open'].max(),2)
MXDATEOPEN = df.loc[df['Open'].idxmax(), ""Date""].strftime(""%Y-%m-%d"")
MINRATE = round(df['Open'].min(),2)
MNDATEOPEN = df.loc[df['Open'].idxmin(), ""Date""].strftime(""%Y-%m-%d"")
last_date = df.loc[df.index[0], ""Date""].strftime(""%Y-%m-%d"")

output = plotly.linechart(df,
                          x=""Date"",
                          y=['Open','Close'],
                          title=f""<b>INR USD rates of last month</b><br><span style='font-size: 13px;'>Last value as of {last_date}: Open={LASTOPEN}, Close={LASTCLOSE}</span>"")
df.to_csv(f""{TICKER}_LastMonth.csv"", index=False)
%%writefile message.md
Hello world,

The **TICKER** price is Open LASTOPEN and Close LASTCLOSE right now. <br>
**Yesterday Open**: YESTERDAYOPEN <br>
**Yesterday Close**: YESTERDAYCLOSE <br>    
The Max Open rate of **TICKER** was on MXDATEOPEN which was MAXRATE. <br>
The Min Open rate of **TICKER** was on MNDATEOPEN which was MINRATE. <br>

Attached is the excel file for your reference. <br>

Have a nice day.
<br>

PS: You can [send the email again](link_webhook) if you need a fresh update.<br>
<div><strong>Full Name</strong></div>
<div>Open source lover | <a href=""http://www.naas.ai/"" target=""_blank"">Naas</a></div>
<div>+ 33 1 23 45 67 89</div>
<div><small>This is an automated email from my Naas account</small></div>
naas.dependency.add(""message.md"")
markdown_file = ""message.md""
content = open(markdown_file, ""r"").read()
md = markdown2.markdown(content)
md
post = md.replace(""LASTOPEN"", str(LASTOPEN))
post = post.replace(""LASTCLOSE"", str(LASTCLOSE))
post = post.replace(""YESTERDAYOPEN"", str(YESTERDAYOPEN))
post = post.replace(""YESTERDAYCLOSE"", str(YESTERDAYCLOSE))
post = post.replace(""MXDATEOPEN"", str(MXDATEOPEN))
post = post.replace(""MAXRATE"", str(MAXRATE))
post = post.replace(""MNDATEOPEN"", str(MNDATEOPEN))
post = post.replace(""MINRATE"", str(MINRATE))
post = post.replace(""TICKER"", str(TICKER))
post
link_webhook = naas.webhook.add()
subject = f""üìà {TICKER} Open and close rates as of today""
content = post
files = [f""{TICKER}_LastMonth.csv""]

naas.notification.send(email_to=email_to,
                       subject=subject,
                       html=content,
                       email_from=email_from,
                       files=files)
# import naas
# naas.scheduler.add(""0 8 1-5 * *"")"
10061,FEC - Creer un dashboard PowerBI,"import pandas as pd
from datetime import datetime, timedelta
import os
import re
import naas
LOGO = ""https://landen.imgix.net/e5hx7wyzf53f/assets/26u7xg7u.png?w=400""
COLOR_1 = None
COLOR_2 = None
def get_all_fec(file_regex,
                sep="","",
                decimal=""."",
                encoding=None,
                header=None,
                usecols=None,
                names=None,
                dtype=None):
    # Create df init
    df = pd.DataFrame()

    # Get all files in INPUT_FOLDER
    files = [f for f in os.listdir() if re.search(file_regex, f)]
    if len(files) == 0:
        print(f""Aucun fichier FEC ne correspond au standard de nomination"")
    else:
        for file in files:
            # Open file and create df
            print(file)
            tmp_df = pd.read_csv(file,
                                 sep=sep,
                                 decimal=decimal,
                                 encoding=encoding,
                                 header=header,
                                 usecols=usecols,
                                 names=names,
                                 dtype=dtype)
            # Add filename to df
            tmp_df['NOM_FICHIER'] = file

            # Concat df
            df = pd.concat([df, tmp_df], axis=0, sort=False)
    return df
file_regex = ""^\d{9}FEC\d{8}.txt""

db_init = get_all_fec(file_regex,
                      sep='\t',
                      decimal=',',
                      encoding='ISO-8859-1',
                      header=0)
db_init
db_clean = db_init.copy()

# Selection des colonnes √† conserver
to_select = ['NOM_FICHIER',
             'EcritureDate',
             'CompteNum',
             'CompteLib',
             'EcritureLib',
             'Debit',
             'Credit']
db_clean = db_clean[to_select]

# Renommage des colonnes
to_rename = {'EcritureDate': ""DATE"", 
             'CompteNum': ""COMPTE_NUM"", 
             'CompteLib': ""RUBRIQUE_N3"", 
             'EcritureLib': ""RUBRIQUE_N4"", 
             'Debit': ""DEBIT"", 
             'Credit': ""CREDIT""}
db_clean = db_clean.rename(columns=to_rename)

#suppression des espaces colonne ""COMPTE_NUM""
db_clean[""COMPTE_NUM""] = db_clean[""COMPTE_NUM""].astype(str).str.strip()

# Mise au format des colonnes
db_clean = db_clean.astype({""NOM_FICHIER"" : str,
                            ""DATE"" : str,
                            ""COMPTE_NUM"" : str,
                            ""RUBRIQUE_N3"" : str,
                            ""RUBRIQUE_N4"" : str,
                            ""DEBIT"" : float,
                            ""CREDIT"" : float,
                            })

# Mise au format colonne date
db_clean[""DATE""] = pd.to_datetime(db_clean[""DATE""])

db_clean.head(5)
db_enr = db_clean.copy()

# Ajout colonnes entit√© et p√©riode
db_enr['ENTITY'] = db_enr['NOM_FICHIER'].str[:9]
db_enr['PERIOD'] = db_enr['NOM_FICHIER'].str[12:-6]
db_enr['PERIOD'] = pd.to_datetime(db_enr['PERIOD'], format='%Y%m')
db_enr['PERIOD'] = db_enr['PERIOD'].dt.strftime(""%Y-%m"")

# Ajout colonne month et month_index
db_enr['MONTH'] = db_enr['DATE'].dt.strftime(""%b"")
db_enr['MONTH_INDEX'] = db_enr['DATE'].dt.month

# Calcul de la valeur debit-cr√©dit
db_enr[""VALUE""] = (db_enr[""DEBIT""]) - (db_enr[""CREDIT""])

db_enr.head(5)
# Calcul r√©sultat pour √©quilibrage bilan dans capitaux propre
db_rn = db_enr.copy()

db_rn = db_rn[db_rn['COMPTE_NUM'].str.contains(r'^6|^7')]

to_group = [""ENTITY"", ""PERIOD""]
to_agg = {""VALUE"": ""sum""}
db_rn = db_rn.groupby(to_group, as_index=False).agg(to_agg)

db_rn [""COMPTE_NUM""] = ""10999999""
db_rn [""RUBRIQUE_N3""] = ""RESULTAT""

# Reorganisation colonne
to_select = ['ENTITY',
             'PERIOD',
             'COMPTE_NUM',
             'RUBRIQUE_N3',
             'VALUE']
db_rn = db_rn[to_select]
db_rn
# Calcul var v = cr√©ation de dataset avec Period_comp pour merge
db_var = db_enr.copy()

# Regroupement 
to_group = [""ENTITY"",
            ""PERIOD"",
            ""COMPTE_NUM"",
            ""RUBRIQUE_N3""]
to_agg = {""VALUE"": ""sum""}
db_var = db_var.groupby(to_group, as_index=False).agg(to_agg)

# Ajout des r√©sultats au dataframe
db_var = pd.concat([db_var, db_rn], axis=0, sort=False)

# Creation colonne COMP
db_var['PERIOD_COMP'] = (db_var['PERIOD'].str[:4].astype(int) - 1).astype(str) + db_var['PERIOD'].str[-3:]

db_var
db_comp = db_var.copy()

# Suppression de la colonne p√©riode
db_comp = db_comp.drop(""PERIOD_COMP"", axis=1)

# Renommage des colonnes
to_rename = {'VALUE': ""VALUE_N-1"", 
             'PERIOD': ""PERIOD_COMP""}
db_comp = db_comp.rename(columns=to_rename)

db_comp.head(5)
# Jointure entre les 2 tables
join_on = [""ENTITY"",
           ""PERIOD_COMP"",
           ""COMPTE_NUM"",
           ""RUBRIQUE_N3""]
db_var = pd.merge(db_var, db_comp, how='left', on=join_on).drop(""PERIOD_COMP"", axis=1).fillna(0)

#Cr√©ation colonne Var V
db_var[""VARV""] = db_var[""VALUE""] - db_var[""VALUE_N-1""]

#Cr√©ation colonne Var P (%)
db_var[""VARP""] = db_var[""VARV""] / db_var[""VALUE_N-1""]

db_var
db_cat = db_var.copy()

# Calcul des rubriques niveau 2
def rubrique_N2(row): 
        numero_compte = str(row[""COMPTE_NUM""])
        value = float(row[""VALUE""])
        
# BILAN SIMPLIFIE type IFRS NIV2

        to_check = [""^10"", ""^11"", ""^12"", ""^13"", ""^14""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""CAPITAUX_PROPRES""
        
        to_check = [""^15"", ""^16"", ""^17"", ""^18"", ""^19""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""DETTES_FINANCIERES""
        
        to_check = [""^20"", ""^21"", ""^22"", ""^23"", ""^25"", ""^26"", ""^27"", ""^28"", ""^29""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""IMMOBILISATIONS""
        
        to_check = [""^31"", ""^32"", ""^33"", ""^34"", ""^35"", ""^36"", ""^37"", ""^38"", ""^39""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""STOCKS""
        
        to_check = [""^40""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""DETTES_FOURNISSEURS""
        
        to_check = [""^41""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""CREANCES_CLIENTS""
        
        to_check = [""^42"", ""^43"", ""^44"", ""^45"", ""^46"", ""^47"", ""^48"", ""^49""]
        if any (re.search(x,numero_compte) for x in to_check):
            if value > 0:
                return ""AUTRES_CREANCES""
            else:
                return ""AUTRES_DETTES""
        
        to_check = [""^50"", ""^51"", ""^52"", ""^53"", ""^54"", ""^58"", ""^59""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""DISPONIBILITES""

        
# COMPTE DE RESULTAT DETAILLE NIV2

        to_check = [""^60""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""ACHATS""
        
        to_check= [""^61"", ""^62""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""SERVICES_EXTERIEURS""
        
        to_check = [""^63""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""TAXES""        
    
        to_check = [""^64""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""CHARGES_PERSONNEL""    
    
        to_check = [""^65""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""AUTRES_CHARGES""  
    
        to_check = [""^66""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""CHARGES_FINANCIERES""
        
        to_check = [""^67""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""CHARGES_EXCEPTIONNELLES""
        
        to_check = [""^68"", ""^78""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""AMORTISSEMENTS""
        
        to_check = [""^69""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""IMPOT""
        
        to_check = [""^70""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""VENTES""
        
        to_check = [""^71"", ""^72""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""PRODUCTION_STOCKEE_IMMOBILISEE""
        
        to_check = [""^74""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""SUBVENTIONS_D'EXPL.""
        
        to_check = [""^75"", ""^791""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""AUTRES_PRODUITS_GESTION_COURANTE""
        
        to_check = [""^76"", ""^796""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""PRODUITS_FINANCIERS""
        
        to_check = [""^77"", ""^797""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""PRODUITS_EXCEPTIONNELS""
        
        to_check = [""^78""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""REPRISES_AMORT._DEP.""

        to_check = [""^8""]
        if any (re.search(x,numero_compte) for x in to_check):
            return ""COMPTES_SPECIAUX""

        
# Calcul des rubriques niveau 1
def rubrique_N1(row):
    categorisation = row.RUBRIQUE_N2
    
# BILAN SIMPLIFIE type IFRS N1

    to_check = [""CAPITAUX_PROPRES"", ""DETTES_FINANCIERES""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""PASSIF_NON_COURANT""
    
    to_check = [""IMMOBILISATIONS""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""ACTIF_NON_COURANT""
    
    to_check = [""STOCKS"", ""CREANCES_CLIENTS"", ""AUTRES_CREANCES""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""ACTIF_COURANT""   
        
    to_check = [""DETTES_FOURNISSEURS"", ""AUTRES_DETTES""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""PASSIF_COURANT""
    
    to_check = [""DISPONIBILITES""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""DISPONIBILITES""

    
# COMPTE DE RESULTAT SIMPLIFIE N1
    
    to_check = [""ACHATS""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""COUTS_DIRECTS""
    
    to_check = [""SERVICES_EXTERIEURS"", ""TAXES"", ""CHARGES_PERSONNEL"", ""AUTRES_CHARGES"", ""AMORTISSEMENTS""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""CHARGES_EXPLOITATION""
    
    to_check = [""CHARGES_FINANCIERES""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""CHARGES_FINANCIERES""
        
    to_check = [""CHARGES_EXCEPTIONNELLES"", ""IMPOT""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""CHARGES_EXCEPTIONNELLES""        

    to_check = [""VENTES"", ""PRODUCTION_STOCKEE_IMMOBILISEE""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""CHIFFRE_D'AFFAIRES""
    
    to_check = [""SUBVENTIONS_D'EXPL."", ""AUTRES_PRODUITS_GESTION_COURANTE"", ""REPRISES_AMORT._DEP.""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""PRODUITS_EXPLOITATION""
    
    to_check = [""PRODUITS_FINANCIERS""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""PRODUITS_FINANCIERS""
        
    to_check = [""PRODUITS_EXCEPTIONNELS""]
    if any(re.search(x, categorisation) for x in to_check):
        return ""PRODUITS_EXCEPTIONNELS""      
        

# Calcul des rubriques niveau 0
def rubrique_N0(row):
    masse = row.RUBRIQUE_N1
    
    to_check = [""ACTIF_NON_COURANT"", ""ACTIF_COURANT"", ""DISPONIBILITES""]
    if any(re.search(x, masse) for x in to_check):
        return ""ACTIF""
    
    to_check = [""PASSIF_NON_COURANT"", ""PASSIF_COURANT""]
    if any(re.search(x, masse) for x in to_check):
        return ""PASSIF""

    to_check = [""COUTS_DIRECTS"", ""CHARGES_EXPLOITATION"", ""CHARGES_FINANCIERES"", ""CHARGES_EXCEPTIONNELLES""]
    if any(re.search(x, masse) for x in to_check):
        return ""CHARGES""   
        
    to_check = [""CHIFFRE_D'AFFAIRES"", ""PRODUITS_EXPLOITATION"", ""PRODUITS_FINANCIERS"", ""PRODUITS_EXCEPTIONNELS""]
    if any(re.search(x, masse) for x in to_check):
        return ""PRODUITS""  
    
    
# Mapping des rubriques 
db_cat[""RUBRIQUE_N2""] = db_cat.apply(lambda row: rubrique_N2(row), axis=1)
db_cat[""RUBRIQUE_N1""] = db_cat.apply(lambda row: rubrique_N1(row), axis=1)
db_cat[""RUBRIQUE_N0""] = db_cat.apply(lambda row: rubrique_N0(row), axis=1)    


# Reorganisation colonne
to_select = ['ENTITY',
             'PERIOD',
             'COMPTE_NUM', 
             'RUBRIQUE_N0',
             'RUBRIQUE_N1',
             'RUBRIQUE_N2',
             'RUBRIQUE_N3',
             'VALUE',
             'VALUE_N-1',
             'VARV',
             'VARP']
db_cat = db_cat[to_select]

db_cat
# Creation du dataset ref_entite
dataset_entite = db_cat.copy()

# Regrouper par entite
to_group = [""ENTITY""]
to_agg = {""ENTITY"": ""max""}
dataset_entite = dataset_entite.groupby(to_group, as_index=False).agg(to_agg)

# Affichage du mod√®le de donn√©e
dataset_entite
# Creation du dataset ref_scenario
dataset_scenario = db_cat.copy()

# Regrouper par entite
to_group = [""PERIOD""]
to_agg = {""PERIOD"": ""max""}
dataset_scenario = dataset_scenario.groupby(to_group, as_index=False).agg(to_agg)

# Affichage du mod√®le de donn√©e
dataset_scenario
# Creation du dataset KPIS (CA, MARGE, EBE, BFR, CC, DF)
dataset_kpis = db_cat.copy()

# KPIs CA
dataset_kpis_ca = dataset_kpis[dataset_kpis.RUBRIQUE_N1.isin([""CHIFFRE_D'AFFAIRES""])]

to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N1""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ca = dataset_kpis_ca.groupby(to_group, as_index=False).agg(to_agg)

# Passage value postif
dataset_kpis_ca[""VALUE""] = dataset_kpis_ca[""VALUE""]*-1


# COUTS_DIRECTS
dataset_kpis_ha = dataset_kpis[dataset_kpis.RUBRIQUE_N1.isin([""COUTS_DIRECTS""])]

to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N1""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ha = dataset_kpis_ha.groupby(to_group, as_index=False).agg(to_agg)

# Passage value n√©gatif
dataset_kpis_ha[""VALUE""] = dataset_kpis_ha[""VALUE""]*-1


# KPIs MARGE BRUTE (CA - COUTS DIRECTS)
dataset_kpis_mb = dataset_kpis_ca.copy()
dataset_kpis_mb = pd.concat([dataset_kpis_mb, dataset_kpis_ha], axis=0, sort=False)

to_group = [""ENTITY"",
            ""PERIOD""]
to_agg = {""VALUE"": ""sum""}

dataset_kpis_mb = dataset_kpis_mb.groupby(to_group, as_index=False).agg(to_agg)
dataset_kpis_mb[""RUBRIQUE_N1""] = ""MARGE""
dataset_kpis_mb = dataset_kpis_mb[[""ENTITY"", ""PERIOD"", ""RUBRIQUE_N1"", ""VALUE""]]


# CHARGES EXTERNES
dataset_kpis_ce = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""SERVICES_EXTERIEURS""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ce = dataset_kpis_ce.groupby(to_group, as_index=False).agg(to_agg)

# Passage value negatif
dataset_kpis_ce[""VALUE""] = dataset_kpis_ce[""VALUE""]*-1


# IMPOTS
dataset_kpis_ip = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""TAXES""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ip = dataset_kpis_ip.groupby(to_group, as_index=False).agg(to_agg)

# Passage value negatif
dataset_kpis_ip[""VALUE""] = dataset_kpis_ip[""VALUE""]*-1


# CHARGES DE PERSONNEL
dataset_kpis_cp = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""CHARGES_PERSONNEL""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_cp = dataset_kpis_cp.groupby(to_group, as_index=False).agg(to_agg)

# Passage value negatif
dataset_kpis_cp[""VALUE""] = dataset_kpis_cp[""VALUE""]*-1


# AUTRES_CHARGES
dataset_kpis_ac = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""AUTRES_CHARGES""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ac = dataset_kpis_ac.groupby(to_group, as_index=False).agg(to_agg)

# Passage value negatif
dataset_kpis_ac[""VALUE""] = dataset_kpis_ac[""VALUE""]*-1


# SUBVENTIONS D'EXPLOITATION
dataset_kpis_ac = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""SUBVENTIONS_D'EXPL.""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_ac = dataset_kpis_ac.groupby(to_group, as_index=False).agg(to_agg)


# KPIs EBE = MARGE - CHARGES EXTERNES - TAXES - CHARGES PERSONNEL - AUTRES CHARGES + SUBVENTION D'EXPLOITATION
dataset_kpis_ebe = dataset_kpis_mb.copy()
dataset_kpis_ebe = pd.concat([dataset_kpis_ebe, dataset_kpis_ce, dataset_kpis_ip, dataset_kpis_cp, dataset_kpis_ac], axis=0, sort=False)

to_group = [""ENTITY"", ""PERIOD""]
to_agg = {""VALUE"": ""sum""}

dataset_kpis_ebe = dataset_kpis_ebe.groupby(to_group, as_index=False).agg(to_agg)
dataset_kpis_ebe[""RUBRIQUE_N1""] = ""EBE""
dataset_kpis_ebe = dataset_kpis_ebe[[""ENTITY"", ""PERIOD"", ""RUBRIQUE_N1"", ""VALUE""]]


# KPIs CREANCES CLIENTS
dataset_kpis_cc = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""CREANCES_CLIENTS""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_cc = dataset_kpis_cc.groupby(to_group, as_index=False).agg(to_agg)

# Renommage colonne
to_rename = {'RUBRIQUE_N2': ""RUBRIQUE_N1""}
dataset_kpis_cc = dataset_kpis_cc.rename(columns=to_rename)


# KPIs STOCKS
dataset_kpis_st = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""STOCKS""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_st = dataset_kpis_st.groupby(to_group, as_index=False).agg(to_agg)

# Renommage colonne
to_rename = {'RUBRIQUE_N2': ""RUBRIQUE_N1""}
dataset_kpis_st = dataset_kpis_st.rename(columns=to_rename)


# KPIs DETTES FOURNISSEURS
dataset_kpis_df = dataset_kpis[dataset_kpis.RUBRIQUE_N2.isin([""DETTES_FOURNISSEURS""])]
to_group = [""ENTITY"", ""PERIOD"", ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_df = dataset_kpis_df.groupby(to_group, as_index=False).agg(to_agg)

# Renommage colonne
to_rename = {'RUBRIQUE_N2': ""RUBRIQUE_N1""}
dataset_kpis_df = dataset_kpis_df.rename(columns=to_rename)

# Passage value positif
dataset_kpis_df[""VALUE""] = dataset_kpis_df[""VALUE""].abs()


# KPIs BFR = CREANCES + STOCKS - DETTES FOURNISSEURS
dataset_kpis_bfr_df = dataset_kpis_df.copy()

# Passage dette fournisseur value n√©gatif
dataset_kpis_bfr_df[""VALUE""] = dataset_kpis_bfr_df[""VALUE""]*-1

dataset_kpis_bfr_df = pd.concat([dataset_kpis_cc, dataset_kpis_st, dataset_kpis_bfr_df], axis=0, sort=False)

to_group = [""ENTITY"", ""PERIOD""]
to_agg = {""VALUE"": ""sum""}
dataset_kpis_bfr_df = dataset_kpis_bfr_df.groupby(to_group, as_index=False).agg(to_agg)

# Creation colonne Rubrique_N1 = BFR
dataset_kpis_bfr_df[""RUBRIQUE_N1""] = ""BFR""

# Reorganisation colonne
dataset_kpis_bfr_df = dataset_kpis_bfr_df[[""ENTITY"", ""PERIOD"", ""RUBRIQUE_N1"", ""VALUE""]]


# Creation du dataset final
dataset_kpis_final = pd.concat([dataset_kpis_ca, dataset_kpis_mb, dataset_kpis_ebe, dataset_kpis_cc, dataset_kpis_st, dataset_kpis_df, dataset_kpis_bfr_df], axis=0, sort=False)


# Creation colonne COMP
dataset_kpis_final['PERIOD_COMP'] = (dataset_kpis_final['PERIOD'].str[:4].astype(int) - 1).astype(str) + dataset_kpis_final['PERIOD'].str[-3:]
dataset_kpis_final
# creation base comparable pour dataset_kpis
dataset_kpis_final_comp = dataset_kpis_final.copy()

# Suppression de la colonne p√©riode
dataset_kpis_final_comp = dataset_kpis_final_comp.drop(""PERIOD_COMP"", axis=1)

# Renommage des colonnes
to_rename = {'VALUE': ""VALUE_N-1"", 
             'PERIOD': ""PERIOD_COMP""}
dataset_kpis_final_comp = dataset_kpis_final_comp.rename(columns=to_rename)
dataset_kpis_final_comp
# Jointure entre les 2 tables dataset_kpis_final et dataset_kpis_vf
join_on = [""ENTITY"",
           ""PERIOD_COMP"",
           ""RUBRIQUE_N1""]
dataset_kpis_final = pd.merge(dataset_kpis_final, dataset_kpis_final_comp, how='left', on=join_on).drop(""PERIOD_COMP"", axis=1).fillna(0)

#Cr√©ation colonne Var V
dataset_kpis_final[""VARV""] = dataset_kpis_final[""VALUE""] - dataset_kpis_final[""VALUE_N-1""]

#Cr√©ation colonne Var P (%)
dataset_kpis_final[""VARP""] = dataset_kpis_final[""VARV""] / dataset_kpis_final[""VALUE_N-1""]

dataset_kpis_final
# Creation du dataset evol_ca
dataset_evol_ca = db_enr.copy()

# Filtre COMPTE_NUM = Chiffre d'Affaire (RUBRIQUE N1)
dataset_evol_ca = dataset_evol_ca[dataset_evol_ca['COMPTE_NUM'].str.contains(r'^70|^71|^72')]

# Regroupement 
to_group = [""ENTITY"",
            ""PERIOD"",
            ""MONTH"",
            ""MONTH_INDEX"",
            ""RUBRIQUE_N3""]
to_agg = {""VALUE"": ""sum""}
dataset_evol_ca = dataset_evol_ca.groupby(to_group, as_index=False).agg(to_agg)

dataset_evol_ca[""VALUE""] = dataset_evol_ca[""VALUE""].abs()


# Calcul de la somme cumul√©e
dataset_evol_ca = dataset_evol_ca.sort_values(by=[""ENTITY"", 'PERIOD', 'MONTH_INDEX']).reset_index(drop=True)
dataset_evol_ca['MONTH_INDEX'] = pd.to_datetime(dataset_evol_ca['MONTH_INDEX'], format=""%m"").dt.strftime(""%m"")
dataset_evol_ca['VALUE_CUM'] = dataset_evol_ca.groupby([""ENTITY"", ""PERIOD""], as_index=True).agg({""VALUE"": ""cumsum""})

# Affichage du mod√®le de donn√©e
dataset_evol_ca
#Creation du dataset charges
dataset_charges = db_cat.copy()

# Filtre RUBRIQUE_N0 = CHARGES
dataset_charges = dataset_charges[dataset_charges[""RUBRIQUE_N0""] == ""CHARGES""]

# Mettre en valeur positive VALUE
dataset_charges[""VALUE""] = dataset_charges[""VALUE""].abs()

# Affichage du mod√®le de donn√©e
dataset_charges
# Creation du dataset tr√©sorerie
dataset_treso = db_enr.copy()

# Filtre RUBRIQUE_N1 = TRESORERIE
dataset_treso = dataset_treso[dataset_treso['COMPTE_NUM'].str.contains(r'^5')].reset_index(drop=True)

# Cash in / Cash out ?
dataset_treso.loc[dataset_treso.VALUE > 0, ""CASH_IN""] = dataset_treso.VALUE
dataset_treso.loc[dataset_treso.VALUE < 0, ""CASH_OUT""] = dataset_treso.VALUE

# Regroupement 
to_group = [""ENTITY"",
            ""PERIOD"",
            ""MONTH"",
            ""MONTH_INDEX""]
to_agg = {""VALUE"": ""sum"",
          ""CASH_IN"": ""sum"",
          ""CASH_OUT"": ""sum""}
dataset_treso = dataset_treso.groupby(to_group, as_index = False).agg(to_agg).fillna(0)

# Cumul par p√©riode
dataset_treso = dataset_treso.sort_values([""ENTITY"", ""PERIOD"", ""MONTH_INDEX""])
dataset_treso['MONTH_INDEX'] = pd.to_datetime(dataset_treso['MONTH_INDEX'], format=""%m"").dt.strftime(""%m"")
dataset_treso['VALUE_LINE'] = dataset_treso.groupby([""ENTITY"", 'PERIOD'], as_index=True).agg({""VALUE"": ""cumsum""})

# Mettre en valeur positive CASH_OUT
dataset_treso[""CASH_OUT""] = dataset_treso[""CASH_OUT""].abs()

# Affichage du mod√®le de donn√©e
dataset_treso
# Creation du dataset Bilan
dataset_bilan = db_cat.copy()

# Filtre RUBRIQUE_N0 = ACTIF & PASSIF
dataset_bilan = dataset_bilan[(dataset_bilan[""RUBRIQUE_N0""].isin([""ACTIF"", ""PASSIF""]))]

# Regroupement R0/R1/R2
to_group = [""ENTITY"",
            ""PERIOD"",
            ""RUBRIQUE_N0"",
            ""RUBRIQUE_N1"",
            ""RUBRIQUE_N2""]
to_agg = {""VALUE"": ""sum""}
dataset_bilan = dataset_bilan.groupby(to_group, as_index = False).agg(to_agg).fillna(0)


# Mettre en valeur positive VALUE
dataset_bilan[""VALUE""] = dataset_bilan[""VALUE""].abs()

# Selectionner les colonnes
to_select = [""ENTITY"",
             ""PERIOD"",
             ""RUBRIQUE_N0"",
             ""RUBRIQUE_N1"",
             ""RUBRIQUE_N2"",
             ""VALUE""]
dataset_bilan = dataset_bilan[to_select]

# Affichage du mod√®le de donn√©e
dataset_bilan
def df_to_csv(df, filename):
    # Sauvegarde en csv
    df.to_csv(filename,
              sep="";"",
              decimal="","",
              index=False)
    
    # Cr√©ation du lien url
    naas_link = naas.asset.add(filename)
    
    # Cr√©ation de la ligne
    data = {
        ""OBJET"": filename,
        ""URL"": naas_link,
        ""DATE_EXTRACT"": datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
    }
    return pd.DataFrame([data])
dataset_logo = {
    ""OBJET"": ""Logo"",
    ""URL"": LOGO,
    ""DATE_EXTRACT"": datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
}
logo = pd.DataFrame([dataset_logo])
logo
import json

color = {""name"":""Color"",
         ""dataColors"":[COLOR_1, COLOR_2]}

with open(""color.json"", ""w"") as write_file:
    json.dump(color, write_file)

dataset_color = {
    ""OBJET"": ""Color"",
    ""URL"": naas.asset.add(""color.json""),
    ""DATE_EXTRACT"": datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
}
pbi_color = pd.DataFrame([dataset_color])
pbi_color
entite = df_to_csv(dataset_entite, ""dataset_entite.csv"")
entite
scenario = df_to_csv(dataset_scenario, ""dataset_scenario.csv"")
scenario
kpis = df_to_csv(dataset_kpis_final, ""dataset_kpis_final.csv"")
kpis
evol_ca = df_to_csv(dataset_evol_ca, ""dataset_evol_ca.csv"")
evol_ca
charges = df_to_csv(dataset_charges, ""dataset_charges.csv"")
charges
treso = df_to_csv(dataset_treso, ""dataset_treso.csv"")
treso
bilan = df_to_csv(dataset_bilan, ""dataset_bilan.csv"")
bilan
db_powerbi = pd.concat([logo, pbi_color, entite, scenario, kpis, evol_ca, charges, treso, bilan], axis=0)
db_powerbi
df_to_csv(db_powerbi, ""powerbi.csv"")"
10062,SendGrid - Get all messages,"import naas
import requests
import urllib
import pandas as pd
SENDGRID_API_KEY = naas.secret.get(""SENDGRID_API"")
def get_messages(msg_id=None,
                 from_email=None,
                 subject=None,
                 to_email=None,
                 status=None,
                 clicks=None,
                 limit=1000):
    
    kargs = locals()
    params = {}
    for k in kargs:
        v = kargs.get(k)
        if v is not None:
            params[k] = v
    req_url = f""https://api.sendgrid.com/v3/messages?{urllib.parse.urlencode(params)}""
    headers = {
        ""Authorization"": f""Bearer {SENDGRID_API_KEY}"",
        ""Content-Type"": ""application/json""
    }
    res = requests.get(req_url,
                       headers=headers)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        raise(e)
    res_json = res.json()
    messages = res_json.get(""messages"")
    
    # Formatting
    df = pd.DataFrame(messages)
    df[""last_event_time""] = df[""last_event_time""].astype(str).str.replace(""T"", "" "").str.replace(""Z"", """")
    df.columns = df.columns.str.upper()
    return df

df_messages = get_messages(limit=1000)
df_messages"
10063,SendGrid - Send message,"import requests
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import *
SENDGRID_API_KEY = """"
# Email senders and receivers
email_from ="""" 
email_from_name = None #custom email from name, 24 char max
email_to = """"
email_to_name = None #custom email from name
email_to_cc = None #emails in copy

# Email content
subject=""Test SendGrid""
content=""Hi there, Best regards!""
def send_email(email_from,
               email_to,
               subject,
               content,
               email_to_cc=None,
               email_from_name=None,
               email_to_name=None,
               content_type=""text/plain""):
    
    # Connect to SendGrid
    SG = SendGridAPIClient(api_key=SENDGRID_API_KEY)

    from_email = Email(email_from, name=email_from_name)
    to_email = To(email_to, name=email_to_name)
    cc_email = Cc(email=email_to_cc)
    content = Content(content_type, content)

    mail = Mail(from_email, to_email, subject, content)
    if email_to_cc is not None:
        mail.add_cc(cc_email)
    try:
        res = SG.client.mail.send.post(request_body=mail.get())
        if res.status_code == 202:
            print(f""üìß Email successfully sent to {email_to}"")
    except requests.HTTPError as e:
        raise(e)
send_email(email_from=email_from,
           email_to=email_to,
           subject=subject,
           content=content,
           email_to_cc=email_to_cc,
           email_from_name=email_from_name,
           email_to_name=email_to_name)"
10064,Worldometer - World population evolution and projections,"import pandas as pd
import plotly.express as px
from bs4 import BeautifulSoup
import requests
DATA_URLS = [""https://www.worldometers.info/world-population/world-population-by-year/"",
    ""https://www.worldometers.info/world-population/world-population-projections/""
    ]

TABLE_COLS = ['Year',
    'World Population',
    'YearlyChange',
    'NetChange',
    'Density(P/Km¬≤)',
    'UrbanPop',
    'UrbanPop %']
# Generic functions

def scrap_table(url, table_cloumns):
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'html.parser')
    dfs = pd.read_html(page.text)

    for df in dfs:
        if df.columns.to_list() == table_cloumns:
            return df
    return None

def merge_tables_from_urls(urls, table_columns):
    table = None
    for url in urls:
        new_value = scrap_table(url, table_columns)
        if new_value is not None:
            if table is None:
                table = new_value
            else:
                table = table.append(new_value)
    return table
table
def create_graph(x_label, y_label, table, title="""", graph_type=px.line):
    fig = graph_type(table, x=x_label, y=y_label, title=title)
    fig.show()
# Print population graph from year to year
def display_population_graph(table, x_from=None, x_to=None, graph_type=px.line):
    x_label = TABLE_COLS[0]
    y_label = TABLE_COLS[1]
    if x_from is not None:
        table = table[table.Year >= x_from]
    if x_to is not None:
        table = table[table.Year <= x_to]
    title = f""{y_label} by {x_label}, between {table[x_label].to_list()[-1]} and {table[x_label].to_list()[0]}""
    create_graph(x_label, y_label, table, title, graph_type)
table = merge_tables_from_urls(DATA_URLS, TABLE_COLS)

table = table.sort_values(by=[TABLE_COLS[0]], ascending=False)

table.drop_duplicates(subset=TABLE_COLS[0], keep=""first"", inplace=True)
chart1 = display_population_graph(table)
display_population_graph(table, x_from=1800, x_to=2020)
display_population_graph(table, x_from=2000, x_to=2100)
display_population_graph(table, x_from=1950, x_to=2100, graph_type=px.bar)"
10065,Zapier - Trigger workflow,"import naas_drivers
url = ""https://zapier.com/hooks/catch/n/Lx2RH/""
data = { ""first_name"":""Bryan"", ""last_name"":""Helmig"", ""age"": 27 }
result = naas_drivers.zappier.connect(url).send(data)"
10066,Hugging Face - Ask boolean question to T5,"!pip install transformers
!pip install sentencepiece
import json
import torch
from operator import itemgetter
from distutils.util import strtobool
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained('mrm8488/t5-base-finetuned-boolq')
model = AutoModelForSeq2SeqLM.from_pretrained('mrm8488/t5-base-finetuned-boolq').to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
try:model.parallelize()
except:pass
srcs = [
    { 'stream': lambda:open('boolq/train.jsonl', 'r'),
      'keys': ['question', 'passage', 'answer'] },
    { 'stream': lambda:open('boolq/dev.jsonl', 'r'),
      'keys': ['question', 'passage', 'answer'] },
    { 'stream': lambda:open('boolq-nat-perturb/train.jsonl', 'r'),
      'keys': ['question', 'passage', 'roberta_hard'] }
]
model.train()
for _ in range(0): # epochs
    for src in srcs:
        with src['stream']() as s:
            for d in s:
                q, p, a = itemgetter(src['keys'][0], src['keys'][1], src['keys'][2])(json.loads(d))
                tokens = tokenizer('question:'+q+'\ncontext:'+p, return_tensors='pt')
                if len(tokens.input_ids[0]) > model.config.n_positions:
                    continue
                model(input_ids=tokens.input_ids,
                    labels=tokenizer(str(a), return_tensors='pt').input_ids,
                    attention_mask=tokens.attention_mask,
                    use_cache=True
                    ).loss.backward()
model.eval(); # ; suppresses long output on jupyter
def query(q='question', c='context'):
    return strtobool(
        tokenizer.decode(
            token_ids=model.generate(
                input_ids=tokenizer.encode('question:'+q+'\ncontext:'+c, return_tensors='pt')
            )[0],
        skip_special_tokens=True,
        max_length=3)
    )
if __name__ == '__main__':
    ideas = [ 'The idea is to pollute the air instead of riding the bike.', # should be false
              'The idea is to go cycling instead of driving the car.', # should be true
              'The idea is to put your trash everywhere.', # should be false
              'The idea is to reduce transport distances.', # should be true
              'The idea is to put plants on all the roofs.', # should be true
              'The idea is to forbid opensource vaccines.', # should be true
              'The idea is to go buy an Iphone every five years.', # should be false 
              'The idea is to walk once every week in the nature.', # should be true  
              'The idea is to go buy Green bonds.', # should be true  
              'The idea is to go buy fast fashion.', # should be false
              'The idea is to buy single-use items.', # should be false
              'The idea is to drink plastic bottled water.', # should be false
              'The idea is to use import goods.', # should be false
              'The idea is to use buy more food than you need.', # should be false
              'The idea is to eat a lot of meat.', # should be false
              'The idea is to eat less meat.', # should be false
              'The idea is to always travel by plane.', # should be false
              'The idea is to opensource vaccines.' # should be false
             
            ]
    for idea in ideas:
        print('üåè Idea:', idea)
        print('\t‚úÖ Good idea' if query('Is the idea environmentally friendly?', idea) else '\t‚ùå Bad idea' )"
10067,Hugging Face - Naas drivers integration,"from naas_drivers import huggingface
huggingface.get(""text-generation"", model=""gpt2"", tokenizer=""gpt2"")(""What is the most important thing in your life right now?"")
huggingface.get(""summarization"", model=""t5-small"", tokenizer=""t5-small"")('''

There will be fewer and fewer jobs that a robot cannot do better. 
What to do about mass unemployment this is gonna be a massive social challenge and 
I think ultimately we will have to have some kind of universal basic income.

I think some kind of a universal basic income is going to be necessary 
now the output of goods and services will be extremely high 
so with automation they will they will come abundance there will be or almost everything will get very cheap.

The harder challenge much harder challenge is how do people then have meaning like a lot of people 
they find meaning from their employment so if you don't have if you're not needed if 
there's not a need for your labor how do you what's the meaning if you have meaning 
if you feel useless these are much that's a much harder problem to deal with. 

''')
huggingface.get(""text-classification"", 
        model=""distilbert-base-uncased-finetuned-sst-2-english"",
        tokenizer=""distilbert-base-uncased-finetuned-sst-2-english"")('''

It was a weird concept. Why would I really need to generate a random paragraph? 
Could I actually learn something from doing so? 
All these questions were running through her head as she pressed the generate button. 
To her surprise, she found what she least expected to see.

''')
huggingface.get(""fill-mask"",
        model=""distilroberta-base"",
        tokenizer=""distilroberta-base"")('''

It was a beautiful <mask>.

''')
huggingface.get(""feature-extraction"", model=""distilbert-base-cased"", tokenizer=""distilbert-base-cased"")(""Life is a super cool thing"")
huggingface.get(""token-classification"", model=""dslim/bert-base-NER"", tokenizer=""dslim/bert-base-NER"")('''

My name is Wolfgang and I live in Berlin

''')"
10068,Metrics Store - Content creation Track connections,"from naas_drivers import notion
import naas
import pandas as pd
import plotly.express as px
# Input
notion_token = ""secret_ALstzXsSXoF9zbcUakMYE1OufXHVOkb35v1rYBTzz54""
notion_database = ""https://www.notion.so/naas-official/37a23cbfaac5445690301dfd49f035d3?v=8ab1b2f3847d4067ac5ae19a799c7dcb""

# Output
output_html = ""ContentCreator_No_connections.csv""
# Get database object
db_notion = notion.connect(notion_token).database.get(notion_database)

# Get database as dataframe
df_notion = db_notion.df()
df_notion.VALUE=df_notion.VALUE.astype(float)
df_notion
fig = px.line(data_frame=df_notion,
              x='DATE',
              y='VALUE',
              color='GROUP')

#add button control to chart
fig.update_layout(
    title=f""üöÄ<b> Number of Connections</b><br><span style='font-size: 13px;'></span>"",
    title_font=dict(family=""Arial"", size=18, color=""black""),
    legend_title=None,
    plot_bgcolor=""#ffffff"",
    width=1200,
    height=800,
    paper_bgcolor=""white"",
    xaxis_title=""Date"",
    xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
    yaxis_title='No. of connections',
    yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
    margin_pad=10,
    updatemenus=[
        dict(
            active=0,
            buttons=list([
                dict(
                    
                    label=""Both"",
                    method=""update"",
                    args=[{""visible"":[True,True]},
                         {""title"": ""Both""}]),
                dict(
                    
                    label=""Twitter"",
                    method=""update"",
                    args=[{""visible"":[True,False]},
                          {""title"": ""Twitter""}]),
                dict(
                   
                    label=""Linkedin"",
                    method=""update"",
                     args=[{""visible"":[False,True]},
                          {""title"": ""Linkedin""}])
            ]),
            pad={""r"": -80, ""t"": -40},
            direction=""down"",
            x=1,
            xanchor=""left"",
            y=1,
            yanchor=""top"",
            borderwidth=1,
            bordercolor=""black"",
            bgcolor=None
        ),
    ])
fig.write_html(output_html)
naas.asset.add(output_html, params={""inline"": True})"
10069,Spotify - Create Radar Chart to analyze Playlist,"!pip install spotify
!pip install spotipy
import json
import spotipy
import pandas as pd
from spotipy.oauth2 import SpotifyClientCredentials
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from math import pi
#Retrieve Client credentials from Spotify Developer Page
client_id = ''
client_secret = ''

client_credentials_manager = SpotifyClientCredentials(client_id, client_secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

#Retrieve playlist_id by getting the Spotify URI of any playlist
playlist_id = ""spotify:playlist:5fqIcaihygJQGberg0wy0G""
results = sp.playlist(playlist_id)

min_max_scaler = MinMaxScaler()
#Function to Convert JSON to Dataframe
# create a list of song ids
ids=[]

for item in results['tracks']['items']:
        track = item['track']['id']
        ids.append(track)
        
song_meta={'id':[],'album':[], 'name':[], 
           'artist':[],'explicit':[],'popularity':[]}

for song_id in ids:
    # get song's meta data
    meta = sp.track(song_id)
    
    # song id
    song_meta['id'].append(song_id)

    # album name
    album=meta['album']['name']
    song_meta['album']+=[album]

    # song name
    song=meta['name']
    song_meta['name']+=[song]
    
    # artists name
    s = ', '
    artist=s.join([singer_name['name'] for singer_name in meta['artists']])
    song_meta['artist']+=[artist]
    
    # explicit: lyrics could be considered offensive or unsuitable for children
    explicit=meta['explicit']
    song_meta['explicit'].append(explicit)
    
    # song popularity
    popularity=meta['popularity']
    song_meta['popularity'].append(popularity)

song_meta_df=pd.DataFrame.from_dict(song_meta)

# check the song feature
features = sp.audio_features(song_meta['id'])
# change dictionary to dataframe
features_df=pd.DataFrame.from_dict(features)

# convert milliseconds to mins
# duration_ms: The duration of the track in milliseconds.
# 1 minute = 60 seconds = 60 √ó 1000 milliseconds = 60,000 ms
features_df['duration_ms']=features_df['duration_ms']/60000

# combine two dataframe
final_df=song_meta_df.merge(features_df)
#Function for Data Pre-Processing
music_features=features_df[['danceability','energy','loudness','speechiness',
                            'acousticness','instrumentalness','liveness','valence',
                            'tempo','duration_ms']]

music_features.describe()
#Transforming Data so that all values are in the range 0 to 1
#To turn of warning run below command
pd.set_option('mode.chained_assignment', None)
music_features.loc[:]=min_max_scaler.fit_transform(music_features.loc[:])
#Radar Chart with several heads from DataFrame
#Creating Radar Chart
fig = plt.figure(figsize=(10,10))

categories=list(music_features.columns)

N=len(categories)

value = list(music_features.mean())

value+=value[:1]
angles = [n/float(N)*2*pi for n in range(N)]
angles+= angles[:1]


plt.polar(angles,value,color='red')
plt.fill(angles,value,alpha=0.7,color='purple')

plt.title('Playlist Audio Features', size=20, y=1.05)

plt.xticks(angles[:-1],categories,size=15, color='purple')
plt.yticks(color='black', size=15)
plt.show()
"
10070,Boursorama - Get CDS,"import pandas as pd
URL = ""https://www.boursorama.com/bourse/taux/cds/""
dfs = pd.read_html(URL)
df = dfs[0]
df
df.to_excel(r'Get_CDS.csv', index = False)"
10071,Pipedrive - Get contact,"try:
    from pipedrive.client import Client
except:
    !pip install pipedrive-python-lib
    from pipedrive.client import Client
import pandas as pd
DOMAIN = ""https://your_domain.pipedrive.com""
API_KEY = ""your_api_key""
client = Client(domain=DOMAIN)
client.set_api_token(API_KEY)
response = client.persons.get_all_persons() #returns all the records present in CRM
dataKey = response.get(""data"", None) #the key with all records
userInfo = {} #empty dictionary for storing data in user object
users = [] #empty list to store multiple user objects
for dataKeyItem in dataKey: #Iterate through each record
    phonelist = []
    emaillist = []
    for phone in dataKeyItem[""phone""]:
        phonelist.append(phone[""value""])

    for email in dataKeyItem[""email""]:
        emaillist.append(email[""value""])

    userInfo = {
        ""FirstName"": dataKeyItem[""first_name""],
        ""LastName"": dataKeyItem[""last_name""],
        ""Phone"": phonelist,
        ""Email"": emaillist,
        ""Company"": dataKeyItem[""org_name""],
        ""Job"" : dataKeyItem[""your_field_api_key""] #once you create a field you will find the resepect field api key in Accounts - Data Fields section
    }
    users.append(userInfo)
#use below line to adjust the display of columns and rows
pd.set_option(""display.max_rows"", None, ""display.max_columns"", 4)

#the below dataframe has sample data from Pipedrive CRM.
personsDF = pd.DataFrame(users)
print(personsDF)"
10072,Insee - Download PDF recap,"import urllib
# Input
SIRET = ""87794XXXXXXXX""

# Output
PDF_OUTPUT = f""RECAPITULATIF_INSEE_{SIRET}.pdf""
def download_pdf(siret, filepath):
    url = f""https://api.avis-situation-sirene.insee.fr/identification/pdf/{siret}""
    response = urllib.request.urlopen(url)    
    file = open(filepath, 'wb')
    file.write(response.read())
    file.close()
    print(""File saved:"", filepath) 
download_pdf(SIRET, PDF_OUTPUT)"
10073,Draft Kings - Get NBA Moneylines,"import naas
import requests
import pandas as pd
from bs4 import BeautifulSoup
from naas_drivers import emailbuilder
from datetime import datetime
import pytz
# URL to scrap data
URL = 'https://sportsbook.draftkings.com/leagues/basketball/88670846'
# Get all timezone
pytz.all_timezones
# Set Time zone
TIME_ZONE = ""America/New_York""
naas.set_remote_timezone(TIME_ZONE)
# Schedule the Notebook to run at 11 AM each day of the NBA season
naas.scheduler.add(cron='0 11 * 10,11,12,1,2,3,4,5 *')

# To delete your scheduler, uncomment the line below and execute the cell
# naas.scheduler.delete()
# Email
EMAIL_TO = ""<YOUR_EMAIL>""
TODAY = datetime.now(pytz.timezone(TIME_ZONE)).strftime(""%Y-%m-%d"")
EMAIL_SUBJECT = f""üèÄ Draft Kings : Your NBA game of the day {TODAY}""
page = requests.get(URL)
soup = BeautifulSoup(page.content, ""html.parser"")
results = soup.find(id='root')
div = results.find_all('div', class_='parlay-card-10-a')
teams = []
lines = []

for element in div:
    team = element.find_all(""div"", class_=""event-cell__name-text"")
    teams.append(f'{team}')
    line = element.find_all(""span"", class_=""sportsbook-odds american no-margin default-color"")
    lines.append(f'{line}')
team_a = teams[0]
team_b = teams[1]
team_list_a = team_a.split("", "")
team_list_b = team_b.split("","")
team_list2_a = []
team_list2_b = []

for t in team_list_a:
    new = t[:-6]
    team_list2_a.append(new)

for t in team_list_b:
    new = t[:-6]
    team_list2_b.append(new)
team_list3_a = []
team_list3_b = []

for x in team_list2_a:
    new = x[35:]
    team_list3_a.append(new)

for x in team_list2_b:
    new = x[35:]
    team_list3_b.append(new)
team_list4_a = [q.replace('>', '') for q in team_list3_a]
dk_team_names_a = [q.replace('<', '') for q in team_list4_a]

team_list4_b = [q.replace('>', '') for q in team_list3_b]
dk_team_names_b = [q.replace('<', '') for q in team_list4_b]
dk_team_names_a.extend(dk_team_names_b)
# Change Team Names to Match with 3-letter Code
new1 = [team.replace('CLE Cavaliers', 'CLE') for team in dk_team_names_a]
new2 = [team.replace('CHI Bulls', 'CHI') for team in new1]
new3 = [team.replace('MIN Timberwolves', 'MIN') for team in new2]
new4 = [team.replace('MIA Heat', 'MIA') for team in new3]
new5 = [team.replace('IND Pacers', 'IND') for team in new4]
new6 = [team.replace('SA Spurs', 'SAS') for team in new5]
new7 = [team.replace('MIL Bucks', 'MIL') for team in new6]
new8 = [team.replace('GS Warriors', 'GSW') for team in new7]
new9 = [team.replace('SAC Kings', 'SAC') for team in new8]
new10 = [team.replace('UTA Jazz', 'UTA') for team in new9]
new11 = [team.replace('TOR Raptors', 'TOR') for team in new10]
new12 = [team.replace('DEN Nuggets', 'DEN') for team in new11]
new13 = [team.replace('WAS Wizards', 'WAS') for team in new12]
new14 = [team.replace('POR Trail Blazers', 'POR') for team in new13]
new15 = [team.replace('NY Knicks', 'NYK') for team in new14]
new16 = [team.replace('BKN Nets', 'BRK') for team in new15]
new17 = [team.replace('LA Clippers', 'LAC') for team in new16]
new18 = [team.replace('DET Pistons', 'DET') for team in new17]
new19 = [team.replace('DAL Mavericks', 'DAL') for team in new18]
new20 = [team.replace('BOS Celtics', 'BOS') for team in new19]
new21 = [team.replace('PHI 76ers', 'PHI') for team in new20]
new22 = [team.replace('ORL Magic', 'ORL') for team in new21]
new23 = [team.replace('MEM Grizzlies', 'MEM') for team in new22]
new24 = [team.replace('OKC Thunder', 'OKC') for team in new23]
new25 = [team.replace('HOU Rockets', 'HOU') for team in new24]
new26 = [team.replace('NO Pelicans', 'NOP') for team in new25]
new27 = [team.replace('LA Lakers', 'LAL') for team in new26]
new28 = [team.replace('ATL Hawks', 'ATL') for team in new27]
new29 = [team.replace('PHO Suns', 'PHO') for team in new28]
new30 = [team.replace('CHA Hornets', 'CHA') for team in new29]
teams_today = new30
line_a = lines[0]
line_b = lines[1]
line_list_a = line_a.split("", "")
line_list_b = line_b.split("", "")
line_list2_a = []
line_list2_b = []

for t in line_list_a:
    new = t[:-6]
    line_list2_a.append(new)

for t in line_list_b:
    new = t[:-6]
    line_list2_b.append(new)
line_list3_a = []
line_list3_b = []

for x in line_list2_a:
    new = x[63:]
    line_list3_a.append(new)

for x in line_list2_b:
    new = x[63:]
    line_list3_b.append(new)
line_list4_a = [q.replace('>', '') for q in line_list3_a]
line_list5_a = [q.replace('/', '') for q in line_list4_a]
dk_lines_a = [q.replace('<', '') for q in line_list5_a]

line_list4_b = [q.replace('>', '') for q in line_list3_b]
line_list5_b = [q.replace('/', '') for q in line_list4_b]
dk_lines_b = [q.replace('<', '') for q in line_list5_b]
dk_lines_a.extend(dk_lines_b)
lines_today = dk_lines_a
correct_num_games = len(lines_today)
correct_teams_today = []

if len(teams_today) > len(lines_today):
    correct_teams_today = teams_today[:correct_num_games]
else:
    correct_teams_today = teams_today
team1 = correct_teams_today[1::2]
team2 = correct_teams_today[::2]
team1_line = lines_today[1::2]
team2_line = lines_today[::2]
combined_list = pd.DataFrame(
    {'Team 1' : team1, 
     'Team 2' : team2, 
     'Team 1 - Moneyline' : team1_line, 
     'Team 2 - Moneyline' : team2_line
})
combined_list
content = {
    ""header"": emailbuilder.image(src=""https://s3-symbol-logo.tradingview.com/draftkings--600.png"",
                                 link=""https://sportsbook.draftkings.com"",
                                 align=""center"",
                                 width=""20%""),
    ""txt_0"": emailbuilder.text(""Hi Ballers üèÄ,<br><br>""
                               f""Here below the Moneylines* for NBA games as of {TODAY} :<br>""),
    ""table"": emailbuilder.table(combined_list,
                                border=True,
                                header=True,
                                col_size={0: ""15%"", 1: ""15%"", 2: ""35%"", 3: ""35%""},
                                col_align={0: ""center"", 1: ""center"", 2: ""center"", 3: ""center""}),
    ""txt_1"": emailbuilder.text(""<i>*A moneyline bet in sports refers to a wager on the winning team.</i>""),
    ""button_1"": emailbuilder.button(link=""https://sportsbook.draftkings.com"",
                                    text=""Bet on Draft Kings"",
                                    color=""white"",
                                    background_color=""#53d337""),
    ""txt_4"": (""Interested to improve this template, please send contact <a href='https://www.linkedin.com/in/ja-williams-529517187'>JA Williams<a/> or send a message to Naas Core Team at hello@naas.ai.<br><br>""),
    ""heading_5"": emailbuilder.text(""Happy betting üí∏!""),
    ""footer"": emailbuilder.footer_company(naas=True)
}
email_content = emailbuilder.generate(display='iframe', **content)
naas.notification.send(EMAIL_TO,
                       EMAIL_SUBJECT,
                       email_content)"
10074,OwnCloud - Upload file,"!pip install pyocclient
import naas
import owncloud
oc = owncloud.Client('https://cloud.damken.com')

oc.login('YOURNAME', 'YOURPASS')
oc.mkdir('testdir')
oc.put_file('testdir/upload_to_owncloud.ipynb', 'upload_to_owncloud.ipynb')
link_info = oc.share_file_with_link('testdir/upload_to_owncloud.ipynb')
print (""Here is your link: "" + link_info.get_link())
"
10075,OwnCloud - Download file,"!pip install pyocclient
import naas
import owncloud
oc = owncloud.Client('https://cloud.damken.com')

oc.login('YOURNAME', 'YOURPASS')
oc.get_file('testdir/download_to_owncloud.ipynb', 'download_to_owncloud.ipynb')"
10076,Excel - Read file,"import pandas as pd 
excel_file_path = ""Excel-Sales_Jan2020.xlsx""
df = pd.read_excel(excel_file_path)
df"
10077,Excel - Consolidate files,"import pandas as pd
import naas
# Input
excel_file_path1 = ""Excel-Sales_Jan2020.xlsx""
excel_file_path2 = ""Excel-Sales_Jan2020.xlsx""

# Output
excel_output_path = ""Conso.xlsx""
df1 = pd.read_excel(excel_file_path1)
df1
df2 = pd.read_excel(excel_file_path2)
df2
df_concat = pd.concat([df1, df2], axis=0).reset_index(drop=True)
df_concat
df_concat.to_excel(excel_output_path)
print(f'üíæ Excel '{excel_output_path}' successfully saved in Naas.')
naas.asset.add(excel_output_path)"
10078,Excel - Save file,"import pandas as pd
from naas_drivers import yahoofinance
# Input yahoo
ticker = ""TSLA""
date_from = -100
date_to = 'today'
interval = '1d'
moving_averages = [20, 50]

# Output
excel_file_path = f""{ticker}.xlsx""
df_yahoo = yahoofinance.get(ticker,
                            date_from=date_from,
                            date_to=date_to,
                            interval=interval,
                            moving_averages=moving_averages)
df_yahoo.to_excel(excel_file_path)
print(f'üíæ Excel '{excel_file_path}' successfully saved in Naas.')"
10079,Excel - Get dynamic active range,"from openpyxl import load_workbook
from openpyxl.utils import get_column_letter
excel_path = ""Excel_Template.xlsx""
wb = load_workbook(excel_path)
ws = wb.active
ws
def get_active_range(ws):
    max_row = ws.max_row
    max_col = get_column_letter(ws.max_column)
    active_range = f""A1:{max_col}{max_row}""
    return active_range

active_range = get_active_range(ws)
active_range
"
10080,Excel - Custom sheet,"import naas
from openpyxl import load_workbook
from openpyxl.cell import Cell
from openpyxl.styles import Color, PatternFill, Font, Border
from openpyxl.styles.borders import Border, Side
# Inputs
excel_init_path = ""Excel_Template.xlsx""

# Outputs
excel_out_path = ""Excel_Custom.xlsx""
# Sheet Range
sheet_range = ""A1:M54""

# Sheet Font
sheet_font = Font(name='Arial', bold=False, color='000000', size='11')

# Border style
sheet_border = Border(
    left=Side(border_style='thin', color='000000'),
    right=Side(border_style='thin',color='000000'),
    top=Side(border_style='thin', color='000000'),
    bottom=Side(border_style='thin',color='000000')
)
# Number range
number_range = ""B2:M54""

# Number format
number_format =  '#,##0'
# Header range
header_range = ""1:1""

# Header background
header_bg = PatternFill(start_color='24292e',
                        end_color='24292e',
                        fill_type= 'solid' )

# Header font
header_font = Font(name='Arial',
                   bold=True,
                   color='FFFFFF',
                   size='11')
# Total range
total_range = ""54:54""

# Total background
total_bg = PatternFill(start_color='47DD82',
                       end_color='47DD82',
                       fill_type= 'solid' )
wb = load_workbook(excel_init_path)
ws = wb.active
ws
cell_range = ws[sheet_range]
for row in cell_range:
    for cell in row:
        cell.font = sheet_font
        cell.border = sheet_border
cell_range = ws[number_range]
for row in cell_range:
    for cell in row:
        cell.number_format = number_format 
for cell in ws[header_range]:    
    cell.fill = header_bg
    cell.font = header_font
for cell in ws[total_range]:    
    cell.fill = total_bg
wb.save(excel_out_path)
naas.asset.add(excel_out_path)"
10081,PDF - Transform to MP3,"!pip install pdfminer.six
!pip install gTTS
from io import StringIO
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from gtts import gTTS
def convert_pdf_to_string(file_path):

	output_string = StringIO()
	with open(file_path, 'rb') as in_file:
	    parser = PDFParser(in_file)
	    doc = PDFDocument(parser)
	    rsrcmgr = PDFResourceManager()
	    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())
	    interpreter = PDFPageInterpreter(rsrcmgr, device)
	    for page in PDFPage.create_pages(doc):
	        interpreter.process_page(page)

	return(output_string.getvalue())

                
def convert_title_to_filename(title):
    filename = title.lower()
    filename = filename.replace(' ', '_')
    return filename


def split_to_title_and_pagenum(table_of_contents_entry):
    title_and_pagenum = table_of_contents_entry.strip()
    
    title = None
    pagenum = None
    
    if len(title_and_pagenum) > 0:
        if title_and_pagenum[-1].isdigit():
            i = -2
            while title_and_pagenum[i].isdigit():
                i -= 1

            title = title_and_pagenum[:i].strip()
            pagenum = int(title_and_pagenum[i:].strip())
        
    return title, pagenum
    
pdf_name = 'Installation_Guide.pdf' # .pdf file you want to convert
print(convert_pdf_to_string(pdf_name))
rr = convert_pdf_to_string(pdf_name)
string_of_text = ''
for text in rr:
    string_of_text += text

final_file = gTTS(text=string_of_text, lang='en')  # store file in variable
final_file.save(""Generated Speech.mp3"")  # save file to computer"
10082,Google Analytics - Get unique visitors,"import pandas as pd
import plotly.graph_objects as go
import naas
from naas_drivers import googleanalytics
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
csv_output = ""googleanalytics_unique_visitors.csv""
html_output = ""googleanalytics_unique_visitors.html""
naas.scheduler.add(cron=""0 8 * * *"")
naas.dependency.add(json_path)

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_unique_visitors = googleanalytics.connect(json_path).views.get_unique_visitors(view_id)
df_unique_visitors.tail(5)
df_unique_visitors.to_csv(csv_output, index=False)
def plot_unique_visitors(df: pd.DataFrame):
    """"""
    Plot PageView in Plotly.
    """"""
    # Prep dataframe
    df[""Date""] = pd.to_datetime(df['Year Month'] + ""01"")
    
    # Get last month value
    value = ""{:,.0f}"".format(df.loc[df.index[-1], ""Users""]).replace("","", "" "")
    
    # Create data
    data = go.Bar(
        x=df[""Date""],
        y=df['Users'],
        text=df['Users'],
#         marker=dict(color=""black""),
        orientation=""v""
    )
    # Create layout
    layout = go.Layout(
        yaxis={'categoryorder': 'total ascending'},
        margin={""l"":150, ""pad"": 20},
        title=f""<b>Number of Unique Visitors by Month</b><br><span style='font-size: 13px;'>Unique visitors this month: {value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        xaxis_title=""Months"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title=""No visitors"",
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(textposition=""outside"")
    return fig

fig = plot_unique_visitors(df_unique_visitors)
fig.show()
# Export in HTML
fig.write_html(html_output)

# Shave with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)"
10083,Google Analytics - Get pageview ranking,"import pandas as pd
import plotly.graph_objects as go
import naas
from naas_drivers import googleanalytics
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
csv_output = ""googleanalytics_pages_views.csv""
html_output = ""googleanalytics_pages_views.html""
naas.scheduler.add(cron=""0 8 * * *"")
naas.dependency.add(json_path)

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_pageview = googleanalytics.connect(json_path=json_path).views.get_pageview(view_id)
df_pageview
df_pageview.to_csv(csv_output, index=False)
def plot_pageview(df: pd.DataFrame):
    """"""
    Plot PageView in Plotly.
    """"""
    # Prep dataframe
    df.loc[df.Pages == ""/"", ""Pages""] = ""landing""
    df.loc[df.Pages != ""landing"", ""Pages""] = df.Pages.str[1:]
    
    # Get total views
    value = ""{:,.0f}"".format(df[""Pageview""].sum()).replace("","", "" "")
    
    # Create data
    data = go.Bar(y=df['Pages'],
                  x=df['Pageview'],
                  text=df['Pageview'],
#                   marker=dict(color=""black""),
                  orientation=""h"")
    # Create layout
    layout = go.Layout(
        yaxis={'categoryorder': 'total ascending'},
        margin={""l"":150, ""pad"": 20},
        title=f""<b>Most visited web pages, by total visits</b><br><span style='font-size: 13px;'>Total visits: {value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        xaxis_title=""No of views"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(textposition=""outside"")
    return fig

fig = plot_pageview(df_pageview)
# Export in HTML
fig.write_html(html_output)

# Shave with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)"
10084,Google Analytics - Get time on landing page,"from datetime import timedelta
import pandas as pd
import plotly.graph_objects as go
import naas
from naas_drivers import googleanalytics
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
naas.scheduler.add(cron=""0 8 * * *"")
naas.dependency.add(json_path)

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_avg_time = googleanalytics.connect(json_path=json_path).views.get_time_landing(view_id=view_id, landing_path=""/"")
df_avg_time
avg_time_on_landing['avg_time_landing']
def gen_tickvals(avg_time_landing: pd.Series) -> tuple:
    """"""
    Generate tick text and values.
    """"""
    delta = int(avg_time_landing.std() / 3)
    minimum = int(avg_time_landing.min() - delta)
    maximum = int(avg_time_landing.max() + delta)
    tickvals = list(range(minimum, maximum, delta))
    ticktext = [str(timedelta(seconds=v)) for v in tickvals]
    return tickvals, ticktext

def plot_time_spent_on_landing(df: pd.DataFrame):
    """"""
    Plot time spent on landing page in Plotly.
    """"""
    tickvals, ticktext = gen_tickvals(df['avg_time_landing'])
    data = go.Scatter(
        x=pd.to_datetime(df['Year Month'] + ""01""),
        y=df['avg_time_landing']
    )

    fig = go.Figure(data=data)
    fig.update_traces(mode='lines+markers')
    fig.update_layout(title=""Avg. Time on Landing Page (in seconds)"", template=""none"")
    fig.update_yaxes(ticktext=ticktext, tickvals=tickvals)
    return fig
plot_time_spent_on_landing(avg_time_on_landing)
avg_time_on_landing['avg_time_landing']
str(timedelta(seconds=70))
"
10085,Google Analytics - Get unique visitors by country,"import plotly.graph_objects as go
import plotly.express as px
import naas
# Using a dropin driver in a cell for now. (Faster iterations)
# from naas_drivers import googleanalytics
%run Google_Analytics_driver.ipynb
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
df = googleanalytics.connect(json_path=json_path).views.get_data(
    view_id,
    metrics=""ga:newUsers"",
    pivots_dimensions=""ga:country"",
    dimensions=""ga:date"",
    format_type=""pivot"",
    start_date=""30daysAgo"",
    end_date=""today""
)
df
df
"
10086,Google Analytics - Follow number of sessions daily,"import plotly.graph_objects as go
import naas
import datetime
import pandas as pd
from naas_drivers import googleanalytics
# Get your credential from Google Cloud Platform
json_path = 'naas-googleanalytics.json'

# Get view id from google analytics
view_id = ""228952707""

# Setup your data parameters
dimensions = ""daily"" #hourly, daily, weekly, monthly
start_date = ""30daysAgo"" #XdaysAgo or date in ISO format %Y-%m-%d
end_date = ""today"" #Today or date in ISO format %Y-%m-%d
# Chart title
title = ""Sessions""

# Outputs path
name_output = f""Google_Analytics_sessions_{dimensions}""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.html""
df = googleanalytics.connect(json_path, view_id).sessions.get_trend(dimensions, start_date, end_date)
df
def create_linechart(df: pd.DataFrame, label, value, varv, varp, title):
    """"""
    Plot linechart as an area chart in Plotly.
    """"""
    # Prep data
    df[""VALUE_D""] = df[value].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[varv].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[varv] > 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[varp].map(""{:,.0%}"".format).str.replace("","", "" "")
    df.loc[df[varp] > 0, ""VARP_D""] = ""+"" + df[""VARP_D""]    

    # Create hovertext
    df[""TEXT""] = (f""<b>{title} as of "" + df[label].astype(str) + "" : "" + df[""VALUE_D""] + ""</b><br><span style='font-size: 13px;'>"" + df[""VARP_D""] + "" vs last value ("" + df[""VARV_D""] + "")</span>"")
    
    # Get subtitle
    title_display = df.loc[df.index[-1], ""TEXT""] 
    
    # Create data
    data = go.Scatter(
        x=df[label],
        y=df[value],
        stackgroup=""one"",
        text=df[""TEXT""],
        hoverinfo=""text"",
    )
    
    # Create layout
    layout = go.Layout(
        title=title_display,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(mode='lines+markers')
    return fig

fig = create_linechart(df, ""DATE"", ""VALUE"", ""VARV"", ""VARP"", title)
fig
df.to_csv(csv_output, index=False)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(csv_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
fig.write_html(html_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)
fig.write_image(image_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(image_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(image_output)"
10087,Google Analytics - Follow average session duration daily,"import plotly.graph_objects as go
import naas
import datetime
import pandas as pd
from naas_drivers import googleanalytics
# Get your credential from Google Cloud Platform
json_path = 'naas-googleanalytics.json'

# Get view id from google analytics
view_id = ""228952707""

# Setup your data parameters
dimensions = ""daily"" #hourly, daily, weekly, monthly
start_date = ""30daysAgo"" #XdaysAgo or date in ISO format %Y-%m-%d
end_date = ""today"" #Today or date in ISO format %Y-%m-%d
# Chart title
title = ""Average session duration""

# Outputs path
name_output = f""Google_Analytics_average_session_duration_{dimensions}""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.html""
df = googleanalytics.connect(json_path, view_id).avg_session_duration.get_trend(dimensions, start_date, end_date)
df
def create_linechart(df: pd.DataFrame, label, value, varv, varp, title):
    """"""
    Plot linechart as an area chart in Plotly.
    """"""
    # Prep data
    df[""VALUE_D""] = df[value].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[varv].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[varv] > 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[varp].map(""{:,.0%}"".format).str.replace("","", "" "")
    df.loc[df[varp] > 0, ""VARP_D""] = ""+"" + df[""VARP_D""]    

    # Create hovertext
    df[""TEXT""] = (f""<b>{title} as of "" + df[label].astype(str) + "" : "" + df[""VALUE_D""] + ""</b><br><span style='font-size: 13px;'>"" + df[""VARP_D""] + "" vs last value ("" + df[""VARV_D""] + "")</span>"")
    
    # Get subtitle
    title_display = df.loc[df.index[-1], ""TEXT""] 
    
    # Create data
    data = go.Scatter(
        x=df[label],
        y=df[value],
        stackgroup=""one"",
        text=df[""TEXT""],
        hoverinfo=""text"",
    )
    
    # Create layout
    layout = go.Layout(
        title=title_display,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(mode='lines+markers')
    return fig

fig = create_linechart(df, ""DATE"", ""VALUE"", ""VARV"", ""VARP"", title)
fig
df.to_csv(csv_output, index=False)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(csv_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
fig.write_html(html_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)
fig.write_image(image_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(image_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(image_output)"
10088,Google Analytics - Follow number of visitors daily,"import plotly.graph_objects as go
import naas
import datetime
import pandas as pd
from naas_drivers import googleanalytics
# Get your credential from Google Cloud Platform
json_path = 'naas-googleanalytics.json'

# Get view id from google analytics
view_id = ""228952707""

# Setup your data parameters
dimensions = ""daily"" #hourly, daily, weekly, monthly
start_date = ""30daysAgo"" #XdaysAgo or date in ISO format %Y-%m-%d
end_date = ""today"" #Today or date in ISO format %Y-%m-%d
# Chart title
title = ""Visitors""

# Outputs path
name_output = f""Google_Analytics_visitors_{dimensions}""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.html""
df = googleanalytics.connect(json_path, view_id).users.get_trend(dimensions, start_date, end_date)
df
def create_linechart(df: pd.DataFrame, label, value, varv, varp, title):
    """"""
    Plot linechart as an area chart in Plotly.
    """"""
    # Prep data
    df[""VALUE_D""] = df[value].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[varv].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[varv] > 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[varp].map(""{:,.0%}"".format).str.replace("","", "" "")
    df.loc[df[varp] > 0, ""VARP_D""] = ""+"" + df[""VARP_D""]    

    # Create hovertext
    df[""TEXT""] = (f""<b>{title} as of "" + df[label].astype(str) + "" : "" + df[""VALUE_D""] + ""</b><br><span style='font-size: 13px;'>"" + df[""VARP_D""] + "" vs last value ("" + df[""VARV_D""] + "")</span>"")
    
    # Get subtitle
    title_display = df.loc[df.index[-1], ""TEXT""] 
    
    # Create data
    data = go.Scatter(
        x=df[label],
        y=df[value],
        stackgroup=""one"",
        text=df[""TEXT""],
        hoverinfo=""text"",
    )
    
    # Create layout
    layout = go.Layout(
        title=title_display,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(mode='lines+markers')
    return fig

fig = create_linechart(df, ""DATE"", ""VALUE"", ""VARV"", ""VARP"", title)
fig
df.to_csv(csv_output, index=False)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(csv_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
fig.write_html(html_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)
fig.write_image(image_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(image_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(image_output)"
10089,Google Analytics - Follow number of new visitors daily,"import plotly.graph_objects as go
import naas
import datetime
import pandas as pd
from naas_drivers import googleanalytics
# Get your credential from Google Cloud Platform
json_path = 'naas-googleanalytics.json'

# Get view id from google analytics
view_id = ""228952707""

# Setup your data parameters
dimensions = ""daily"" #hourly, daily, weekly, monthly
start_date = ""30daysAgo"" #XdaysAgo or date in ISO format %Y-%m-%d
end_date = ""today"" #Today or date in ISO format %Y-%m-%d
# Chart title
title = ""New visitors""

# Outputs path
name_output = f""Google_Analytics_newuvisitors_{dimensions}""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.html""
df = googleanalytics.connect(json_path, view_id).new_users.get_trend(dimensions, start_date, end_date)
df
def create_linechart(df: pd.DataFrame, label, value, varv, varp, title):
    """"""
    Plot linechart as an area chart in Plotly.
    """"""
    # Prep data
    df[""VALUE_D""] = df[value].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[varv].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[varv] > 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[varp].map(""{:,.0%}"".format).str.replace("","", "" "")
    df.loc[df[varp] > 0, ""VARP_D""] = ""+"" + df[""VARP_D""]    

    # Create hovertext
    df[""TEXT""] = (f""<b>{title} as of "" + df[label].astype(str) + "" : "" + df[""VALUE_D""] + ""</b><br><span style='font-size: 13px;'>"" + df[""VARP_D""] + "" vs last value ("" + df[""VARV_D""] + "")</span>"")
    
    # Get subtitle
    title_display = df.loc[df.index[-1], ""TEXT""] 
    
    # Create data
    data = go.Scatter(
        x=df[label],
        y=df[value],
        stackgroup=""one"",
        text=df[""TEXT""],
        hoverinfo=""text"",
    )
    
    # Create layout
    layout = go.Layout(
        title=title_display,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(mode='lines+markers')
    return fig

fig = create_linechart(df, ""DATE"", ""VALUE"", ""VARV"", ""VARP"", title)
fig
df.to_csv(csv_output, index=False)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(csv_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
fig.write_html(html_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)
fig.write_image(image_output)

# Share with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(image_output)

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(image_output)"
10090,Google Analytics - Get bounce rate,"import pandas as pd
import plotly.graph_objects as go
import naas
from naas_drivers import googleanalytics
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
csv_output = ""googleanalytics_bounce_rate.csv""
html_output = ""googleanalytics_bounce_rate.html""
naas.scheduler.add(cron=""0 8 * * *"")
naas.dependency.add(json_path)

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_bounce_rate = googleanalytics.connect(json_path=json_path).views.get_bounce_rate(view_id=view_id)
df_bounce_rate
df_bounce_rate.to_csv(csv_output, index=False)
def plot_bounce_rate(df: pd.DataFrame):
    """"""
    Plot bounce rate as an area chart in Plotly.
    """"""
    # Prep dataframe
    df[""Date""] = pd.to_datetime(df['Year Month'] + ""01"")
    
    # Get total views
    value = ""{:,.0%}"".format(df[""Bounce Rate""].mean())
    
    # Create data
    data = go.Scatter(
        x=df[""Date""],
        y=df['Bounce Rate'],
        stackgroup=""one""
    )
    
    # Create layout
    layout = go.Layout(
        yaxis={""tickformat"": ',.0%'},
        title=f""<b>Bounce Rate</b><br><span style='font-size: 13px;'>Average bounce rate: {value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        yaxis_title=""Bounce rate %"",
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        xaxis_title=""Mounths"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        margin_pad=10,
    )
    fig = go.Figure(data=data, layout=layout)
    fig.update_traces(mode='lines+markers')
    return fig

fig = plot_bounce_rate(df_bounce_rate)
fig
# Export in HTML
fig.write_html(html_output)

# Shave with naas
#-> Uncomment the line below (by removing the hashtag) to share your asset with naas
# naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(html_output)"
10091,Google Analytics - Get stats per country,"try:
    import pycountry
except:
    !pip install pycountry
    import pycountry
import plotly.graph_objects as go
import plotly.express as px
import naas
from naas_drivers import googleanalytics
json_path = 'naas-googleanalytics.json'
view_id = ""228952707""
naas.scheduler.add(cron=""0 8 * * *"")
naas.dependency.add(json_path)

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_country = googleanalytics.connect(json_path=json_path).views.get_data(
    view_id,
    metrics=""ga:sessions"",
    pivots_dimensions=""ga:country"",
    dimensions=""ga:month"",
    start_date=None,
    end_date=None,
    format_type=""pivot""
)
df_country
sessions_per_country = googleanalytics.connect(json_path=json_path).views.get_country(view_id) # default: metrics=""ga:sessions""
sessions_per_country
users_per_country = googleanalytics.views.get_country(view_id, metrics=""ga:users"")  
sessions_per_country.head()
users_per_country.head()
sessions_per_country = sessions_per_country.reset_index().rename(columns={""index"": ""Country""})
mapping = {country.name: country.alpha_3 for country in pycountry.countries}
sessions_per_country['iso_alpha'] = sessions_per_country['Country'].apply(lambda x: mapping.get(x))
sessions_per_country
fig = px.choropleth(sessions_per_country, locations=""iso_alpha"",
                    color=""Sessions"", 
                    hover_name=""Country"",
                    color_continuous_scale=""Greens"")
fig.show()
"
10092,HTML - Create a website,"from urllib.request import urlopen
from IPython.display import IFrame
import naas 
html = urlopen(""http://www.example.com/"").read().decode('utf-8')
print(html)
html_file = open(""site.html"",""w"")
html_file.write(html)
html_file.close()
IFrame(""https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf"", width=900, height=600)
naas.asset.add(""site.html"", {""inline"": True})"
10093,Snowflake - Update table,"import csv
from snowflakeconnector import SnowflakeConnector
username = ""sanjaynaas""
password = ""Password123""
account = ""iz84541.europe-west4.gcp""
database = ""DEMO_DB""
table_name = ""NAAS""
with open('Excel-Sales_Feb2020.csv') as f:
    reader = csv.reader(f)
    d_list = list(reader)
d_list
#Initialize SnowflakeConnector
instance = SnowflakeConnector(username,password,account,database)
for i in range(1,len(d_list)):
    table_insert_csv_query= ""INSERT INTO NAAS values""
    table_insert_csv_query = table_insert_csv_query +str(tuple(d_list[i]))
    instance.execute_query(table_insert_csv_query,query_type=""push"")
table_display_query = ""select * from "" + table_name
#Fetch records from Snowflake database
instance.execute_query(table_display_query,query_type=""pull"")
instance.close_connection()"
10094,Snowflake - Delete table,"from snowflakeconnector import SnowflakeConnector
import snowflake.connector as snow
username = ""sanjaynaas""
password = ""Password123""
account = ""iz84541.europe-west4.gcp""
database = ""DEMO_DB""
table_name = ""NAAS_NEW""
warehouse_name = ""COMPUTE_WH""
schema_name = ""PUBLIC""
conn = snow.connect(user=username,password=password,account=account)
cur = conn.cursor()
instance = SnowflakeConnector(username,password,account,database)
admin = ""USE ROLE SYSADMIN""
cur.execute(admin)
warehouse_selection = ""USE WAREHOUSE "" + warehouse_name
cur.execute(warehouse_selection)
database_selection = ""USE DATABASE "" + database
cur.execute(database_selection)
schema_selection = ""USE SCHEMA "" + schema_name
cur.execute(schema_selection)
instance.execute_query(""show tables"",query_type=""pull"")
table_deletion = ""DROP TABLE "" + table_name 
cur.execute(table_deletion)
table_display_query = ""SHOW TABLES"" 
#Fetch records from Snowflake database
instance.execute_query(table_display_query,query_type=""pull"")"
10095,Snowflake - Read Table,"import pandas as pd
from snowflakeconnector import SnowflakeConnector
import snowflake.connector as snow
username = ""sanjaynaas""
password = ""Password123""
account = ""iz84541.europe-west4.gcp""
database = ""DEMO_DB""
table_name = ""NAAS""
warehouse_name = ""COMPUTE_WH""
schema_name = ""PUBLIC""
conn = snow.connect(user=username,password=password,account=account)
cur = conn.cursor()
instance = SnowflakeConnector(username,password,account,database)
admin = ""USE ROLE SYSADMIN""
cur.execute(admin)
warehouse_selection = ""USE WAREHOUSE "" + warehouse_name
cur.execute(warehouse_selection)
database_selection = ""USE DATABASE "" + database
cur.execute(database_selection)
schema_selection = ""USE SCHEMA "" + schema_name
cur.execute(schema_selection)
table_display_query = ""select * from "" + table_name
#Fetch records from Snowflake database
instance.execute_query(table_display_query,query_type=""pull"")
data = instance.execute_query(table_display_query,query_type=""pull"")
table_description_query = ""DESCRIBE "" + table_name
table_details = instance.execute_query(table_description_query,query_type=""pull"")
header = table_details[0]
data = pd.DataFrame(data)
data.columns=[header]
data.to_csv('naas_output.csv')
data"
10096,Snowflake - Create table from csv,"import csv
from snowflakeconnector import SnowflakeConnector
import snowflake.connector as snow
import csv
username = ""sanjaynaas""
password = ""Password123""
account = ""iz84541.europe-west4.gcp""
database = ""DEMO_DB""
table_name = ""NAAS_NEW""
warehouse_name = ""COMPUTE_WH""
schema_name = ""PUBLIC""
conn = snow.connect(user=username,password=password,account=account)
cur = conn.cursor()
instance = SnowflakeConnector(username,password,account,database)
admin = ""USE ROLE SYSADMIN""
cur.execute(admin)
warehouse_selection = ""USE WAREHOUSE "" + warehouse_name
cur.execute(warehouse_selection)
database_selection = ""USE DATABASE "" + database
cur.execute(database_selection)
schema_selection = ""USE SCHEMA "" + schema_name
cur.execute(schema_selection)
table_creation = ""CREATE TABLE "" + table_name + ""(MONTH varchar, YEAR int,ITEM varchar,AMOUNT int,CURRENCY varchar)""
cur.execute(table_creation)
with open('Excel-Sales_Feb2020.csv') as f:
    reader = csv.reader(f)
    d_list = list(reader)
d_list
for i in range(1,len(d_list)):
    table_insert_csv_query= ""INSERT INTO "" + table_name + "" VALUES""
    table_insert_csv_query = table_insert_csv_query +str(tuple(d_list[i]))
    instance.execute_query(table_insert_csv_query,query_type=""push"")
table_display_query = ""SELECT * FROM "" + table_name
#Fetch records from Snowflake database
instance.execute_query(table_display_query,query_type=""pull"")"
10097,Microsoft Word - Convert to HMTL,"try:
    import mammoth
except:
    !pip install mammoth
    import mammoth
with open(""naas.docx"", ""rb"") as docx_file:
    result = mammoth.convert_to_html(docx_file)
    html = result.value # The generated HTML
    messages = result.messages # Any messages, such as warnings during conversion
html"
10098,YouTube - Extract and summarize transcript,"!pip install youtube_transcript_api
from youtube_transcript_api import YouTubeTranscriptApi
from naas_drivers import huggingface
video_id = ""I6XbLIRa0v0""
file_name = ""What on earth is data science?""
json = YouTubeTranscriptApi.get_transcript(video_id)
para = """"
for i in json :
    para += i[""text""]
    para += "" ""
para
text = huggingface.get(""summarization"", model=""t5-small"", tokenizer=""t5-small"")(para)
text"
10099,Youtube - Send track to Spotify,"try:
    import spotipy
except:
    !pip install spotipy
    import spotipy

import spotipy.util as util
from spotipy.oauth2 import SpotifyClientCredentials
from spotipy import SpotifyOAuth

try:
    import youtube_dl
except:
    !pip install youtube_dl
    import youtube_dl

try:
    import google_auth_oauthlib.flow
except:
    !pip install google_auth_oauthlib
    import google_auth_oauthlib.flow

try:
    import googleapiclient.discovery
    import googleapiclient.errors
except:
    !pip install googleapiclient
    import googleapiclient.discovery
    import googleapiclient.errors

import requests, json, os, re, time
from urllib.parse import parse_qs, urlparse
from subprocess import Popen, PIPE
from signal import SIGTERM, SIGKILL
# Spotify credentials
SPOTIFY_CLIENT_ID = 'SPOTIFY_CLIENT_ID'
SPOTIFY_CLIENT_SECRET = 'SPOTIFY_CLIENT_SECRET'
SPOTIFY_PLAY_LIST_ID = 'SPOTIFY_PLAYLIST_ID'
SPOTIFY_USERNAME = 'SPOTIFY_USERNAME'

scope = 'user-follow-modify playlist-modify-private'
redir_uri = ""http://localhost:8000""

# Max number of tracks to find in Spotify against each query
spotify_limit = 5
# Google developer API key - required if using Youtube playlist URL as an input
GOOGLE_API_KEY = ""GOOGLE_API_KEY""

# Google API OAUTH credentials - required for obtaining liked videos from user account(s)
GOOGLE_CLIENT_ID = ""GOOGLE_CLIENT_ID""
GOOGLE_CLIENT_SECRET = ""GOOGLE_CLEINT_SECRET""
GOOGLE_PROJECT_ID = ""GOOGLE_PROJECT_ID""

# Avoiding blacklisting by YouTube
youtubeapi_lim = 99 # Maximum number of videos to fetch by GOOGLEAPICLIENT from a playlist - set to >=99 to avoid ERROR 429 - too many requests
youtubedl_lim = 99 # Maximum number of urls to process by YOUTUBE-DL- set to >=99 to avoid ERROR 429 - too many requests
sleep = 0.5 # Time to pause between fetching individual YT video data

# Get data on the Youtube content
ydl_opts = {'retries': 1, 
            'download': False}
ydl = youtube_dl.YoutubeDL(ydl_opts)
# ************ YOUTUBE VIDEO / PLAYLIST / USER INFO

'''INPUT: Youtube video URL, playlist URL or Google API authentication

    OPTIONS:

    youtube_video_url = 'https://www.youtube.com/watch?v=qgaRVvAKoqQ' # URL of a single video

    youtube_video_url = ['https://www.youtube.com/watch?v=qgaRVvAKoqQ', ...] # A list of video URLs

    youtube_video_url='https://www.youtube.com/playlist?list=PLbZIPy20-1pN7mqjckepWF78ndb6ci_qi' # Playlist URL

    youtube_video_url=['https://www.youtube.com/playlist?list=PLbZIPy20-1pN7mqjckepWF78ndb6ci_qi',...] # A list of Playlist URLs

    youtube_video_url=['https://www.youtube.com/playlist?list=PLbZIPy20-1pN7mqjckepWF78ndb6ci_qi',
                        'https://www.youtube.com/watch?v=qgaRVvAKoqQ'] # A mixed list of video & playlist URLs

    youtube_video_url=""client_secrets.json"" # A path to a JSON file with YouTube API credentials

    # A dictionary file with YouTube credentials - requires ""Desktop"" OAuth type
    youtube_video_url={""installed"":{
        ""client_id"":GOOGLE_CLIENT_ID,
        ""project_id"":GOOGLE_PROJECT_ID,
        ""auth_uri"":""https://accounts.google.com/o/oauth2/auth"",
        ""token_uri"":""https://oauth2.googleapis.com/token"",
        ""auth_provider_x509_cert_url"":""https://www.googleapis.com/oauth2/v1/certs"",
        ""client_secret"":GOOGLE_CLIENT_SECRET,
        ""redirect_uris"":[""urn:ietf:wg:oauth:2.0:oob"",""http://localhost:""]}}
    
*********************************************************************
'''

youtube_video_url = 'INPUT_AS_PER_ABOVE'
# FUNCTIONS TO OBTAIN VIDEO METADATA IF YOUTUBE API LOGIN PROVIDED INSTEAD OF URLS

def get_youtube_client(creds,api_service_name = ""youtube"",api_version = ""v3""):
        """""" Log Into Youtube, Copied from Youtube Data API """"""
        
        # Using a JSON file with user credentials
        if '.json' in creds:
            # Get credentials and create an API client
            scopes = [""https://www.googleapis.com/auth/youtube.readonly""]
            flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_secrets_file(
                creds, scopes)

            creds = flow.run_console()

            # from the Youtube DATA API - for user data, e.g. liked items
            youtube_client = googleapiclient.discovery.build(api_service_name, api_version, credentials = creds)
        else:
            # Using API KEY - for public data
            youtube_client = googleapiclient.discovery.build(api_service_name, api_version, developerKey = creds)

        return(youtube_client)

def get_liked_videos(creds,lim):

    """"""Grab Our Liked Videos & Create A Dictionary Of Important Song Information
    https://developers.google.com/youtube/v3/docs/videos/list
    
    CREDS: either json file with credentials OR developer API key
    """"""
    
    request = get_youtube_client(creds).videos().list(
        part=""snippet,contentDetails,statistics"",
        myRating=""like"",
        maxResults=lim
    )
    response = request.execute()

    youtube_urls=[]

    # collect each video and get important information
    for item in response[""items""]:
        video_title = item[""snippet""][""title""]
        youtube_url = ""https://www.youtube.com/watch?v={}"".format(item[""id""])
        youtube_urls.append(youtube_url)

    return(youtube_urls)
# FUNCTIONS FOR GETTING INFO ON YOUTUBE VIDEOS FROM PLAYLIST OR VIDEO URLS

# Split Youtube playlist into individual video URLs
def getPlaylistLinks(url,api_key,api_service_name = ""youtube"",api_version = ""v3""):
    
    try:
        query = parse_qs(urlparse(url).query, keep_blank_values=True)
        playlist_id = query[""list""][0]

        print(f'get all playlist items links from {playlist_id}')
        youtube = googleapiclient.discovery.build(api_service_name,api_version, developerKey = api_key)

        request = youtube.playlistItems().list(
            part = ""snippet"",
            playlistId = playlist_id,
            maxResults = 50
        )
        response = request.execute()

        playlist_items = []
        
        while request is not None:
            response = request.execute()
            playlist_items += response[""items""]
            request = youtube.playlistItems().list_next(request, response)

        #print(f""total: {len(playlist_items)}"")
        playlist=[f'https://www.youtube.com/watch?v={t[""snippet""][""resourceId""][""videoId""]}' 
                  for t in playlist_items]
    except Exception as e:
        playlist=[]
        print(e)
        
    return(playlist)

def llower(x):
    
    # Converts list or tuple elements or strings to lower-case
    
    if isinstance(x,(list,tuple)):
        x=[str(xx).lower() for xx in x]
    elif isinstance(x,str):
        x=str(x).lower()
    else:
        print('LLOWER():Invalid input format - only use list, tuple or scalars as input')
        
    return(x)

# Function for downloading YT video data
def getVideoInfo(url,out_url,down=False,sleep=1):
    
    # https://ostechnix.com/youtube-dl-tutorial-with-examples-for-beginners/
    
    time.sleep(sleep)
    
    try:
        with ydl:
            info = ydl.extract_info(
            url,
            download=down
        )

        kz=list(info.keys())
   
        print(info['title'])
         
        # If we have both artist and track
        if ('artist' in llower(kz) )& ('track' in llower(kz)):
            
            outi={}
            outi['artist']=info['artist'].lower()
            outi['track']=info['track'].lower()
            out_url.append(outi)
        
        # If category = music or VEVO in decription or tag
        elif ('music' in llower(info['categories']))|('vevo' in llower(info['description']))|('vevo' in llower(info['tags'])):
            
            # Items to remove
            rem=['official video','remastered','explicit',')','(','|','""',""'""]
            
            title=info['title'].lower() # title in lower-case
            
            # Remove REM strings (above)
            for r in range(len(rem)):
                title = title.replace(rem[r],'')
            
            # Split artist and track info
            ordr=[0,1] # Order: artist, title
            if '-' in title:
                title = title.split('-')
            elif ':' in title:
                title = title.split(':')
            elif 'by' in title:
                title = title.split('by')
                ordr=[1,0] # Order: title, artist
            
            # If we have both artist and track
            if isinstance(title,list):
                title=[t.strip() for t in title]

                outi={}
                outi['artist']=title[ordr[0]]
                outi['track']=title[ordr[1]]
                out_url.append(outi)
            
    except Exception as e:
        print(e)

    return(out_url,info)
# GET VIDEO METADATA
def processURLs(youtube_video_url,lim,api_key,youtubeapi_lim):
    # If a single URL, convert to list

    if isinstance(youtube_video_url,(list,tuple))==False:
        youtube_video_url=[youtube_video_url]
    
    # MAIN BODY
    
    # Iterate through Youtube URLs

    out=[]
    count=1
    all_info=[]
    
    for url in youtube_video_url:

        # If number of fetched videos is less than the limit
        if count<=lim:
            
            print('>>>>>>>>>>>>>>>>>>>>>')
            print('Processing URL: ',url)
            
            # YOUTUBE CREDENTIALS IN A DICT SUPPLIED INSTEAD OF PLAYLIST OR VIDEO URL(s)
            # > convert to a list of URLs of liked videos
            if isinstance(url,dict):
                # save as JSON
                with open('yt_secrets.json', 'w') as outfile:
                    json.dump(url, outfile)
                url='yt_secrets.json'

            # YOUTUBE CREDENTIALS IN A JSON FILE SUPPLIED INSTEAD OF PLAYLIST OR VIDEO URL(s)
            if '.json' in url.lower():
                # Get liked videos from the account
                url=get_liked_videos(url,youtubeapi_lim)
            
            out_url=[]
            
            # It is a list - split into videos and download data of individual videos
            if playlist_str in url:

                print('This is a Youtube playlist - extracting individual URLs!')

                # Can be a playlist or a list of videos

                urls=getPlaylistLinks(url,api_key)

                l=len(urls)

                print('The Youtube playlist contains ',l,' videos')

                for u in urls:
                    if count<=lim:
                        out_url,info=getVideoInfo(u,out_url,sleep=sleep)
                        count+=1
                        all_info.append(info)
                    else:
                        break

                if len(out_url)>0:
                    print('The Youtube playlist contains videos with ',len(out_url),' music tracks with known artists: ')
                    for lu in range(len(out_url)):
                        print(out_url[lu]['artist'],' - ',out_url[lu]['track'])
                else:
                    print('Sorry, the Youtube playlist does not contain any videos with music track names and / or artist info')

            # It is a  single video
            else:

                print('This is a single Youtube video!')
                
                url_split=url.split('&list=')
                
                if isinstance(url_split,list):
                    url=url_split[0]
                
                out_url,info=getVideoInfo(url,out_url,sleep=sleep)
                all_info.append(info)
                count+=1

                if len(out_url)>0:
                    print('This Youtube video contains valid artists & track data: ')
                    for lu in range(len(out_url)):
                        print(out_url[lu]['artist'],' - ',out_url[lu]['track'])
                else:
                    print('Sorry, this Youtube video does not contain music track name and / or known artist info')

            out=out+out_url

        # Number of videos is larger than the limit
        else:
            break
            
    return(out,all_info)
# Helper function for address already in use error
def freePort(prt):
    try:
        command = ""netstat -ano | findstr ""+str(prt)
        c = Popen(command, shell=True, stdout=PIPE, stderr = PIPE)
        stdout, stderr = c.communicate()
        if len(stdout)>0:
            pid = int(stdout.decode().strip().split(' ')[-1])
            os.kill(pid,SIGTERM)
            #os.kill(pid,SIGKILL)
    except Exception as e:
        print(e)
        # Change port no
        prt+=1
        
    return(prt)

# Function for findin the track on Spotify - public scope
def findSpotifyTrack(out,client_id,client_secret,spotify_limit):
    
    # Log into Spotify API - for accessing public data
    client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
    sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

    # FIND TRACKS
    items=[]
    spotify_ids=[]

    # Search for tracks
    for t in out:

        query='artist:' + t['artist'] + ' track:' + t['track']
        track_id = sp.search(q=query,type='track',limit=spotify_limit)

        l_out=len(track_id['tracks']['items'])

        if l_out>0:
            for ls in range(l_out):
                artist=track_id['tracks']['items'][ls]['artists'][0]['name']
                track=track_id['tracks']['items'][ls]['name']
                spotify_id=track_id['tracks']['items'][ls]['id']

                song=artist+' - '+track
                items.append(song)

                spotify_id=""spotify:track:""+spotify_id
                spotify_ids.append(spotify_id)

    return(spotify_ids,items)
        
# Function for adding a track to user playlist - user / private scope
def addSpotifyTrack(spotify_ids,client_id, client_secret, redirect_uri_port,username,playlist_id,retry=3):
    
    spotify_idsF=[]
    retry_count=1
    
    while retry_count<=retry:
        
        redirect_uri=""http://localhost:""+str(redirect_uri_port)
        
        # Spotify user authorization - for accessing user data
        creds = SpotifyOAuth(scope=scope, client_id=client_id, client_secret=client_secret, 
                             redirect_uri=redirect_uri)
        sp2 = spotipy.Spotify(auth_manager=creds)
        
        # ADD TRACKS TO THE PLAYLIST
        try:
            sp2.user_playlist_add_tracks(username, playlist_id=playlist_id, tracks=spotify_ids)
            spotify_idsF=spotify_ids
            
            retry_count=retry+1
        except Exception as e:
            print(e)
            err=str(e)
            if 'address already in use' in err.lower():
                # If port not available, free it or change port no
                redirect_uri_port=freePort(redirect_uri_port)
                retry_count+=1
                
            else:
                # Other type of error - terminate
                retry_count=retry+1
        
    return(spotify_idsF)
# **** OBTAIN YOUTUBE VIDEO DETAILS FROM VIDEO OR PLAYLIST URLS
out,all_info=processURLs(youtube_video_url,youtubedl_lim,GOOGLE_API_KEY,youtubeapi_lim)

# Saving all track info for reference
with open('all_track_info.json', 'w') as outfile:
    json.dump(all_info,outfile)

print('***********************')
print('Youtube data processing completed - total number of tracks with interpreter info found: ',len(out))

for i in range(len(out)):
    print(out[i]['artist']+' - '+out[i]['track'])
# Find tracks in spotify
spotify_ids,items = findSpotifyTrack(out,
                                     SPOTIFY_CLIENT_ID,
                                     SPOTIFY_CLIENT_SECRET,
                                     spotify_limit)

# Add tracks to Spotify playlist - user / private scope
if len(spotify_ids) > 0:
    spotify_idsF = addSpotifyTrack(spotify_ids,
                                   SPOTIFY_CLIENT_ID,
                                   SPOTIFY_CLIENT_SECRET,
                                   redir_uri_port,
                                   SPOTIFY_USERNAME,
                                   SPOTIFY_PLAY_LIST_ID)
else:
    spotify_idsF = []
if len(spotify_idsF)==0:
    print('Completed: No tracks found & added to Spotify playlist, sorry!')
else:
    print('Completed: ',len(spotify_idsF),' tracks added to Spotify playlist: ')
    print('------------------------')
    for i in items:
        print(i+'\n')"
10100,YouTube - Summarize video,"from naas_drivers import youtube
video_url = ""https://www.youtube.com/watch?v=4ds2FDI_60g&list=PLseBeHeWM4DHat4s5W2OeetaPB3sN2oTL&index=7""
summary = youtube.transcript.summarize(video_url)
summary"
10101,YouTube - Download video,"!pip install pytube3 --upgrade
from pytube import YouTube
youtube_video_url = 'https://www.youtube.com/watch?v=bleU_nrckr8'

yt_obj = YouTube(youtube_video_url)
for stream in yt_obj.streams:
    print(stream)
filters = yt_obj.streams.filter(progressive=True, file_extension='mp4')
 
for mp4_filter in filters:
    print(mp4_filter)
filters = yt_obj.streams.filter(progressive=True, file_extension='mp4')
 
filters.get_highest_resolution()
filters.get_lowest_resolution()
filters.get_highest_resolution().download()"
10102,YouTube - Extract transcript from video,"import pandas as pd
from naas_drivers import youtube
video_url = ""https://www.youtube.com/watch?v=4ds2FDI_60g&list=PLseBeHeWM4DHat4s5W2OeetaPB3sN2oTL&index=7""
json = youtube.transcript.get(video_url)
df = pd.DataFrame(json)
df
para = """"
for i in json:
    para += i[""text""]
    para += "" ""
para"
10103,YouTube - Get statistics from video,"import naas
from naas_drivers import youtube
# Youtube api Key
YOUTUBE_API_KEY = naas.secret.get('YOUTUBE_API_KEY')

# Channel ID
video_url = ""https://www.youtube.com/watch?v=W8H57kam9kg&t=3685s""
df_stats = youtube.connect(YOUTUBE_API_KEY).video.get_statistics(video_url)
df_stats"
10104,YouTube - Get statistics from channel,"import naas
from naas_drivers import youtube
# Youtube api Key
YOUTUBE_API_KEY = naas.secret.get('YOUTUBE_API_KEY')

# Channel ID
channel_url = ""https://www.youtube.com/channel/UCKKG5hzjXXU_rRdHHWQ8JHQ""
df_stats = youtube.connect(YOUTUBE_API_KEY).channel.get_statistics(channel_url)
df_stats"
10105,YouTube - Get uploads from channel,"import naas
from naas_drivers import youtube
# Youtube api Key
YOUTUBE_API_KEY = naas.secret.get('YOUTUBE_API_KEY')

# Channel ID
channel_url = ""https://www.youtube.com/channel/UCKKG5hzjXXU_rRdHHWQ8JHQ""
df_uploads = youtube.connect(YOUTUBE_API_KEY).channel.get_uploads(channel_url)
df_uploads"
10106,Naas Auth - Bearer validate,"from naas_drivers import naasauth
naasauth.connect()
naasauth.bearer.validate()
"
10107,Naas Auth - Connect,"from naas_drivers import naasauth
#naasauth.connect(token=""yourjupyterhubtoken"")
#naasauth.connect(""yourjupyterhubtoken"")
naasauth.connect()
naasauth.access_token
naasauth.headers"
10108,Naas Auth - Users me,"from naas_drivers import naasauth
naasauth.connect()
naasauth.user.me()"
10109,Plotly - Create Candlestick,"import naas
from naas_drivers import yahoofinance
import plotly.graph_objects as go
import pandas as pd
title = ""Candlestick""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
date_from = -360 # Date can be number or date or today
date_to = ""today""
df = yahoofinance.get(""TSLA"", date_from=date_from, date_to=date_to)
df
fig = go.Figure()
fig = go.Figure(data=[go.Candlestick(x=df['Date'],
                open=df['Open'],
                high=df['High'],
                low=df['Low'],
                close=df['Close'])])

fig.update_layout(
    title=title,
    plot_bgcolor=""#ffffff"",
    width=1200,
    height=800,
    xaxis_tickfont_size=14,
    yaxis=dict(
        title='Price in $',
        titlefont_size=16,
        tickfont_size=14,
    )
)
config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10110,Plotly - Create Waterfall chart,"import naas
import plotly.graph_objects as go
title = ""EBITDA (simple)""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
fig = go.Figure(go.Waterfall(
    name = ""20"", orientation = ""v"",
    measure = [""relative"", ""relative"", ""total"", ""relative"", ""relative"", ""total""],
    decreasing = {""marker"":{""color"":""#d94228""}},
    increasing = {""marker"":{""color"":""#5ee290""}},
    totals = {""marker"":{""color"":""#3f3f3f""}},
    x = [""Sales"", ""Consulting"", ""Revenue"", ""Direct expenses"", ""Other expenses"", ""EBITDA""],
    textposition = ""outside"",
    text = [""+60"", ""+80"", ""140"", ""-40"", ""-20"", ""80""],
    y = [60, 80, 0, -40, -20, 0],
    connector = {""line"":{""color"":""white""}},
))

fig.update_layout(
    title=title ,
    plot_bgcolor=""#ffffff"",
    width=1200,
    height=800,
    xaxis_tickfont_size=14,
    yaxis=dict(
        title='USD (millions)',
        titlefont_size=16,
        tickfont_size=14,
    ),
    legend=dict(
        x=0,
        y=1.0,
        bgcolor='white',
        bordercolor='white'
    ),
    bargap=0.1, # gap between bars of adjacent location coordinates.
    bargroupgap=0.1 # gap between bars of the same location coordinate.
)
config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10111,Plotly - Create Vertical Barchart stacked,"import naas
import plotly.graph_objects as go
import pandas as pd
title = ""Sales Evolution (stacked)""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
x_axis = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]
y_axis_value1 = [219, 146, 112, 127, 124, 180, 236, 207, 236, 263, 350, 430, 474, 526, 488, 537, 500, 439]
y_axis_value2 = [16, 13, 10, 11, 28, 37, 43, 55, 56, 88, 105, 156, 270, 299, 340, 403, 549, 499]

df1 = pd.DataFrame(zip(x_axis, y_axis_value1), columns=[""LABEL"", ""VALUE""])
df1[""GROUPS""] = ""USA""

df2 = pd.DataFrame(zip(x_axis, y_axis_value2), columns=[""LABEL"", ""VALUE""])
df2[""GROUPS""] = ""Rest of world""

df = pd.concat([df1, df2])
df
def create_barchart(df, label, groups, value, title):
    fig = go.Figure()
    
    list_groups = df[groups].unique()
    for group in list_groups:
        tmp_df = df[df[groups] == group]
        fig.add_trace(go.Bar(x=tmp_df[label],
                             y=tmp_df[value],
                             name=group))
    fig.update_layout(
        title=title ,
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        xaxis_tickfont_size=14,
        legend=dict(
            bgcolor='white',
            bordercolor='white'
        ),
        barmode='stack',
        bargap=0.1, # gap between bars of adjacent location coordinates.
        bargroupgap=0.1 # gap between bars of the same location coordinate.
    )
    config = {'displayModeBar': False}
    fig.show(config=config)
    return df

fig = create_barchart(df, label=""LABEL"", groups=""GROUPS"", value=""VALUE"", title=title)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10112,Plotly - Create Linechart,"import naas
from naas_drivers import yahoofinance, plotly
title = ""Linechart""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
date_from = -360 # Date can be number or date or today
date_to = ""today""
df = yahoofinance.get(""TSLA"",
                      date_from=date_from,
                      date_to=date_to)
df
fig = plotly.linechart(df,
                       x=""Date"",
                       y=[""Open"", ""Close""],
                       title=title,
                       yaxis_title=""Price in $"")
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10113,Plotly - Create Mapchart world,"import naas
import plotly.graph_objects as go
import pandas as pd
title = ""Worldmap""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/2014_world_gdp_with_codes.csv')
df
fig = go.Figure()

fig = go.Figure(data=go.Choropleth(
    locations = df['CODE'],
    z = df['GDP (BILLIONS)'],
    text = df['COUNTRY'],
    colorscale = 'Blues',
    autocolorscale=False,
    reversescale=True,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_tickprefix = '$',
    colorbar_title = 'GDP<br>Billions US$',
))

fig.update_layout(
    title=title ,
    plot_bgcolor=""#ffffff"",
    legend_x=1,
    geo=dict(
        showframe=False,
        showcoastlines=False,
        #projection_type='equirectangular'
    ),
    dragmode= False,
    width=1200,
    height=800,

)

config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10114,Plotly - Create Leaderboard,"import plotly.express as px
import pandas as pd
title = ""Leaderboard""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
data = [
    {""LABEL"": ""A"", ""VALUE"": 88},
    {""LABEL"": ""B"", ""VALUE"": 12},
    {""LABEL"": ""C"", ""VALUE"": 43},
    {""LABEL"": ""D"", ""VALUE"": 43},
    {""LABEL"": ""E"", ""VALUE"": 2},
    {""LABEL"": ""F"", ""VALUE"": 87},
    {""LABEL"": ""G"", ""VALUE"": 67},
    {""LABEL"": ""H"", ""VALUE"": 111},
    {""LABEL"": ""I"", ""VALUE"": 24},
    {""LABEL"": ""J"", ""VALUE"": 123},
]
df = pd.DataFrame(data)
df = df.sort_values(by=[""VALUE""], ascending=True) #Order will be reversed in plot 
df
def create_barchart(df, label, value):
    last_value = '{:,.0f}'.format(df[value].sum())
    fig = px.bar(df,
                 y=label,
                 x=value,
                 orientation='h',
                 text=value)
    fig.update_layout(
        title=f""<b>Ranking by label</b><br><span style='font-size: 13px;'>Total value: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        legend_title=""Packs"",
        legend_title_font=dict(family=""Arial"", size=11, color=""black""),
        legend_font=dict(family=""Arial"", size=10, color=""black""),
        font=dict(family=""Arial"", size=12, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        xaxis_title=None,
        xaxis_showticklabels=False,
        yaxis_title=None,
        margin_pad=10,
        margin_t=100,
    )
    # Display fig        
    config = {'displayModeBar': False}
    fig.show(config=config)
    return fig

fig = create_barchart(df, ""LABEL"", ""VALUE"")
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10115,Plotly - Create Bubblechart,"import naas
import plotly.express as px
import pandas as pd
title = ""Life Expectancy vs GDP per Capita GDP, 2007""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
df = px.data.gapminder()
df
fig = px.scatter(
    df.query(""year==2007""),
    x=""gdpPercap"", 
    y=""lifeExp"",
    size=""pop"",
    color=""continent"",
    hover_name=""country"",
    log_x=True,
    size_max=60
)

fig.update_layout(
    plot_bgcolor=""#ffffff"",
    margin=dict(l=0, r=0, t=50, b=50),
    width=1200,
    height=800,
    showlegend=False,
    xaxis_nticks=36,
    title= title,
    xaxis=dict(
        title='GDP per capita (dollars)',
        gridcolor='white',
        type='log',
        gridwidth=2,
    ),
    yaxis=dict(
        title='Life Expectancy (years)',
        gridcolor='white',
        gridwidth=2,
    ))

config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10116,Plotly - Create Horizontal Barchart,"import plotly.graph_objects as go
fig = go.Figure(
    go.Bar(
        x=[20, 14, 23],
        y=['giraffes', 'orangutans', 'monkeys'],
        orientation='h',
        text=[20, 14, 23],
    )
)
fig.update_layout(
    title=""Horizontal barchart"" ,
    plot_bgcolor=""#ffffff"",
    width=1200,
    height=800,
    margin_pad=10,
    xaxis_showticklabels=False,
    bargap=0.1, # gap between bars of adjacent location coordinates.
    bargroupgap=0.2 # gap between bars of the same location coordinate.
)
config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10117,Plotly - Create Vertical Barchart group,"import naas
import plotly.graph_objects as go
import pandas as pd
title = ""Sales Evolution (group)""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
x_axis = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]
y_axis_value1 = [219, 146, 112, 127, 124, 180, 236, 207, 236, 263, 350, 430, 474, 526, 488, 537, 500, 439]
y_axis_value2 = [16, 13, 10, 11, 28, 37, 43, 55, 56, 88, 105, 156, 270, 299, 340, 403, 549, 499]

df1 = pd.DataFrame(zip(x_axis, y_axis_value1), columns=[""LABEL"", ""VALUE""])
df1[""GROUPS""] = ""USA""

df2 = pd.DataFrame(zip(x_axis, y_axis_value2), columns=[""LABEL"", ""VALUE""])
df2[""GROUPS""] = ""Rest of world""

df = pd.concat([df1, df2])
df
def create_barchart(df, label, groups, value, title):
    fig = go.Figure()
    
    list_groups = df[groups].unique()
    for group in list_groups:
        tmp_df = df[df[groups] == group]
        fig.add_trace(go.Bar(x=tmp_df[label],
                             y=tmp_df[value],
                             name=group))
    fig.update_layout(
        title=title ,
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        xaxis_tickfont_size=14,
        legend=dict(
            bgcolor='white',
            bordercolor='white'
        ),
        barmode='group',
        bargap=0.1, # gap between bars of adjacent location coordinates.
        bargroupgap=0.1 # gap between bars of the same location coordinate.
    )
    config = {'displayModeBar': False}
    fig.show(config=config)
    return df

fig = create_barchart(df, label=""LABEL"", groups=""GROUPS"", value=""VALUE"", title=title)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10118,Plotly - Create Gantt chart,"import plotly.express as px
import pandas as pd
df = pd.DataFrame([
    dict(Task=""Job A"", Start='2009-01-01', Finish='2009-02-28'),
    dict(Task=""Job B"", Start='2009-03-05', Finish='2009-04-15'),
    dict(Task=""Job C"", Start='2009-02-20', Finish='2009-05-30')
])
df
fig = px.timeline(df, x_start=""Start"", x_end=""Finish"", y=""Task"")
fig.update_yaxes(autorange=""reversed"") # otherwise tasks are listed from the bottom up
fig.update_layout(
    plot_bgcolor=""#ffffff"",
    width=1200,
    height=600)
config = {'displayModeBar': False}
fig.show(config=config)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10119,Plotly - Create Leaderboard stacked,"import plotly.express as px
import pandas as pd
title = ""Leaderboard_stacked""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
data = [
    {""LABEL"": ""A"", ""GROUPS"": ""1"", ""VALUE"": 88},
    {""LABEL"": ""A"", ""GROUPS"": ""2"", ""VALUE"": 12},
    {""LABEL"": ""B"", ""GROUPS"": ""1"", ""VALUE"": 43},
    {""LABEL"": ""B"", ""GROUPS"": ""2"", ""VALUE"": 43},
    {""LABEL"": ""C"", ""GROUPS"": ""1"", ""VALUE"": 2},
    {""LABEL"": ""C"", ""GROUPS"": ""2"", ""VALUE"": 87},
    {""LABEL"": ""D"", ""GROUPS"": ""1"", ""VALUE"": 67},
    {""LABEL"": ""D"", ""GROUPS"": ""2"", ""VALUE"": 111},
    {""LABEL"": ""E"", ""GROUPS"": ""1"", ""VALUE"": 24},
    {""LABEL"": ""E"", ""GROUPS"": ""2"", ""VALUE"": 123},
]
df = pd.DataFrame(data)
df
# Calc VALUE TOTAL by LABEL to sort bar
def prep_data(df):
    # Calc total
    df_tot = df.groupby([""LABEL""], as_index=False).agg({""VALUE"": ""sum""}).rename(columns={""VALUE"": ""VALUE_TOT""})
    
    # Merge dataframe
    df = pd.merge(df, df_tot, on=""LABEL"")

    # Sort values
    df = df.sort_values(by=[""VALUE_TOT"", ""LABEL""])
    return df

df_plot = prep_data(df)
df_plot
def create_barchart(df, label, groups, value):
    last_value = '{:,.0f}'.format(df[value].sum())
    colors = {
        ""1"": ""blue"",
        ""2"": ""green"",
    }
    fig = px.bar(df,
                 y=label,
                 color=groups,
                 x=value,
                 orientation='h',
                 color_discrete_map=colors,
                 text=value)
    fig.update_layout(
        title=f""<b>Ranking by label</b><br><span style='font-size: 13px;'>Total value: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        legend_title=""Packs"",
        legend_title_font=dict(family=""Arial"", size=11, color=""black""),
        legend_font=dict(family=""Arial"", size=10, color=""black""),
        font=dict(family=""Arial"", size=12, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        xaxis_title=None,
        xaxis_showticklabels=False,
        yaxis_title=None,
        margin_pad=10,
        margin_t=100,
    )
    fig.update_yaxes(categoryarray=df[label].unique())
    # Display fig        
    config = {'displayModeBar': False}
    fig.show(config=config)
    return fig

fig = create_barchart(df_plot, ""LABEL"", ""GROUPS"", ""VALUE"")
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10120,Plotly - Create Vertical Barchart,"import naas
import plotly.graph_objects as go
import pandas as pd
title = ""Sales Evolution""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
x_axis = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012]
y_axis = [219, 146, 112, 127, 124, 180, 236, 207, 236, 263, 350, 430, 474, 526, 488, 537, 500, 439]
value_dict = zip(x_axis, y_axis)

df = pd.DataFrame(value_dict, columns=[""LABEL"", ""VALUE""])
df
def create_barchart(df, label, value, title):
    fig = go.Figure()

    fig.add_trace(go.Bar(x=df[label],
                         y= df[value],
                         marker_color='#5ee290'))
    fig.update_layout(
        title=title,
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        xaxis_tickfont_size=14,
        legend=None,
        bargap=0.1, # gap between bars of adjacent location coordinates.
        bargroupgap=0.1 # gap between bars of the same location coordinate.
    )
    config = {'displayModeBar': False}
    fig.show(config=config)
    return fig

fig = create_barchart(df, label=""LABEL"", value=""VALUE"", title=title)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10121,Plotly - Create Heatmap,"import naas
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
title = ""Heatmap""

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
data = [[54.25, 65, 52, 51, 49],
        [20, 16, 60, 80, 30],
        [30, 60, 51, 59, 20],
        [40, 30, 12, 25, 20]]
df = pd.DataFrame(data)
df
config = {'displayModeBar': False}

def update_layout(fig, title=None, plot_bgcolor=None, width=1200, height=800, showlegend=None, margin=None):
    fig.update_layout(
        title=title,
        plot_bgcolor=plot_bgcolor,
        width=width,
        margin=margin,
        height=height,
        showlegend=showlegend)
    return fig

def heatmap(df,
            x,
            y,
            x_label,
            y_label,
            color,
            colorscale=[[0, ""#d94228""],[0.5, ""#d94228""],[0.5, ""#5ee290""],[1.0, ""#5ee290""]],
            title=None,
            margin=None):
    fig = go.Figure()
    fig = px.imshow(df,
                    labels=dict(x=x_label, y=y_label, color=color),
                    x=x,
                    y=y)
    fig.update_xaxes(side=""top"")
    fig.update_traces(xgap=5, selector=dict(type='heatmap'))
    fig.update_traces(ygap=5, selector=dict(type='heatmap'))
    fig.update_traces(dict(showscale=False, 
                           coloraxis=None, 
                           colorscale=colorscale),
                           selector={'type':'heatmap'})
    fig = update_layout(fig, title=title, margin=margin)
    fig.show(config=config)
    return fig

fig = heatmap(df,
              x=['Global', 'Americas', 'EMEA', 'Asia', 'Oceania'],
              y=['Product 1', 'Product 2', 'Product 3', 'Product 4'],
              x_label=""Business lines"",
              y_label=""Products"",
              color=""Sales"",
              title=title,
              margin=dict(l=0, r=0, t=150, b=50))   
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10122,IUCN - Extinct species,"import pandas as pd
import plotly.express as px
# Input csv
csv_input = ""Table 3  Species by kingdom and class - show all.csv""
# We load the csv file
data = pd.read_csv(csv_input, ',')

# We set the column Name as index
data.set_index('Name', inplace = True)

# Then we select the columns EX, EW and Name, and all the lines we want in the graph
table = data.loc[[""Total"",
                  ""GASTROPODA"",
                  ""BIVALVIA"",
                  ""AVES"",
                  ""MAMMALIA"",
                  ""ACTINOPTERYGII"",
                  ""CEPHALASPIDOMORPHI"",
                  ""INSECTA"",
                  ""AMPHIBIA"",
                  ""REPTILIA"",
                  ""ARACHNIDA"",
                  ""CLITELLATA"",
                  ""DIPLOPODA"",
                  ""ENOPLA"",
                  ""TURBELLARIA"",
                 ""MALACOSTRACA"",
                 ""MAXILLOPODA"",
                 ""OSTRACODA""]# add species here
                 ,""EX"":""EW""]
table
# We add a new column 'CATEGORY' to our Dataframe
table[""CATEGORY""] = [""Total"",
                     ""Molluscs"",
                     ""Molluscs"",
                     ""Birds"",
                     ""Mammals"",
                     ""Fishes"",
                     ""Fishes"",
                     ""Insects"",
                     ""Amphibians"",
                     ""Reptiles"",
                      ""Others"",
                      ""Others"",
                      ""Others"",
                      ""Others"",
                      ""Others"",
                    ""Crustaceans"",
                    ""Crustaceans"",
                    ""Crustaceans""]
table = table.loc[:,[""CATEGORY"",""EX""]] # we drop the column ""EW""
table
# ---NOTE : If you want to add new species, you have to also add his category
# We groupby CATEGORIES :
table.reset_index(drop=True, inplace=True)
table = table.groupby(['CATEGORY']).sum().reset_index()
table.rename(columns = {'EX':'Extincted'}, inplace=True)
table
# We use plotly to show datas with an horizontal bar chart
def create_barchart(table):
    Graph = table.sort_values('Extincted', ascending=False)
    fig = px.bar(Graph,
                 x=""Extincted"",
                 y=""CATEGORY"",
                 color=""CATEGORY"",
                 orientation=""h"")
    fig.update_layout(title_text=""Number of species that have gone extinct since 1500"",
                      title_x=0.5)
    fig.add_annotation(x=800,
                       y=0,
                       text=""Source : IUCN Red List of Threatened Species<br>https://www.iucnredlist.org/statistics"",
                       showarrow=False)
    fig.show()
    return fig
    
fig = create_barchart(table)
"
10123,Bubble - Send data,"from naas_drivers import bubble
url = ""https://appname.bubbleapps.io/api/1.1/wf/endpoint_name""
data = {""first_name"":""Bryan"",
        ""last_name"":""Helmig"",
        ""age"": 27 }
def send_data(url, data):
    bubble.send(url, data)
result = send_data(url, data)"
10124,MongoDB - Send data,"from naas_drivers import mongo
import pandas as pd
# Credentials
user = ""my user""
passwd = ""my passwd""
host = ""url""
port = 9090

# DB parameters
collection_name = ""col""
db_name = ""db_name""
data = {""LABEL"": ""Label 1"", ""VALUE"": 0}
df = pd.DataFrame([data])
df
mongo.connect(host, port, user, passwd).send(df, collection_name, db_name, replace=False)"
10125,MongoDB - Get data,"from naas_drivers import mongo
# Credentials
user = ""my user""
passwd = ""my passwd""
host = ""url""
port = 9090

# DB parameters
collection_name = ""col""
db_name = ""db_name""
df = naas_drivers.mongo.connect(host, port, user, passwd).get(collection_name, db_name)
df"
10126,MongoDB - Send data to Google Sheets,"from naas_drivers import mongo, gsheet
import pandas as pd
import naas
user =""your user""
passwd =""your password""
host =""Your Connection URL""
port = 9090
collection_name =""COLLECTION NAME""
db_name =""DATABASE NAME""
spreadsheet_id = ""------""
sheet_name = ""Sheet1""
naas.scheduler.add(cron='0 9 * * *') #Send in production this notebook and run it, every day at 9:00.

# use this to delete your automation
# naas.scheduler.delete()
df = mongo.connect(host, port, user, passwd).get(collection_name, db_name)
gsheet.connect(spreadsheet_id).send(
    sheet_name=sheet_name,
    data=df,
    append=False
)"
10127,XML - Transform sitemap to dataframe,"import naas
import json 
try:
    import xmltodict
except:
    !pip install xmltodict
    import xmltodict
import pandas as pd
import requests
website = ""https://zapier.com""
def sitemap_to_df(url):
    df = None
    key = ""urlset.url.url""
    r = requests.get(f'{url}/sitemap.xml')
    data_dict = xmltodict.parse(r.content) 
    if key and len(key.split('.')) > 0:
        keys = key.split('.')
        keys.reverse()
        data = data_dict.get(keys.pop())
        while(len(keys) > 1):
            data = data.get(keys.pop())
        df = pd.DataFrame.from_dict(data=data)
    elif key and data_dict.get(key):
        df = pd.DataFrame.from_dict(data=data_dict.get(key))
    else:
        df = pd.DataFrame.from_dict(data=data_dict)
    return df
df = sitemap_to_df(website)
df
naas.get_remote_timezone()
naas.set_remote_timezone(""Europe/Lisbon"")"
10128,Notion - Get users,"from naas_drivers import notion 
# Enter Token API
token = ""*****""
users = notion.connect(token).users.list()
users"
10129,Notion - Sent Gmail On New Item,"import naas
from naas_drivers import notion, gsheet
from naas_drivers import html
import pandas as pd
# Notion
token = ""NOTION_TOKEN""
database_id = ""NOTION_DATABASE_ID""

# Gsheet
spreadsheet_id = ""SPREADSHEET_ID""
mail_list_sheet_name = ""Sheet1""
item_list_sheet_name = ""Sheet2""
your_email = ""YOUR_EMAIL_ID""
#Schedule the notebook to run every 15 minutes
naas.scheduler.add(cron=""*/15 * * * *"")
email_list_data = gsheet.connect(spreadsheet_id).get(sheet_name = mail_list_sheet_name)
try:
    item_list_history = gsheet.connect(spreadsheet_id).get(sheet_name = item_list_sheet_name)
except:
    item_list_history = []
firstname_list = email_list_data['FIRSTNAME']
email_list = email_list_data['EMAIL']
def create_notion_connection():
    database = notion.connect(token).database.get(database_id)
    df_db = database.df()
    print(df_db)
    return df_db
#Send data to Gsheet
def send_data_to_gsheet(data):
    gsheet.connect(spreadsheet_id)
    gsheet.send(
        sheet_name = item_list_sheet_name,
        data = data
    )
#Get new notion items list 
def get_new_items_list(df_db):
    
    if not list(item_list_history):
        new_items = df_db
    else:
        item_list_history['Id'] = item_list_history['Id'].astype(int)
        df_db['Id'] = df_db['Id'].astype(int)

        common = df_db.merge(item_list_history, on=[""Id""])
        new_items = df_db[~df_db.Id.isin(common.Id)]  
        
    data = [] 
    
    for i in range(len(new_items.index)):
        dictionary = {}
        for col in new_items.columns:
            dictionary[col] = str(new_items.iloc[i][col])          
        data.append(dictionary)
    
    send_data_to_gsheet(data)
    
    return data
    
#Get email contents
def get_mail_content():
    email_content = html.generate( 
            display = 'iframe',
            title = 'Updates here!!',
            heading = 'Hi {first_name}, you have some new items in you notion list',
            text_1 = 'Following are the new list of items seperated by comma : ',
            text_2 = '{new_items_list}',
            text_3 = 'Have a great day!!'          
    )
    #print(email_content)
    return email_content
#Send mail to recipients
def send_mail(new_items_list):
    email_content = get_mail_content()
    for i in range(len(email_list_data)):
        subject = ""Update on Notion items""
        content = email_content.replace(""{first_name}"",firstname_list[i]).replace(""{new_items_list}"",new_items_list)
        naas.notifications.send(email_to=email_list[i], subject=subject, html=content, email_from=your_email)
df = create_notion_connection()
new_items_list = get_new_items_list(df)
new_items_list = ', '.join([data['Books'] for data in new_items_list])
if new_items_list:
    send_mail(new_items_list)
else:
    print('No new items!!')"
10130,Notion - Explore API,"import requests
import pandas as pd
import json
DATABASE_ID_TEST = ""a296bd16b7284bc494aa91f50ad64d30"" #https://www.notion.so/a296bd16b7284bc494aa91f50ad64d30?v=d37af84a3a6744fb957002073a267c44

PAGE_ID = ""e2e8b31737174dbe86b9ae65f9b8eb9c"" #click on Page and Get ID : https://www.notion.so/Mary-Meeks-2d822179eb59451e91e83086cdd74e5c

INTEGRATION_TOKEN = ""secret_gF6bJPSyOgt5oZgb2sgT1yiMxfS4LqNmWmd2M8S5vzl""
NOTION_DB_URL = ""https://api.notion.com/v1/databases/""

NOTION_PAGE_URL = ""https://api.notion.com/v1/pages/""

NOTION_PAGE_CONTENT = ""https://api.notion.com/v1/blocks/""
database_url = NOTION_DB_URL + DATABASE_ID_TEST 

response = requests.get(database_url, headers={""Authorization"": f""{INTEGRATION_TOKEN}""})
print (response.json())
database_url = NOTION_DB_URL + DATABASE_ID_TEST + ""/query""

query = {""filter"": {""property"": ""High Priority"", ""checkbox"": {""equals"": True}}}
query = {""filter"": {""property"": ""Cost of next trip"", ""number"": {""greater_than_or_equal_to"": 0.5}}}

headers = {""Authorization"": f""{INTEGRATION_TOKEN}"", ""Notion-Version"": ""2021-05-13""}

response = requests.post(database_url, headers=headers, data=query)
print((response.json()['results']))
df_structure = pd.DataFrame.from_dict(response.json()['results'])
print(""The size of the df is"", df_structure.shape)
df_structure.head()
list_dict = []
for index, row in df_structure.iterrows():
    list_dict.append(row['properties'])

temp_df = pd.DataFrame.from_dict(list_dict)
# Get the columns name in a list to use them later
columns = temp_df.columns.values.tolist()
temp_df.head()
for index, value in temp_df.iloc[2].items():
    print(value)
pd.DataFrame.from_dict(list_dict).iloc[0]['Name']
def extract_name_or_plaintext(dictionnary):
    # Given a dictionnary it will output the string of the key name or plain_text
    if 'name' in dictionnary.keys():
        return dictionnary['name']
    elif 'plain_text' in dictionnary.keys():
        return dictionnary['plain_text']
    else:
        return ''

def extract_date(dictionnary):
    # For the moment we extract only the starting date of a date field
    # Example {'id': 'prop_1', 'type': 'date', 'date': {'start': '2018-03-21', 'end': None}}
    # Input : {'start': '2018-03-21', 'end': None}
    return dictionnary['start']
def extract_data(element):
    # input: a dictionnary of a notion property
    # Exemple: {'id': 'W#4k', 'type': 'select', 'select': {'id': 'b305bd26-****-****-****-c78e2034db8f', 'name': 'Client', 'color': 'green'}}
    # output: the string containing the information of the dict. (Client in the exemple)
    if type(element) is dict:
        dict_type = element['type'] 
        informations = element[dict_type]
        
        if dict_type == 'date':
            informations = extract_date(informations)
        
        elif type(informations) is dict:
            informations = extract_name_or_plaintext(informations)
        
        elif type(informations) is list:
            informations_temp = ''
            for element_of_informations_list in informations:
                informations_temp += extract_name_or_plaintext(element_of_informations_list) + "", ""
            informations = informations_temp[:-2]
        return informations
    
    else:
        return ''
all_list = []
for i in range (temp_df.shape[0]):
    temp_list = []
    for index, value in temp_df.iloc[i].items():
        temp_list.append(extract_data(value))
    all_list.append(temp_list)
df_content = pd.DataFrame.from_records(all_list, columns = columns)

df_content.head()
df_full = pd.concat([df_structure, df_content], axis=1)
df_full
page_url = NOTION_PAGE_URL + PAGE_ID 

response = requests.get(page_url, headers={""Authorization"": f""{INTEGRATION_TOKEN}"", ""Notion-Version"": ""2021-05-13""})
print (response.json())
page_url = NOTION_PAGE_CONTENT + PAGE_ID + ""/children""
headers = {""Authorization"": f""{INTEGRATION_TOKEN}"", ""Notion-Version"": ""2021-05-13""}

response = requests.get(page_url, headers=headers)
print(response.json())
page_url = NOTION_PAGE_URL
page_id = DATABASE_ID_TEST

# Surprisingly in this case you need to add ""Content-Type"": ""application/json""
# If not you will an error 400: body failed validation: body.parent should be defined, instead was 'undefined'.
headers = {""Authorization"": f""{INTEGRATION_TOKEN}"", ""Notion-Version"": ""2021-05-13"", ""Content-Type"": ""application/json""}

name = {""title"":[{""text"":{""content"":""Added via API NAAS""}}]}
company = {""rich_text"": [{""text"": {""content"": ""Test Company""}}]}
status = {""select"": {""name"": ""Lost""}}
est_value = {""number"": 10000 }

header = {""object"": ""block"",
          ""type"": ""heading_2"",
          ""heading_2"": {
            ""text"": [{ ""type"": ""text"", ""text"": { ""content"": ""Naas API test"" } }]
                }
             }

paragraph = {""object"": ""block"",
             ""type"": ""paragraph"",
             ""paragraph"": {
               ""text"": [
                 {
                    ""type"": ""text"",
                    ""text"": {
                      ""content"": ""Notebooks as a service for data geeks. Naas notebooks enable you to script faster with low-code formulas & templates. Automate all your tasks in minutes.."",
                                }
                            }
                      ]
                    }
                }

to_do = {""object"": ""block"",
             ""type"": ""to_do"",
             ""to_do"": {
               ""text"": [
                 {
                    ""type"": ""text"",
                    ""text"": {
                      ""content"": ""Automate all your tasks in minutes.."",
                                }
                            },
                 {
                    ""type"": ""text"",
                    ""text"": {
                      ""content"": ""Script faster"",
                                }
                            }
                      ]
                    }
                }
myobj = {
          ""parent"": {""database_id"": page_id}, 
          ""properties"":
            {
              ""Name"":name,
              ""Company"": company,
              ""Status"": status,
              ""Estimated Value"": est_value
            },
          ""children"":[header, paragraph,to_do]
        } 
data = json.dumps(myobj)

response = requests.post(page_url, headers=headers, data=data)

if 'status' in response.json().keys():
    if response.json()['status'] != 200:
        print (""Error:"", response.json()['message'])
elif 'object' in response.json().keys(): 
    print(""‚úÖ Your data was added to Notion"")
    print(response.json())"
10131,Notion - Generate Google Sheets rows for new items in  database,"from naas_drivers import notion, gsheet
import pandas as pd
# Enter Token API
NOTION_TOKEN = ""*****""

# Enter Database URL
DATABASE_URL = ""https://www.notion.so/********""

#Unique column name for notion
col_unique_notion = 'Name'
# Spreadsheet URL
SPREADSHEET_URL = ""------""

# Sheet name
SHEET_NAME = ""Sheet1""

#Unique column# for gsheets
col_unique_gsheet = 'Name'
# Schedule your notebook every hours
naas.scheduler.add(cron=""0 * * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
db_notion = notion.connect(NOTION_TOKEN).database.get(DATABASE_URL)
df_notion = db_notion.df()
df_gsheet = gsheet.connect(SPREADSHEET_URL).get(sheet_name=SHEET_NAME)
df_gsheet
#Iterate through all rows in Notion database and find match in Google Sheets
#If no match is found then add data to df_difference dataframe

df_difference = pd.DataFrame()
for index,row in df_notion.iterrows():
    x = row[col_unique_notion]
    if not (x == df_gsheet[col_unique_gsheet]).any():
        df_difference = df_difference.append(df_notion.loc[index])
# Send data to Google Sheets
gsheet.connect(SPREADSHEET_URL).send(
    sheet_name=SHEET_NAME,
    data=df_difference,
    append=True,
)"
10132,Notion - Send LinkedIn invitations from database,"import naas
from naas_drivers import notion, linkedin
import pandas as pd
import os
from datetime import datetime
import requests
# Enter Token API
NOTION_TOKEN = ""*****""

# Enter Database URL
DATABASE_URL = ""https://www.notion.so/********""

# Column with Linkedin URL
col_lk_notion = 'Name'
# LinkedIn cookies
LI_AT = 'YOUR_COOKIE_LI_AT'
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'

# LinkedIn limit invitations to 100 per week, this notebook only send 10 invites at once.
LIMIT = 10
# CSV to manage and remove profile already in your contact
csv_contact = ""LINKEDIN_EXISTING_CONTACT.csv""

# CSV to manage URL not valid
csv_not_valid = ""LINKEDIN_NOT_VALID.csv""

# CSV to store invitations sent
csv_invitation = ""LINKEDIN_INVITATIONS_SENT.csv""
# Scheduler your invitation everyday at 8:00 AM
naas.scheduler.add(cron=""0 8 * * *"")

# Uncomment the line below to delete your scheduler
# naas.scheduler.delete()
db_notion = notion.connect(NOTION_TOKEN).database.get(DATABASE_URL)
df_notion = db_notion.df()
df_notion
df_lk_invitations = linkedin.connect(LI_AT, JSESSIONID).invitation.get_sent()
df_lk_invitations
def get_csv(output_path):
    df = pd.DataFrame()
    if os.path.exists(output_path):
        df = pd.read_csv(output_path).drop_duplicates()
    return df
df_contacts = get_csv(csv_contact)
df_contacts
df_not_valid = get_csv(csv_not_valid)
df_not_valid
df_csv_invitations = get_csv(csv_invitation)
df_csv_invitations
def get_new_invitations(df_notion,
                        df_lk_invitations,
                        df_csv_invitations,
                        df_contacts,
                        df_not_valid):
    # Cleaning
    df = df_notion.copy()
    df = df[df[col_lk_notion].str.match("".+.com/in/.+"")].reset_index(drop=True)
    df[""PROFILE_ID""] = df.apply(lambda row: row[col_lk_notion].split(""com/in/"")[-1].split(""/"")[0], axis=1)
    print(""‚úîÔ∏è LinkedIn valid URL :"", len(df))
    
    # Get list of pending LinkedIn invitations
    pending_lk_invitations = []
    if len(df_lk_invitations) > 0:
        pending_lk_invitations = df_lk_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending LinkedIn invitations :"", len(pending_lk_invitations))
    
    # Get list of CSV invitations
    pending_csv_invitations = []
    if len(df_csv_invitations) > 0:
        pending_csv_invitations = df_csv_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending CSV invitations :"", len(pending_csv_invitations))
    
    # Get profile already in network
    contacts = []
    if len(df_contacts) > 0:
        contacts = df_contacts[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Already in network :"", len(contacts))
    
    # Get profile not valid
    not_valids = []
    if len(df_not_valid) > 0:
        not_valids = df_not_valid[""PROFILE_ID""].unique().tolist()
    print(""‚ùå Profile not valid:"", len(not_valids))
    
    # Remove pending invitations / already in network / not valid profile from dataframe 
    exclude = (pending_lk_invitations + pending_csv_invitations + contacts + not_valids)
    df = df[~df[""PROFILE_ID""].isin(exclude)].reset_index(drop=True)
    print(""‚û°Ô∏è New invitation:"", len(df))
    return df

df_new_invitations = get_new_invitations(df_notion,
                                         df_lk_invitations,
                                         df_csv_invitations,
                                         df_contacts,
                                         df_not_valid)
df_new_invitations
def send_invitation(df,
                    df_not_valid=None,
                    df_contacts=None,
                    df_csv_invitations=None):
    # Check if new invitations to perform
    if len(df) == 0:
        print(""ü§ô No new invitations to send"")
        return df
    
    # Setup variables
    if df_not_valid is None:
        df_not_valid = pd.DataFrame()
    if df_contacts is None:
        df_contacts = pd.DataFrame()
    if df_csv_invitations is None:
        df_csv_invitations = pd.DataFrame()
        
    # Loop
    count = 1
    for index, row in df.iterrows():
        df_network = pd.DataFrame()
        profile = row[col_lk_notion]
        print(f""‚û°Ô∏è Checking :"", profile)
        
        # Get distance with profile
        try:
            df_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile)
        except Exception as e:
            # If error, profile URL is not valid => append Notion page to CSV not valid to not check it again
            df_not_valid = pd.concat([df_not_valid, df[index:index+1]])
            df_not_valid.to_csv(csv_not_valid, index=False)
            print(""‚ùå URL not valid"", e)
            
        # Check if profile is already in your network
        if len(df_network) > 0:
            distance = df_network.loc[0, ""DISTANCE""]
            # If not profile in network...
            if distance not in [""SELF"", ""DISTANCE_1""]:
                # => send invitation
                try:
                    linkedin.connect(LI_AT, JSESSIONID).invitation.send(recipient_url=profile)
                    print(count, ""- üôå Invitation successfully sent"")
                    df_csv_invitations = pd.concat([df_csv_invitations, df_network])
                    df_csv_invitations.to_csv(csv_invitation, index=False)
                except Exception as e:
                    print(count, ""- ‚ùå Invitation not sent"", e)
                count += 1
            else:
                # If profile already in your network => append network result to CSV existing contact to not check it again
                df_contacts = pd.concat([df_contacts, df_network])
                df_contacts.to_csv(csv_contact, index=False)
                print(f""üëç Already in my network, üíæ saved in CSV"")
            
        # Manage LinkedIn limit
        if count > LIMIT:
            print(""‚ö†Ô∏è LinkedIn invitation limit reached"", LIMIT)
            return df_csv_invitations
    return df_csv_invitations
        
df_csv_invitations = send_invitation(df_new_invitations,
                                     df_not_valid,
                                     df_contacts,
                                     df_csv_invitations)
df_csv_invitations
"
10133,Notion - Update pages from database,"from naas_drivers import notion
# Enter Token API
notion_token = ""*****""

# Enter Database URL
database_url = ""https://www.notion.so/********""
database_id = database_url.split(""/"")[-1].split(""?v="")[0]
pages = notion.connect(notion_token).database.query(database_id, query={})
print(""üìä Pages in Notion DB:"", len(pages))
for page in pages:
    print(page)
#     page.title(""Name"",""Page updated"")
#     page.rich_text(""Text"",""Ceci est toto"")
#     page.number(""Number"", 42)
#     page.select(""Select"",""Value3"") 
#     page.multi_select(""Muti Select"",[""Value1"",""Value2"",""Value3""])
#     page.date(""Date"",""2021-10-03T17:01:26"") #Follow ISO 8601 format
#     page.people(""People"", [""6e3bab71-beeb-484b-af99-ea30fdef4773""]) #list of ID of users
#     page.checkbox(""Checkbox"", False)
#     page.email(""Email"",""jeremy@naas.ai"")
    page.update()
    print(f""‚úÖ Page updated in Notion."")"
10134,Notion - Send Slack Messages For New  Database Items,"from naas_drivers import notion, slack
import naas
# Enter Token API
NOTION_TOKEN = ""secret_J9JIQksrylGmJpErmw49A7U9ON1lIdGLjbVk6tDFh2y""

# Enter Database id
DATABASE_ID = ""https://www.notion.so/naas-official/72c87516d6e1419fb3a69763892898c7?v=2e71afc61e7644409dd874957c98e78e""
# Token
SLACK_TOKEN = ""xoxb-xxx-xxx-xxx""

# Channel name
SLACK_CHANNEL = ""channel-name""
# Schedule your notebook every 15min
naas.scheduler.add(cron=""*/15 * * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()

pages = notion.connect(NOTION_TOKEN).database.query(DATABASE_ID, query={})
def send_message():

    for page in pages:
        # For the first time ever if there is no property/column named 'slack notification sent' in database 
        if ""Slack notification sent"" not in page.properties.keys():
            page_name = page.properties[""Name""]
            page_url = page.url

            slack.connect(SLACK_TOKEN).send(SLACK_CHANNEL, f'New notion page created ""{page_name}"" here: {page_url}')
            page.select('Slack notification sent', ""True"")
            page.update()

            print(f'‚úÖ Notification sent for {page_name}: {page_url}')
        
        # If there is a column present then checks if it is False or None and updates
        else:
            if str(page.properties['Slack notification sent'])!='True':
                page_name = page.properties[""Name""]
                page_url = page.url

                slack.connect(SLACK_TOKEN).send(SLACK_CHANNEL, f'New notion page created ""{page_name}"" here: {page_url}')
                page.select('Slack notification sent', ""True"")
                page.update()
                print(f'‚úÖ Notification sent for {page_name}: {page_url}')
                
send_message()"
10135,Notion - Automate transcript generation from recording link in page property,"# Used to access nested properties in dictionnaries.
import pydash as _

# Used to run multiple jobs at once.
import threading, queue
import uuid

# Used for nicer cell outputs.
try:
    from rich import print
except:
    ! pip install --user rich
    from rich import print

# Used for transcript generation.
import sys
import json
import datetime
import codecs
import time
import os

# Used for email rendering.
import markdown

# Used to download files from google drive.
try:
    import gdown
except:
    !pip install gdown
    import gdown

# Used to interact with AWS, here S3 and Transcribe.
try:
    import boto3
except:
    ! pip install --user boto3
    
# Used to interact with Notion.
from naas_drivers import notion
from naas_drivers.tools.notion import BlockTypeFactory

# Used to schedule, send notification, get/store secrets.
import naas
# Notion secret
NOTION_TOKEN = naas.secret.get('tf_notion_token')
NOTION_DATABASE_ID = ""your notion database id""

# AWS Secret.
# IAM permissions to S3 and Transcribe are needed to have this template working.
AWS_ACCESS_KEY_ID = naas.secret.get('aws_transcribe_s3_key')
AWS_SECRET_ACCESS_KEY = naas.secret.get('aws_transcribe_s3_secret')
AWS_REGION = 'eu-west-3'

# AWS S3 Bucket name that will be used to send recording and fetch transcripts.
BUCKET_NAME = ""your-existing-s3-bucket""

# To whom we need to send notification errors.
EMAIL_ERROR_TO = ""maxime@naas.ai""
notion.connect(NOTION_TOKEN)
database = notion.connect(NOTION_TOKEN).databases.get(NOTION_DATABASE_ID)
def send_error_notification(notion_page, error):
    email_to = EMAIL_ERROR_TO
    subject = ""üõéÔ∏è Transcript error üö®""
    content = markdown.markdown(
f'''
# üö® An error occured while generating transcript.

Notion page: {notion_page}

## Error:

> {error}

''')
    
    naas.notifications.send(email_to, subject, content)
pages = database.query({
    ""filter"": {
        ""and"": [
        {
            ""property"": ""Transcript computed"",
            ""select"": {
                ""is_empty"": True
            }
        },
        {
            ""property"": ""Recording"",
            ""url"": {
                ""is_not_empty"": True
            }
        }
    ]
    }
})
len(pages)
class UnknownSource(Exception):
    """"""This class is an exception used to identify the fact that we do not know how
    to download a recording.
    """"""
    pass

def download_recording(url, out):
    
    if 'drive.google.com' in url:
        gdown.download(url, out, quiet=False, fuzzy=True)
    else:
        raise UnknownSource('Do not know how to download recording.')
def upload_to_s3(filename):
    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)

    with open(filename, ""rb"") as f:
        s3.upload_fileobj(f, BUCKET_NAME, filename)
def create_transcribe_job(job_name, filename, media_format='mp4'):
    
    transcribe = boto3.client('transcribe', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)
    result = transcribe.start_transcription_job(TranscriptionJobName=job_name,
        MediaFormat=media_format,
        Media={
            'MediaFileUri': 's3://{0}/{1}'.format(BUCKET_NAME, filename)
        },
        Settings={
            'ShowSpeakerLabels': True,
            'MaxSpeakerLabels': 5
        },
        OutputBucketName=BUCKET_NAME,
        OutputKey=f'{filename}.json',
        IdentifyLanguage=True)
    return result
def get_transcribe_job_status(job_name):    
    transcribe = boto3.client('transcribe', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name='eu-west-3')
    result = transcribe.get_transcription_job(TranscriptionJobName=f'{job_name}')
    return result
def download_from_s3(object_name):
    s3 = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)
    local_store_path = os.path.join('/', 'tmp', object_name)
    with open(local_store_path, 'wb') as f:
        s3.download_fileobj(BUCKET_NAME, object_name, f)
        return local_store_path
    return ''
def convert_transcript(filename):
    with codecs.open(filename+'.txt', 'w', 'utf-8') as w:
        with codecs.open(filename, 'r', 'utf-8') as f:
            data=json.loads(f.read())
            labels = data['results']['speaker_labels']['segments']
            speaker_start_times={}
            for label in labels:
                for item in label['items']:
                    speaker_start_times[item['start_time']] =item['speaker_label']
            items = data['results']['items']
            lines=[]
            line=''
            time=0
            speaker='null'
            i=0
            for item in items:
                i=i+1
                content = item['alternatives'][0]['content']
                if item.get('start_time'):
                    current_speaker=speaker_start_times[item['start_time']]
                elif item['type'] == 'punctuation':
                    line = line+content
                if current_speaker != speaker:
                    if speaker:
                        lines.append({'speaker':speaker, 'line':line, 'time':time})
                    line=content
                    speaker=current_speaker
                    time=item['start_time']
                elif item['type'] != 'punctuation':
                    line = line + ' ' + content
            lines.append({'speaker':speaker, 'line':line,'time':time})
            sorted_lines = sorted(lines,key=lambda k: float(k['time']))
            for line_data in sorted_lines:
                line='[' + str(datetime.timedelta(seconds=int(round(float(line_data['time']))))) + '] ' + line_data.get('speaker') + ': ' + line_data.get('line')
                w.write(line + '\n\n')
            return filename+'.txt'
def text_to_chunks(text, chunk_size):
    chunks = []
    count = 0
    text_len = len(text)
    
    while count * chunk_size < text_len:
        chunks.append(text[count * chunk_size:count * chunk_size + chunk_size])
        count += 1
    return chunks
def handle_job(job):
    print(f'Handling Job: {job}')
    recording_url = job.recording_url
    notion_page_id = job.notion_page_id

    # Download recording
    download_location = f'{notion_page_id}'
    download_recording(recording_url, download_location)
    print(f'Downloaded file to {download_location}')
    
    # Upload file to s3.
    upload_to_s3(download_location)
    print(f'File uploaded to s3.')

    # Create transcribe job.
    result = create_transcribe_job(job.id, download_location)
    print(f'Transcription job created.')

    # Wait for transcription job to complete.
    waiting_for_job = True
    job_status = None
    transcript = """"
    while waiting_for_job:
        status = get_transcribe_job_status(job.id)
        job_status = _.get(status, 'TranscriptionJob.TranscriptionJobStatus')
        
        
        print(f'Waiting for job. Actual status ""{job_status}""')
        if job_status == ""COMPLETED"":
            print(f'{job}: AWS Transcribe job completed')
            waiting_for_job = False

            media_file_uri = _.get(status, 'TranscriptionJob.Media.MediaFileUri')
            transcript_file_uri = _.get(status, 'TranscriptionJob.Transcript.TranscriptFileUri')
            
            
            local_store_path = download_from_s3(transcript_file_uri.split('/')[-1])
            print(f'Transcript downloaded {local_store_path}')
            converted_transcript_path = convert_transcript(local_store_path)
            
            
            with open(converted_transcript_path, 'r') as tr:
                for line in tr.readlines():
                    transcript = transcript + line
            
            os.remove(local_store_path)
            os.remove(converted_transcript_path)
            
        elif job_status == ""IN_PROGRESS"":
            print(f'Job in progress')
            time.sleep(5)

        elif job_status not in ['COMPLETED', 'IN_PROGRESS', 'QUEUED']:
            waiting_for_job = False
            print(f'This job might be in error state.')
            print(_.get(status, 'TranscriptionJob.FailureReason'))
            raise Exception(f'Error while running aws transcribe. {_.get(status, ""TranscriptionJob.FailureReason"")}')
    
    if transcript != '' and job_status == ""COMPLETED"":
        notion_page_id = notion_page_id
        page = notion.page.get(notion_page_id)
        print(f'Adding toggle.')
        page.toggle('Transcript')
        page.update()
        for block in page.get_blocks():
            if block.type == 'toggle' and _.get(block, 'toggle.text[0].plain_text') == 'Transcript':
                block_id = block.id
                children_blocks = []
                for chunk in text_to_chunks(transcript, 2000):
                    child = BlockTypeFactory.new_default(type='paragraph', payload=chunk)
                    children_blocks.append(child)
                    
                notion.block.append(block_id, children_blocks)
                page.select(""Transcript computed"", ""True"")
                page.update()
                print(f'Transcript added to notion page {notion_page_id}.')
class Job:
    def __init__(self, notion_page_id, recording_url, job_id=None):
        self.notion_page_id = notion_page_id
        self.recording_url = recording_url
        self.id = job_id if job_id else str(uuid.uuid4())
    
    def __str__(self):
        return f'Job ID = {self.id} Notion Page Id = {self.notion_page_id} Recording URL = {self.recording_url}'
for page in pages:
    recording_url = _.get(page.properties, 'Recording.url')
    
    if recording_url != None:
        job = Job(page.id, recording_url)
        try:
            handle_job(job)
        except Exception as e:
            page = notion.page.get(job.notion_page_id)
            
            send_error_notification(page.url, str(e))   
            page.select(""Transcript computed"", ""ERROR"")
            page.update()

naas.scheduler.add(cron=""0 * * * *"")"
10136,Notion - Create page,"from naas_drivers import notion 
# Enter Token API
token = ""*****""

# Enter Database URL
database_id = ""https://www.notion.so/naas-official/fafef5183c42474987e002d30ba55d18?v=814c0ec7f34a4395aba47c4eeced603f""
page = notion.connect(token).page.create(database_id=database_id, title=""My new page"")
page"
10137,Notion - Get database,"from naas_drivers import notion
# Enter Token API
token = ""*****""

# Enter Database URL
database_url = ""https://www.notion.so/********""
database = notion.connect(notion_token).database.get(database_url, query={})
database
df_notion = database.df()
df_notion"
10138,Notion - Add paragraph with link in page,"from naas_drivers import notion 
from naas_drivers.tools.notion import Link
# Enter Token API
token = ""*****""

# Enter Database URL
page_url = ""https://www.notion.so/naas-official/Daily-med-03952fcb93c045bba519a7564a64045e""

# Link
link = ""https://app.naas.ai/""
page = notion.connect(token).page.get(page_url)
res = page.paragraph(""Go to Naas"")
res.paragraph.text[0].href = link
res.paragraph.text[0].text.link = Link(link)
page.update()"
10139,Notion - Update page,"from naas_drivers import notion 
# Enter Token API
token = ""*****""

# Enter Database URL
page_url = ""https://www.notion.so/naas-official/Daily-med-03952fcb93c045bba519a7564a64045e""
page = notion.connect(token).page.get(page_url)
page
page.title(""Name"",""Page updated"")
page.rich_text(""Text"",""Ceci est toto"")
page.number(""Number"", 42)
page.select(""Select"",""Value3"") 
page.multi_select(""Muti Select"",[""Value1"",""Value2"",""Value3""])
page.date(""Date"",""2021-10-03T17:01:26"") #Follow ISO 8601 format
page.people(""People"", [""6e3bab71-beeb-484b-af99-ea30fdef4773""]) #list of ID of users
page.checkbox(""Checkbox"", False)
page.email(""Email"",""jeremy@naas.ai"")
page.phone_number(""Phone number"",""+33 6 21 83 11 12"")
page.update()
page.heading_1(""Heading 1"")
page.heading_2(""Heading 2"")
page.heading_3(""Heading 3"")
page.paragraph(""Paragraph"")
page.numbered_list_item(""This is first"")
page.to_do(""Need this to be done"")
page.embed(""https://docs.google.com/spreadsheets/*************"")
page.video(""https://www.youtube.com/watch?v=8AsMAc4VFJs"")
page.image(""https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png"")
page.code(""pip install naas"")
page.equation(""e=mc2"")
page.update()
page"
10140,Redshift - Connect with SQL Magic and IAM Credentials,"!pip install -q ipython-sql boto3 psycopg2-binary sqlalchemy-redshift
%reload_ext sql

import boto3
import psycopg2
import getpass
import pandas as pd
from urllib import parse

def rs_connect(dbname, dbhost, clusterid, dbport, dbuser, region_name='us-east-1'):

    ''' Connect to redshift using AIM credentials'''
    aaki = getpass.getpass('aws_access_key_id')
    asak = getpass.getpass('aws_secret_access_key')
    aws_session = boto3.session.Session(aws_access_key_id=aaki, aws_secret_access_key=asak, region_name=region_name)
    aaki = ''; asak = ''
    
    aws_rs = aws_session.client('redshift')
    response = aws_rs.get_cluster_credentials(DbUser=dbuser, DbName=dbname, ClusterIdentifier=clusterid, AutoCreate=False)
    
    ''' Convert those credentials into Database user credentials '''
    dbuser = response['DbUser']
    dbpwd = response['DbPassword']
    
    ''' Generate the SQLAlchemy Connection string '''
    connectionString = 'redshift+psycopg2://{username}:{password}@{host}:{port}/{db}?sslmode=prefer'.format(username=parse.quote_plus(dbuser), password=parse.quote_plus(dbpwd), host=dbhost, port=dbport, db=dbname)

    dbuser = None; dbpwd = None; conn_str = None; response = None;
    
    return connectionString

connectionString = rs_connect('database_name', 'host', 'cluster_id', 5439, 'database_user')

%sql $connectionString

%%sql

select
    your,
    sql,
    goes,
    here
from
  your.brain
"
10141,HubSpot - Create contact,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
email = ""test@cashstory.com""
firstname = ""Test""
lastname ='CASHSTORY'
phone = ""+33600000000""
jobtitle = ""Consultant""
website = ""www.cashstory.com""
company = 'CASHSTORY'
hubspot_owner_id = None
create_contact = {""properties"": 
                  {
                    ""email"": email,
                    ""firstname"": firstname,
                    ""lastname"": lastname,
                    ""phone"": phone,
                    ""jobtitle"": jobtitle,
                    ""website"": website,
                    ""company"": company,
                    ""url"": ""test"",
                    ""hubspot_owner_id"": hubspot_owner_id,
                   }
                 }

contact1 = hubspot.connect(HS_API_KEY).contacts.send(create_contact)
contact2 = hubspot.connect(HS_API_KEY).contacts.create(
    email,
    firstname,
    lastname,
    phone,
    jobtitle,
    website,
    company,
    hubspot_owner_id
)
contact1
contact2"
10142,HubSpot - Get closed deals weekly,"from naas_drivers import hubspot
from datetime import datetime, timedelta
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import naas
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
df_pipelines = hubspot.connect(HS_API_KEY).pipelines.get_all()
df_pipelines
pipeline_id = ""8432671""
name_output = ""HubSpot_closed_weekly""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.htlm""
naas.scheduler.add(cron=""0 8 * * *"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_deals = hubspot.connect(HS_API_KEY).deals.get_all()
df_deals
def get_trend(df_deals, pipeline):
    df = df_deals.copy()
    # Filter data
    df = df[df[""pipeline""].astype(str) == str(pipeline)]
    
    # Prep data
    df[""closedate""] = pd.to_datetime(df[""closedate""])
    df[""amount""] = df.apply(lambda row: float(row[""amount""]) if str(row[""amount""]) not in [""None"", """"] else 0, axis=1)
    
    # Calc by week
    df = df.groupby(pd.Grouper(freq='W', key='closedate')).agg({""hs_object_id"": ""count"", ""amount"": ""sum""}).reset_index()
    df[""closedate""] = df[""closedate""] + timedelta(days=-1)
    df = pd.melt(df, id_vars=""closedate"")
    
    # Rename col
    to_rename = {
        ""closedate"": ""LABEL_ORDER"",
        ""variable"": ""GROUP"",
        ""value"": ""VALUE""
    }
    df = df.rename(columns=to_rename).replace(""hs_object_id"", ""No of deals"").replace(""amount"", ""Amount"")
    df[""YEAR""] = df[""LABEL_ORDER""].dt.strftime(""%Y"")
    df = df[df[""YEAR""] == datetime.now().strftime(""%Y"")]
    df[""LABEL""] = df[""LABEL_ORDER""].dt.strftime(""%Y-W%U"")
    df[""LABEL_ORDER""] = df[""LABEL_ORDER""].dt.strftime(""%Y%U"")
    df = df[df[""LABEL_ORDER""].astype(int) <= int(datetime.now().strftime(""%Y%U""))]
    
    # Calc variation
    df_var = pd.DataFrame()
    groups = df.GROUP.unique()
    for group in groups:
        tmp = df[df.GROUP == group].reset_index(drop=True)
        for idx, row in tmp.iterrows():
            if idx == 0:
                value_n1 = 0
            else:
                value_n1 = tmp.loc[tmp.index[idx-1], ""VALUE""]
            tmp.loc[tmp.index[idx], ""VALUE_COMP""] = value_n1
        df_var = pd.concat([df_var, tmp]).fillna(0).reset_index(drop=True)
    df_var[""VARV""] = df_var[""VALUE""] - df_var[""VALUE_COMP""]
    df_var[""VARP""] = df_var[""VARV""] / abs(df_var[""VALUE_COMP""])
    
    # Prep data
    df_var[""VALUE_D""] = df_var[""VALUE""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df_var[""VARV_D""] = df_var[""VARV""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df_var.loc[df_var[""VARV""] > 0, ""VARV_D""] = ""+"" + df_var[""VARV_D""]
    df_var[""VARP_D""] = df_var[""VARP""].map(""{:,.0%}"".format).str.replace("","", "" "")
    df_var.loc[df_var[""VARP""] > 0, ""VARP_D""] = ""+"" + df_var[""VARP_D""]
    
    # Create hovertext
    df_var[""TEXT""] = (""<b>Deal closed as of "" + df_var[""LABEL""] + "" : </b>"" + 
                      df_var[""VALUE_D""] + ""<br>"" + 
                      df_var[""VARP_D""] + "" vs last week ("" + df_var[""VARV_D""] + "")"")
    return df_var

df_trend = get_trend(df_deals, pipeline_id)
df_trend
def create_barchart(df, label, group, value, varv, varp):    
    # Create figure with secondary y-axis
    fig = make_subplots(specs=[[{""secondary_y"": True}]])

    # Add traces
    df1 = df[df[group] == ""No of deals""].reset_index(drop=True)[:]
    total_volume = ""{:,.0f}"".format(df1[value].sum()).replace("","", "" "")
    var_volume = df1.loc[df1.index[-1], varv]
    positive = False
    if var_volume > 0:
        positive = True
    var_volume = ""{:,.0f}"".format(var_volume).replace("","", "" "")
    if positive:
        var_volume = f""+{var_volume}""
    fig.add_trace(
        go.Bar(
            name=""No of deals"",
            x=df1[label],
            y=df1[value],
            offsetgroup=0,
            hoverinfo=""text"",
            text=df1[""VALUE_D""],
            hovertext=df1[""TEXT""],
            marker=dict(color=""#33475b"")
        ),
        secondary_y=False,
    )
    
    df2 = df[df[group] == ""Amount""].reset_index(drop=True)[:]
    total_value = ""{:,.0f}"".format(df2[value].sum()).replace("","", "" "")
    var_value = df2.loc[df2.index[-1], varv]
    positive = False
    if var_value > 0:
        positive = True
    var_value = ""{:,.0f}"".format(var_value).replace("","", "" "")
    if positive:
        var_value = f""+{var_value}""
    fig.add_trace(
        go.Bar(
            name=""Amount"",
            x=df2[label],
            y=df2[value],
            text=df2[""VALUE_D""] + "" K‚Ç¨"",
            offsetgroup=1,
            hoverinfo=""text"",
            hovertext=df2[""TEXT""],
            marker=dict(color=""#ff7a59"")
        ),
        secondary_y=True,
    )

    # Add figure title
    fig.update_layout(
        title=f""<b>Hubspot - Closed deals this year</b><br><span style='font-size: 14px;'>Total deals: {total_volume} ({total_value} K‚Ç¨) | This week: {var_volume} ({var_value} K‚Ç¨) vs last week</span>"",
        title_font=dict(family=""Arial"", size=20, color=""black""),
        legend=None,
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Weeks"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
    )

    # Set y-axes titles
    fig.update_yaxes(
        title_text=""No of deals"",
        title_font=dict(family=""Arial"", size=11, color=""black""),
        secondary_y=False
    )
    fig.update_yaxes(
        title_text=""Amount in K‚Ç¨"",
        title_font=dict(family=""Arial"", size=11, color=""black""),
        secondary_y=True
    )
    fig.show()
    return fig

fig = create_barchart(df_trend, ""LABEL"", ""GROUP"", ""VALUE"", ""VARV"", ""VARP"")
# Export in HTML
df_trend.to_csv(csv_output, index=False)
fig.write_image(image_output)
fig.write_html(html_output)

# Shave with naas
naas.asset.add(csv_output)
naas.asset.add(image_output)
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
# naas.asset.delete(image_output)
# naas.asset.delete(html_output)"
10143,HubSpot - Get all contacts,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
properties_list = []
df_contacts = hubspot.connect(HS_API_KEY).contacts.get_all(properties_list)
df_contacts"
10144,HubSpot - Update followers from linkedin,"from naas_drivers import hubspot, linkedin
import naas
import pandas as pd
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
naas.scheduler.add(cron=""0 8 * * *"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
properties_list = [
    ""hs_object_id"",
    ""firstname"",
    ""lastname"",
    ""linkedinbio"",
    ""linkedinconnections"",
]
hubspot_contacts = hubspot.connect(HS_API_KEY).contacts.get_all(properties_list)
hubspot_contacts
df_to_update = hubspot_contacts.copy()

# Cleaning
df_to_update = df_to_update.fillna(""Not Defined"")

# Filter on ""Not defined""
df_to_update = df_to_update[(df_to_update.linkedinbio != ""Not Defined"") &
                            (df_to_update.linkedinconnections == ""Not Defined"")]

# Limit to last 50 contacts
df_to_update = df_to_update.sort_values(by=""createdate"", ascending=False)[:50].reset_index(drop=True)

df_to_update
for _, row in df_to_update.iterrows():
    linkedinbio = row.linkedinbio
    
    # Get followers
    df = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(linkedinbio)
    linkedinconnections = df.loc[0, ""FOLLOWERS_COUNT""]
        
    # Get linkedinbio
    df_to_update.loc[_, ""linkedinconnections""] = linkedinconnections
    
df_to_update
for _, row in df_to_update.iterrows():
    # Init data
    data = {}
    
    # Get data
    hs_object_id = row.hs_object_id
    linkedinconnections = row.linkedinconnections

    # Update LK Bio
    if linkedinconnections != None:
        data = {""properties"": {""linkedinconnections"": linkedinconnections}}
    hubspot.connect(HS_API_KEY).contacts.patch(hs_object_id, data)"
10145,HubSpot - Associate contact to deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
deal_id = '3452242690'
contact_id = '245201'
hubspot.connect(HS_API_KEY).associations.create(""deal"",
                                                deal_id,
                                                ""contact"",
                                                contact_id)
print(""üôå Association successfully created !"")"
10146,HubSpot - Send LinkedIn invitations from contacts,"import naas
from naas_drivers import hubspot, linkedin
import pandas as pd
import os
# Enter Token API
HS_API_KEY = ""YOUR_HUBSPOT_API_KEY""

# Column with Linkedin URL (internal name in HubSpot)
col_linkedin = 'linkedinbio'

# Get list of properties you want to have from contacts (internal name in HubSpot)
properties_list = [""email"", ""firstname"", ""lastname"", col_linkedin]
# LinkedIn cookies
LI_AT = naas.secret.get(""LI_AT"")
JSESSIONID = naas.secret.get(""JSESSIONID"")

# LinkedIn limit invitations up to 100 per week (Becareful !)
LIMIT = 15
# CSV to manage and remove profile already in your contact
csv_contact = ""LINKEDIN_EXISTING_CONTACT.csv""

# CSV to manage URL not valid
csv_not_valid = ""LINKEDIN_NOT_VALID.csv""

# CSV to store invitations sent
csv_invitation = ""LINKEDIN_INVITATIONS_SENT.csv""
# Scheduler your invitation everyday at 8:00 AM
naas.scheduler.add(cron=""30 9 * * *"")

# Uncomment the line below to delete your scheduler
# naas.scheduler.delete()
df_contacts = gsheet.connect(SPREADSHEET_URL).get(SHEET_NAME)
df_contacts
def filter_data(df):
    df = df
    return df.reset_index(drop=True)

df_hubspot = filter_data(df_contacts)
df_hubspot
df_lk_invitations = linkedin.connect(LI_AT, JSESSIONID).invitation.get_sent()
df_lk_invitations
def get_csv(output_path):
    df = pd.DataFrame()
    if os.path.exists(output_path):
        df = pd.read_csv(output_path).drop_duplicates()
    return df
df_contacts = get_csv(csv_contact)
df_contacts
df_not_valid = get_csv(csv_not_valid)
df_not_valid
df_csv_invitations = get_csv(csv_invitation)
df_csv_invitations
def get_new_invitations(df_hubspot,
                        df_lk_invitations,
                        df_csv_invitations,
                        df_contacts,
                        df_not_valid):
    # Cleaning
    df = df_hubspot.copy()
    df[""PROFILE_ID""] = df.apply(lambda row: row[col_linkedin].split(""com/in/"")[-1].split(""/"")[0], axis=1)
    print(""‚úîÔ∏è LinkedIn valid URL :"", len(df))
    
    # Get list of pending LinkedIn invitations
    pending_lk_invitations = []
    if len(df_lk_invitations) > 0:
        pending_lk_invitations = df_lk_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending LinkedIn invitations :"", len(pending_lk_invitations))
    
    # Get list of CSV invitations
    pending_csv_invitations = []
    if len(df_csv_invitations) > 0:
        pending_csv_invitations = df_csv_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending CSV invitations :"", len(pending_csv_invitations))
    
    # Get profile already in network
    contacts = []
    if len(df_contacts) > 0:
        contacts = df_contacts[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Already in network :"", len(contacts))
    
    # Get profile not valid
    not_valids = []
    if len(df_not_valid) > 0:
        not_valids = df_not_valid[""PROFILE_ID""].unique().tolist()
    print(""‚ùå Profile not valid:"", len(not_valids))
    
    # Remove pending invitations / already in network / not valid profile from dataframe 
    exclude = (pending_lk_invitations + pending_csv_invitations + contacts + not_valids)
    df = df[~df[""PROFILE_ID""].isin(exclude)].reset_index(drop=True)
    print(""‚û°Ô∏è New invitation:"", len(df))
    return df

df_new_invitations = get_new_invitations(df_hubspot,
                                         df_lk_invitations,
                                         df_csv_invitations,
                                         df_contacts,
                                         df_not_valid)
df_new_invitations
def send_invitation(df,
                    df_not_valid=None,
                    df_contacts=None,
                    df_csv_invitations=None):
    # Check if new invitations to perform
    if len(df) == 0:
        print(""ü§ô No new invitations to send"")
        return df
    
    # Setup variables
    if df_not_valid is None:
        df_not_valid = pd.DataFrame()
    if df_contacts is None:
        df_contacts = pd.DataFrame()
    if df_csv_invitations is None:
        df_csv_invitations = pd.DataFrame()
        
    # Loop
    count = 1
    for index, row in df.iterrows():
        df_network = pd.DataFrame()
        profile = row[col_linkedin]
        print(f""‚û°Ô∏è Checking :"", profile)
        
        # Get distance with profile
        try:
            df_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile)
        except Exception as e:
            # If error, profile URL is not valid => append Notion page to CSV not valid to not check it again
            df_not_valid = pd.concat([df_not_valid, df[index:index+1]])
            df_not_valid.to_csv(csv_not_valid, index=False)
            print(""‚ùå URL not valid"", e)
            
        # Check if profile is already in your network
        if len(df_network) > 0:
            distance = df_network.loc[0, ""DISTANCE""]
            # If not profile in network...
            if distance not in [""SELF"", ""DISTANCE_1""]:
                # => send invitation
                try:
                    linkedin.connect(LI_AT, JSESSIONID).invitation.send(recipient_url=profile)
                    print(count, ""- üôå Invitation successfully sent"")
                    df_csv_invitations = pd.concat([df_csv_invitations, df_network])
                    df_csv_invitations.to_csv(csv_invitation, index=False)
                except Exception as e:
                    print(""‚ùå Invitation not sent"", e)
                count += 1
            else:
                # If profile already in your network => append network result to CSV existing contact to not check it again
                df_contacts = pd.concat([df_contacts, df_network])
                df_contacts.to_csv(csv_contact, index=False)
                print(f""üëç Already in my network, üíæ saved in CSV"")
            
        # Manage LinkedIn limit
        if count > LIMIT:
            print(""‚ö†Ô∏è LinkedIn invitation limit reached"", LIMIT)
            return df_csv_invitations
    return df_csv_invitations
        
df_csv_invitations = send_invitation(df_new_invitations,
                                     df_not_valid,
                                     df_contacts,
                                     df_csv_invitations)
if os.path.exists(csv_contact):
    naas.dependency.add(csv_contact)

if os.path.exists(csv_not_valid):
    naas.dependency.add(csv_not_valid)
    
if os.path.exists(csv_invitation):
    naas.dependency.add(csv_invitation)
df_csv_invitations"
10147,HubSpot - Send sales brief,"from naas_drivers import emailbuilder, hubspot
import naas
import pandas as pd
from datetime import datetime
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
# Receivers
email_to = [""your_email_adresse""]

# Email subject
email_subject = f""üöÄ Hubspot - Sales Brief as of {datetime.now().strftime('%d/%m/%Y')} (Draft)""
objective = 300000
df_pipelines = hubspot.connect(HS_API_KEY).pipelines.get_all()
df_pipelines
pipeline_id = ""8432671""
HUBSPOT_CARD = ""https://lib.umso.co/lib_sluGpRGQOLtkyEpz/na1lz0v9ejyurau2.png?w=1200&h=900&fit=max&dpr=2""
NAAS_WEBSITE = ""https://www.naas.ai""
EMAIL_DESCRIPTION = ""Your sales brief""
DATE_FORMAT = ""%Y-%m-%d""
#-> Uncomment the 2 lines below (by removing the hashtag) to schedule your job every monday at 8:00 AM (NB: you can choose the time of your scheduling bot)
# import naas
# naas.scheduler.add(cron=""0 8 * * 1"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
df_dealstages = df_pipelines.copy()

# Filter on pipeline
df_dealstages = df_dealstages[df_dealstages.pipeline_id == pipeline_id]

df_dealstages
properties = [
    ""hs_object_id"",
    ""dealname"",
    ""dealstage"",
    ""pipeline"",
    ""createdate"",
    ""hs_lastmodifieddate"",
    ""closedate"",
    ""amount""
]
df_deals = hubspot.connect(HS_API_KEY).deals.get_all(properties)

# Filter on pipeline
df_deals = df_deals[df_deals.pipeline == pipeline_id].reset_index(drop=True)

df_deals
def format_number(num):
    NUMBER_FORMAT = ""{:,.0f} ‚Ç¨""
    num = str(NUMBER_FORMAT.format(num)).replace("","", "" "")
    return num
def format_pourcentage(num):
    NUMBER_FORMAT = ""{:,.0%}""
    num = str(NUMBER_FORMAT.format(num))
    return num
def format_varv(num):
    NUMBER_FORMAT = ""+{:,.0f} ‚Ç¨""
    num = str(NUMBER_FORMAT.format(num)).replace("","", "" "")
    return num
df_sales = pd.merge(df_deals.drop(""pipeline"", axis=1),
                    df_dealstages.drop([""pipeline"", ""pipeline_id"", ""createdAt"", ""updatedAt"", ""archived""], axis=1),
                    left_on=""dealstage"",
                    right_on=""dealstage_id"",
                    how=""left"")
df_sales
df_sales_c = df_sales.copy()

# Cleaning
df_sales_c[""amount""] = df_sales_c[""amount""].fillna(""0"")
df_sales_c.loc[df_sales_c[""amount""] == """", ""amount""] = ""0""

# Formatting
df_sales_c[""amount""] = df_sales_c[""amount""].astype(float)
df_sales_c[""probability""] =  df_sales_c[""probability""].astype(float)
df_sales_c.createdate = pd.to_datetime(df_sales_c.createdate)
df_sales_c.hs_lastmodifieddate = pd.to_datetime(df_sales_c.hs_lastmodifieddate)
df_sales_c.closedate = pd.to_datetime(df_sales_c.closedate)

# Calc
df_sales_c[""forecasted""] = df_sales_c[""amount""] * df_sales_c[""probability""]

df_sales_c
df_details = df_sales_c.copy()

# Groupby
to_group = [
    ""dealstage_label"",
    ""probability"",
    ""displayOrder""
]
to_agg = {
    ""amount"": ""sum"",
    ""dealname"": ""count"",
    ""forecasted"": ""sum""
}
df_details = df_details.groupby(to_group, as_index=False).agg(to_agg)

# Sort
df_details = df_details.sort_values(""displayOrder"")

df_details
forecasted = df_details.forecasted.sum()
forecasted
won = df_details[df_details[""probability""] == 1].forecasted.sum()
won
weighted = df_details[df_details[""probability""] < 1].forecasted.sum()
weighted
completion_p = forecasted / objective
completion_p
completion_v = objective - forecasted
completion_v
today = datetime.now().strftime(DATE_FORMAT)
today
df = df_details.copy()

details = []

for _, row in df.iterrows():
    # status part
    dealstage = row.dealstage_label
    probability = row.probability
    detail = f""{dealstage} ({format_pourcentage(probability)})""
    
    # amount part
    amount = row.amount
    number = row.dealname
    forecasted_ = row.forecasted
    if (probability < 1 and probability > 0):
        detail = f""{detail}: <ul><li>Amount : {format_number(amount)}</li><li>Number : {number}</li><li>Weighted amount : <b>{format_number(forecasted_)}</b></li></ul>""
    else:
        detail = f""{detail}: {format_number(amount)}""
        
    details += [detail]

details
df_inactive = df_sales_c.copy()

df_inactive.hs_lastmodifieddate = pd.to_datetime(df_inactive.hs_lastmodifieddate).dt.strftime(DATE_FORMAT)

df_inactive[""inactive_time""] = (datetime.now() - pd.to_datetime(df_inactive.hs_lastmodifieddate, format=DATE_FORMAT)).dt.days
df_inactive.loc[(df_inactive[""inactive_time""] > 30, ""inactive"")] = ""inactive""
df_inactive = df_inactive[(df_inactive.inactive == 'inactive') &
                          (df_inactive.amount != 0) & 
                          (df_inactive.probability > 0.) & 
                          (df_inactive.probability < 1)].sort_values(""amount"", ascending=False).reset_index(drop=True)

df_inactive
inactives = []

for _, row in df_inactive[:10].iterrows():
    # status part
    dealname = row.dealname
    dealstage_label = row.dealstage_label
    amount = row.amount
    probability = row.probability
    inactive = f""{dealname} ({dealstage_label}): <b>{format_number(amount)}</b>""
    inactives += [inactive]

inactives
import plotly.graph_objects as go

fig = go.Figure(go.Waterfall(name=""20"",
                             orientation = ""v"",
                             measure = [""relative"", ""relative"", ""total"", ""relative"", ""total""],
                             x = [""Won"", ""Pipeline"", ""Forecast"", ""Missing"", ""Objective""],
                             textposition = ""outside"",
                             text = [format_number(won), format_varv(weighted), format_number(forecasted), format_varv(completion_v), format_number(objective)],
                             y = [won, weighted, forecasted, completion_v, objective],
                            decreasing = {""marker"":{""color"":""#33475b""}},
                            increasing = {""marker"":{""color"":""#33475b""}},
                            totals = {""marker"":{""color"":""#ff7a59""}}
))


fig.update_layout(title = ""Sales Metrics"", plot_bgcolor=""#ffffff"", hovermode='x')
fig.update_yaxes(tickprefix=""‚Ç¨"", gridcolor='#eaeaea')
fig.show()
fig.write_html(""GRAPH_FILE.html"")
fig.write_image(""GRAPH_IMG.png"")

params = {""inline"": True}

graph_url = naas.asset.add(""GRAPH_FILE.html"", params=params)
graph_image = naas.asset.add(""GRAPH_IMG.png"")
def email_brief(today,
                forecasted,
                won,
                weighted,
                objective,
                completion_p,
                completion_v,
                details,
                inactives
                ):
    content = {
        'title': (f""<a href='{NAAS_WEBSITE}'>""
                   f""<img align='center' width='100%' target='_blank' style='border-radius:5px;'""
                   f""src='{HUBSPOT_CARD}' alt={EMAIL_DESCRIPTION}/>""
                   ""</a>""),
        
        'txt_intro': (f""Hi there,<br><br>""
                      f""Here is your weekly sales email as of {today}.""),

        'title_1': emailbuilder.text(""Overview"", font_size=""27px"", text_align=""center"", bold=True),
        ""text_1"": emailbuilder.text(f""As of today, your yearly forecasted revenue is {format_number(forecasted)}.""),
        ""list_1"": emailbuilder.list([f""Won : {format_number(won)}"",
                                     f""Weighted pipeline : <b>{format_number(weighted)}</b>""]),
        ""text_1_2"": emailbuilder.text(f""You need to find üëâ <u>{format_number(completion_v)}</u> to reach your goal !""),
        ""text_1_1"": emailbuilder.text(f""Your yearly objective is {format_number(objective)} ({format_pourcentage(completion_p)} completion).""),
        'image_1': emailbuilder.image(graph_image, link=graph_url),
        
        'title_2': emailbuilder.text(""üöÄ Pipeline"", font_size=""27px"", text_align=""center"", bold=True),
        ""list_2"": emailbuilder.list(details),

        'title_3': emailbuilder.text(""üßê Actions needed"", font_size=""27px"", text_align=""center"", bold=True),
        'text_3': emailbuilder.text(""Here are deals where you need to take actions :""),
        'list_3': emailbuilder.list(inactives),
        'text_3_1': emailbuilder.text(""If you need more details, connect to Hubspot with the link below.""),
        'button_1': emailbuilder.button(link=""https://app.hubspot.com/"",
                                        text=""Go to Hubspot"",
                                        background_color=""#ff7a59""),
        
        'title_4': emailbuilder.text(""Glossary"", text_align=""center"", bold=True, underline=True),
        'list_4': emailbuilder.list([""Yearly forecasted revenue :  Weighted amount + WON exclude LOST"",
                                     ""Yearly objective : Input in script"",
                                     ""Inactive deal : No activity for more than 30 days""]),
        
        'footer_cs': emailbuilder.footer_company(naas=True),
    }
    
    email_content = emailbuilder.generate(display='iframe', **content)
    return email_content

email_content = email_brief(today,
                            forecasted,
                            won,
                            weighted,
                            objective,
                            completion_p,
                            completion_v,
                            details,
                            inactives)
naas.notification.send(email_to,
                       email_subject,
                       email_content)"
10148,HubSpot - Delete contact,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
contact_id = ""280751""
hubspot.connect(HS_API_KEY).contacts.delete(contact_id)"
10149,HubSpot - Update jobtitle country industry from linkedin,"from naas_drivers import hubspot, linkedin
import naas
import pandas as pd
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
naas.scheduler.add(cron=""0 8 * * *"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
properties_list = [
    ""hs_object_id"",
    ""firstname"",
    ""lastname"",
    ""linkedinbio"",
    ""jobtitle"",
    ""country"",
    ""industry"",
]
hubspot_contacts = hubspot.connect(HS_API_KEY).contacts.get_all(properties_list)
hubspot_contacts
df_to_update = hubspot_contacts.copy()

# Cleaning
df_to_update = df_to_update.fillna(""Not Defined"")

# Filter on ""Not defined""
df_to_update = df_to_update[(df_to_update.linkedinbio != ""Not Defined"") &
                            (df_to_update.jobtitle == ""Not Defined"") &
                            (df_to_update.country == ""Not Defined"") & 
                            (df_to_update.industry == ""Not Defined"")]

# Limit to last 50 contacts
df_to_update = df_to_update.sort_values(by=""createdate"", ascending=False)[:50].reset_index(drop=True)

df_to_update
for _, row in df_to_update.iterrows():
    linkedinbio = row.linkedinbio
    
    # Get followers
    df = linkedin.connect(LI_AT, JSESSIONID).profile.get_identity(linkedinbio)
    jobtitle = df.loc[0, ""OCCUPATION""]
    industry = df.loc[0, ""INDUSTRY_NAME""]
    country = df.loc[0, ""COUNTRY""]
        
    # Get linkedinbio
    df_to_update.loc[_, ""jobtitle""] = jobtitle
    df_to_update.loc[_, ""industry""] = industry
    df_to_update.loc[_, ""country""] = country
    
df_to_update
for _, row in df_to_update.iterrows():
    # Init data
    data = {}
    
    # Get data
    hs_object_id = row.hs_object_id
    jobtitle = row.jobtitle
    industry = row.industry
    country = row.country

    # Update 
    if jobtitle != None:
        data = {""properties"": 
                {""jobtitle"": jobtitle,
                 ""industry"": industry,
                 ""country"": country}
               }
    hubspot.connect(HS_API_KEY).contacts.patch(hs_object_id, data)"
10150,HubSpot - Create Task,"from datetime import datetime, timedelta
import requests
import json
HS_API_TOKEN = ""YOUR_HUBSPOT_API_KEY"" 
# Assign owner ID
owner_id = 111111086

#Associated contact ID
asso_contactids=1551

# Time delay to

#Associated contact IDset due date for tasks in days
time_delay = 10


# Task data
subject = ""My Third task""
body = ""Call contacts""
status = ""NOT_STARTED"" # NOT_STARTED | COMPLETED | IN_PROGRESS | WAITING | DEFERRED
def create_task(owner_id,
                time_delay,
                subject,
                body,
                status,
                asso_contactids=[],
                asso_companyids=[],
                asso_dealids=[],
                asso_ownerids=[],
                engagement=""TASK""):
    """"""
    Engagement type = TASK | NOTE | EMAIL | MEETING | CALL 
    """"""
    
    # Calc timestamp
    tstampobj = datetime.now() + timedelta(days=time_delay)
    tstamp = tstampobj.timestamp() * 1000
     
    # Create payload
    payload = json.dumps({
        ""engagement"": {
            ""active"": 'true',
            ""ownerId"": owner_id,
            ""type"": engagement,
            ""timestamp"": tstamp
        },
        ""associations"": {
            ""contactIds"": [1551],
            ""companyIds"": asso_companyids,
            ""dealIds"": asso_dealids,
            ""ownerIds"": [owner_id],
        },
        ""metadata"": {
            ""body"": body,
            ""subject"": subject,
            ""status"": status,
        }
    })
    url = ""https://api.hubapi.com/engagements/v1/engagements""
    params = {""hapikey"": HS_API_TOKEN}
    headers = {'Content-Type': ""application/json""}
    # Post requests
    res = requests.post(url,
                        data=payload,
                        headers=headers,
                        params=params)
    # Check requests
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        raise (e)
    res_json = res.json()
    
    # Fetch the task id of the current task created
    task_id = res_json.get(""engagement"").get(""id"")
    print(""üéÜ Task created successfully: "", task_id)
    return res_json
create_task(owner_id,
            time_delay,
            subject,
            body,
            status)
"
10151,HubSpot - Delete Task,"from datetime import datetime, timedelta
import requests, math
import json
HS_API_TOKEN = ""YOUR_HUBSPOT_API_KEY"" 
task_id = 19996680052
def delete_task(uid):
    #set headers
    params = {""hapikey"": HS_API_TOKEN}
    headers = {'Content-Type': ""application/json""}
    
    #check if tasks already exist
    get_url = ""https://api.hubapi.com/engagements/v1/engagements/""
    get_res = requests.get(
                url=f""{get_url}/{uid}"",
                headers= headers,
                params= params,
                allow_redirects=False,    
    )
    if get_res.status_code == 200:
        print(""Task found..Deleting"")
        #delete task
        del_url = ""http://api.hubapi.com/engagements/v1/engagements/""
    
        # Post requests
        res = requests.delete(
                url=f""{url}/{uid}"",
                headers= headers,
                params= params,
                allow_redirects=False,
            )
    else:
        print(""Task not found. Unable to delete"")
delete_task(task_id)
"
10152,HubSpot - Delete deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
deal_id = '3501002068'
hubspot.connect(HS_API_KEY).deals.delete(deal_id)"
10153,HubSpot - Create deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
dealname = ""TEST""
closedate = None #must be in format %Y-%m-%d
amount = None
hubspot_owner_id = None
df_pipelines = hubspot.connect(HS_API_KEY).pipelines.get_all()
df_pipelines
dealstage = '5102584'
send_deal = {""properties"": 
                  {
                    ""dealstage"": dealstage,
                    ""dealname"": dealname,
                    ""amount"": amount,
                    ""closedate"": closedate,
                    ""hubspot_owner_id"": hubspot_owner_id,
                   }
                 }

deal1 = hubspot.connect(HS_API_KEY).deals.send(send_deal)
deal2 = hubspot.connect(HS_API_KEY).deals.create(
    dealname,
    dealstage,
    closedate
)
deal1
deal2"
10154,HubSpot - Update deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
deal_id = ""3501002068""
dealname = ""TEST""
dealstage = '5102584'
closedate = '2021-12-31' #date format must be %Y-%m-%d
amount = '100.50'
hubspot_owner_id = None
update_deal = {""properties"": 
                  {
                    ""dealstage"": dealstage,
                    ""dealname"": dealname,
                    ""amount"": amount,
                    ""closedate"": closedate,
                    ""hubspot_owner_id"": hubspot_owner_id,
                   }
                 }

deal1 = hubspot.connect(HS_API_KEY).deals.patch(deal_id,
                                                update_deal)
deal2 = hubspot.connect(HS_API_KEY).deals.update(
    deal_id,
    dealname,
    dealstage,
    closedate,
    amount,
    hubspot_owner_id
)
deal1
deal2"
10155,HubSpot - Get all pipelines and dealstages,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
df_pipelines = hubspot.connect(HS_API_KEY).pipelines.get_all()
df_pipelines"
10156,HubSpot - Update Task,"from datetime import datetime, timedelta
from naas_drivers import hubspot
import requests, math
import json
HS_API_TOKEN = ""YOUR_HUBSPOT_API_KEY"" 
task_id = 19996848972
owner_id = 111111086
subject = ""Task updated""
body = ""This is third update""
time_delay = 1
def patch(uid, data):
    #set headers
    params = {""hapikey"": HS_API_TOKEN}
    headers = {'Content-Type': ""application/json""}
    base_url = ""https://api.hubapi.com/engagements/v1/engagements/""
    res = requests.patch(
         url=f""{base_url}/{uid}"",
         headers=headers,
         params=params,
         json=data,
         allow_redirects=False,
    )
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        return e
    # Message success
    print(f""‚úîÔ∏è successfully updated."")
    return res.json()
def update_task(task_id,owner_id,subject,body,time_delay):
    
    #set headers
    url = ""https://api.hubapi.com/engagements/v1/engagements/""
    params = {""hapikey"": HS_API_TOKEN}
    headers = {'Content-Type': ""application/json""}
    
    # Calc timestamp to set delay
    tstampobj = datetime.now() + timedelta(days=time_delay)
    tstamp = tstampobj.timestamp() * 1000

    #check if tasks already exist
    
    get_task = requests.get(
                url=f""{get_url}/{task_id}"",
                headers= headers,
                params= params,
                allow_redirects=False,    
    )
    
    if get_task.status_code == 200:
        print(""Task found..Updating"")
        
        # Update Task
        data = {
            ""engagement"": {
                ""ownerId"": owner_id,
                ""timestamp"": tstamp,
            },
            ""metadata"": {
                #""type"": ""TASK"",
                ""body"": body,
                #""status"": ""NOT_STARTED"", # NOT_STARTED | COMPLETED | IN_PROGRESS | WAITING | DEFERRED
                #""forObjectType"": ""CONTACT"",
                #""subject"": subject,
            }
        }
       
        patch(task_id,data)
        
    else:
        print(""Task not found. Unable to update"")
update_task(task_id,owner_id,subject,body,time_delay)"
10157,HubSpot - Get all deals,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
properties_list = []
df_deals = hubspot.connect(HS_API_KEY).deals.get_all(properties_list)
df_deals"
10158,HubSpot - Send deals to gsheet,"from naas_drivers import hubspot, gsheet
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
properties_list = []
spreadsheet_id = """"
sheet_name = """"
df_deals = hubspot.connect(HS_API_KEY).deals.get_all(properties_list)
gsheet.connect(spreadsheet_id).send(
    sheet_name=sheet_name,
    data=df_deals,
    append=False
)"
10159,HubSpot - Update linkedinbio from google,"from naas_drivers import hubspot
import naas
import pandas as pd
from googlesearch import search
import time
import re
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
naas.scheduler.add(cron=""0 8 * * *"")

# -> Uncomment the line below (by removing the hashtag) and execute this cell to remove your scheduler
# naas.scheduler.delete()
properties_list = [
    ""hs_object_id"",
    ""firstname"",
    ""lastname"",
    ""linkedinbio"",
]
hubspot_contacts = hubspot.connect(HS_API_KEY).contacts.get_all(properties_list)
hubspot_contacts
df_to_update = hubspot_contacts.copy()

# Cleaning
df_to_update = df_to_update.fillna(""Not Defined"")

# Filter on ""Not defined""
df_to_update = df_to_update[(df_to_update.firstname != ""Not Defined"") & 
                            (df_to_update.lastname != ""Not Defined"") &
                            (df_to_update.linkedinbio == ""Not Defined"")].reset_index(drop=True)

df_to_update
def get_bio(firstname, lastname):
    # Init linkedinbio
    linkedinbio = None
    
    # Create query
    query = f""{firstname}+{lastname}+Linkedin""
    print(""Google query: "", query)
    
    # Search in Google
    for i in search(query, tld=""com"", num=10, stop=10, pause=2):
        pattern = ""https:\/\/.+.linkedin.com\/in\/.([^?])+""
        result = re.search(pattern, i)

        # Return value if result is not None
        if result != None:
            linkedinbio = result.group(0).replace("" "", """")
            return linkedinbio
        else:
            time.sleep(2)
    return linkedinbio
for _, row in df_to_update.iterrows():
    firstname = row.firstname
    lastname = row.lastname
    
    # Get linkedinbio
    linkedinbio = get_bio(firstname, lastname)
    df_to_update.loc[_, ""linkedinbio""] = linkedinbio
    
df_to_update
for _, row in df_to_update.iterrows():
    # Init data
    data = {}
    
    # Get data
    hs_object_id = row.hs_object_id
    linkedinbio = row.linkedinbio

    # Update LK Bio
    if linkedinbio != None:
        data = {""properties"": {""linkedinbio"": linkedinbio}}
    hubspot.connect(HS_API_KEY).contacts.patch(hs_object_id, data)"
10160,HubSpot - Get new deals created weekly,"from naas_drivers import hubspot, emailbuilder
from datetime import datetime, timedelta
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import naas
HS_API_KEY = ""YOUR_API_KEY""
df_pipelines = hubspot.connect(HS_API_KEY).pipelines.get_all()
df_pipelines
pipeline_id = ""000000""
# Scheduler at 08:00 on Monday and Friday.
naas.scheduler.add(cron=""0 8 * * 1,5"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
# Email info
EMAIL_TO = ""YOUR_EMAIL""
EMAIL_SUBJECT = ""[HubSpot] Weekly update : New deals created""
name_output = ""HubSpot_new_deals_weekly""
csv_output = f""{name_output}.csv""
image_output = f""{name_output}.png""
html_output = f""{name_output}.html""
df_deals = hubspot.connect(HS_API_KEY).deals.get_all()
df_deals
def get_trend(df_deals, pipeline):
    df = df_deals.copy()
    # Filter data
    df = df[df[""pipeline""].astype(str) == str(pipeline)]
    
    # Prep data
    df[""createdate""] = pd.to_datetime(df[""createdate""])
    df[""amount""] = df.apply(lambda row: float(row[""amount""]) if str(row[""amount""]) not in [""None"", """"] else 0, axis=1)
    
    # Calc by week
    df = df.groupby(pd.Grouper(freq='W', key='createdate')).agg({""hs_object_id"": ""count"", ""amount"": ""sum""}).reset_index()
    df[""createdate""] = df[""createdate""] + timedelta(days=-1)
    df = pd.melt(df, id_vars=""createdate"")
    
    # Rename col
    to_rename = {
        ""createdate"": ""LABEL_ORDER"",
        ""variable"": ""GROUP"",
        ""value"": ""VALUE""
    }
    df = df.rename(columns=to_rename).replace(""hs_object_id"", ""No of deals"").replace(""amount"", ""Amount"")
    df[""YEAR""] = df[""LABEL_ORDER""].dt.strftime(""%Y"")
    df = df[df[""YEAR""] == datetime.now().strftime(""%Y"")]
    df[""LABEL""] = df[""LABEL_ORDER""].dt.strftime(""%Y-W%U"")
    df[""LABEL_ORDER""] = df[""LABEL_ORDER""].dt.strftime(""%Y%m%d"")
    
    # Calc variation
    df_var = pd.DataFrame()
    groups = df.GROUP.unique()
    for group in groups:
        tmp = df[df.GROUP == group].reset_index(drop=True)
        for idx, row in tmp.iterrows():
            if idx == 0:
                value_n1 = 0
            else:
                value_n1 = tmp.loc[tmp.index[idx-1], ""VALUE""]
            tmp.loc[tmp.index[idx], ""VALUE_COMP""] = value_n1
        df_var = pd.concat([df_var, tmp]).fillna(0).reset_index(drop=True)
    df_var[""VARV""] = df_var[""VALUE""] - df_var[""VALUE_COMP""]
    df_var[""VARP""] = df_var[""VARV""] / abs(df_var[""VALUE_COMP""])
    
    # Prep data
    df_var[""VALUE_D""] = df_var[""VALUE""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df_var[""VARV_D""] = df_var[""VARV""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df_var.loc[df_var[""VARV""] > 0, ""VARV_D""] = ""+"" + df_var[""VARV_D""]
    df_var[""VARP_D""] = df_var[""VARP""].map(""{:,.0%}"".format).str.replace("","", "" "")
    df_var.loc[df_var[""VARP""] > 0, ""VARP_D""] = ""+"" + df_var[""VARP_D""]
    
    # Create hovertext
    df_var[""TEXT""] = (""<b>Deal created as of "" + df_var[""LABEL""] + "" : </b>"" + 
                      df_var[""VALUE_D""] + ""<br>"" + 
                      df_var[""VARP_D""] + "" vs last week ("" + df_var[""VARV_D""] + "")"")
    return df_var

df_trend = get_trend(df_deals, pipeline_id)
df_trend
def create_barchart(df, label, group, value, varv, varp):    
    # Create figure with secondary y-axis
    fig = make_subplots(specs=[[{""secondary_y"": True}]])

    # Add traces
    df1 = df[df[group] == ""No of deals""].reset_index(drop=True)[:]
    total_volume = ""{:,.0f}"".format(df1[value].sum()).replace("","", "" "")
    var_volume = df1.loc[df1.index[-1], varv]
    positive = False
    if var_volume > 0:
        positive = True
    var_volume = ""{:,.0f}"".format(var_volume).replace("","", "" "")
    if positive:
        var_volume = f""+{var_volume}""
    fig.add_trace(
        go.Bar(
            name=""No of deals"",
            x=df1[label],
            y=df1[value],
            offsetgroup=0,
            hoverinfo=""text"",
            text=df1[""VALUE_D""],
            hovertext=df1[""TEXT""],
            marker=dict(color=""#33475b"")
        ),
        secondary_y=False,
    )
    
    df2 = df[df[group] == ""Amount""].reset_index(drop=True)[:]
    total_value = ""{:,.0f}"".format(df2[value].sum()).replace("","", "" "")
    var_value = df2.loc[df2.index[-1], varv]
    positive = False
    if var_value > 0:
        positive = True
    var_value = ""{:,.0f}"".format(var_value).replace("","", "" "")
    if positive:
        var_value = f""+{var_value}""
    fig.add_trace(
        go.Bar(
            name=""Amount"",
            x=df2[label],
            y=df2[value],
            text=df2[""VALUE_D""] + "" K‚Ç¨"",
            offsetgroup=1,
            hoverinfo=""text"",
            hovertext=df2[""TEXT""],
            marker=dict(color=""#ff7a59"")
        ),
        secondary_y=True,
    )

    # Add figure title
    fig.update_layout(
        title=f""<b>Hubspot - New deals created per week</b><br><span style='font-size: 14px;'>Total deals: {total_volume} ({total_value} K‚Ç¨) | This week: {var_volume} ({var_value} K‚Ç¨) vs last week</span>"",
        title_font=dict(family=""Arial"", size=20, color=""black""),
        legend=None,
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Weeks"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
    )

    # Set y-axes titles
    fig.update_yaxes(
        title_text=""No of deals"",
        title_font=dict(family=""Arial"", size=11, color=""black""),
        secondary_y=False
    )
    fig.update_yaxes(
        title_text=""Amount in K‚Ç¨"",
        title_font=dict(family=""Arial"", size=11, color=""black""),
        secondary_y=True
    )
    fig.show()
    return fig

fig = create_barchart(df_trend, ""LABEL"", ""GROUP"", ""VALUE"", ""VARV"", ""VARP"")
# Export in HTML
df_trend.to_csv(csv_output, index=False)
fig.write_image(image_output)
fig.write_html(html_output)

# Shave with naas
csv_url = naas.asset.add(csv_output)
image_url = naas.asset.add(image_output)
html_url = naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below (by removing the hashtag)  to delete your asset
# naas.asset.delete(csv_output)
# naas.asset.delete(image_output)
# naas.asset.delete(html_output)
def create_email():
    content = {
        ""header"": emailbuilder.image(src=""https://lib.umso.co/lib_sluGpRGQOLtkyEpz/na1lz0v9ejyurau2.png?w=1200&h=900&fit=max&dpr=2"",
                                     link=""https://www.hubspot.com/"",
                                     align=""center"",
                                     width=""100%""),
        ""txt_0"": emailbuilder.text(""Hi !<br><br>""
                                   f""Here below your weekly update on new deal created as of {datetime.now().strftime('%Y-%m-%d')} :<br>""),
        ""image"": emailbuilder.image(src=image_url,
                                    link=html_url,
                                    align=""center"",
                                    width=""100%""),
        ""button_1"": emailbuilder.button(link=""https://app.hubspot.com"",
                                        text=""Go to HubSpot"",
                                        color=""white"",
                                        background_color=""#ff7a59""),
        ""txt_4"": (""Interested to improve this template, please contact the Naas Core Team at hello@naas.ai.<br><br>""),
        ""heading_5"": emailbuilder.text(""Let's close those opportunities üí∏!""),
        ""footer"": emailbuilder.footer_company(naas=True)
    }
    return emailbuilder.generate(display='iframe', **content)

email_content = create_email()
naas.notification.send(EMAIL_TO,
                       EMAIL_SUBJECT,
                       email_content)"
10161,HubSpot - Get contact,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
contact_id = ""253101""
contact = hubspot.connect(HS_API_KEY).contacts.get(contact_id)
contact"
10162,HubSpot - Send contacts to gsheet,"from naas_drivers import hubspot, gsheet
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
properties_list = []
spreadsheet_id = """"
sheet_name = """"
df_contacts = hubspot.connect(HS_API_KEY).contacts.get_all(properties_list)
gsheet.connect(spreadsheet_id).send(
    sheet_name=sheet_name,
    data=df_contacts,
    append=False
)"
10163,HubSpot - Get contacts associated to deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
deal_id = '3452242690'
result = hubspot.connect(HS_API_KEY).associations.get('deal', 
                                                      deal_id,
                                                      'contact')
result"
10164,HubSpot - Get deal,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
deal_id = '70915045'
deal = hubspot.connect(HS_API_KEY).deals.get(deal_id)
deal"
10165,HubSpot - Create contacts from linkedin post likes,"from naas_drivers import linkedin, hubspot
import naas
import requests
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
# Coookies
LI_AT = ""AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2"" # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = ""ajax:837990740022038xxxxx"" # EXAMPLE ajax:8379907400220387585

# Post URL
POST_URL = ""----""
df_posts = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(POST_URL)

# Display the number of likes
print(""Number of likes: "", df_posts.PROFILE_URN.count())
# Show dataframe with list of profiles from likes 
df_posts
def create_contacts_from_post(df,
                              c_profile_urn=""PROFILE_URN"",
                              c_firstname=""FIRSTNAME"",
                              c_lastname=""LASTNAME"",
                              c_occupation=""OCCUPATION""):
    for _, row in df.iterrows():
        profile_urn = row[c_profile_urn]
        firstname = row[c_firstname]
        lastname = row[c_lastname]
        occupation = row[c_occupation]
        linkedinbio = f""https://www.linkedin.com/in/{profile_urn}""
        email = None
        phone = None

        # contact
        try:
            contact = linkedin.connect(LI_AT, JSESSIONID).profile.get_contact(linkedinbio)
            email = contact.loc[0, ""EMAIL""]
            phone = contact.loc[0, ""PHONENUMBER""]
        except:
            print(""No contact info"")

        # With send method
        data = {""properties"": 
                {
                    ""linkedinbio"": linkedinbio,
                    ""firstname"": firstname,
                    ""lastname"": lastname,
                    ""jobtitle"": occupation,
                    ""email"": email,
                    ""phone"": phone,
                }
               }
        print(data)
        hubspot.connect(HS_API_KEY).contacts.send(data)
create_contacts_from_post(df_posts)"
10166,HubSpot - Update contact,"from naas_drivers import hubspot
HS_API_KEY = 'YOUR_HUBSPOT_API_KEY'
contact_id = ""280751""
email = ""test@cashstory.com""
firstname = ""Jean test""
lastname ='CASHSTOrY'
phone = ""+336.00.00.00.00""
jobtitle = ""Consultant""
website = ""www.cashstory.com""
company = 'CASHSTORY'
hubspot_owner_id = None
update_contact = {""properties"": 
                  {
                    ""email"": email,
                    ""firstname"": firstname,
                    ""lastname"": lastname,
                    ""phone"": phone,
                    ""jobtitle"": jobtitle,
                    ""website"": website,
                    ""company"": company,
                    ""url"": ""test3"",
                    ""hubspot_owner_id"": hubspot_owner_id,
                   }
                 }

contact_id1 = hubspot.connect(HS_API_KEY).contacts.patch(contact_id,
                                                         update_contact)
contact_id2 = hubspot.connect(HS_API_KEY).contacts.update(
    contact_id,
    email,
    firstname,
    lastname,
    phone,
    jobtitle,
    website,
    company,
    hubspot_owner_id
)
contact_id1
contact_id2"
10167,HubSpot - Get Task,"from datetime import datetime, timedelta
import requests, math
import json
HS_API_TOKEN = ""YOUR_HUBSPOT_API_KEY"" 
contact_id = 1551
owner_id = 111111086

# Time delay to get tasks created since N days, where N is no of days. For ex. Get tasks created since 1 day
time_delay = 10

#Number of tasks to be retrieved
no_of_tasks = 10
def get_task(contact_id,owner_id,time_delay,no_of_tasks):
    """"""
    Engagement type = TASK  
    """"""
    
    # Calc timestamp
    Previous_Date = datetime.now() - timedelta(days=time_delay)
    Previous_tstamp = Previous_Date.timestamp() * 1000
    Previous_tstamp = math.trunc(Previous_tstamp)
     
    
    url = ""https://api.hubapi.com/engagements/v1/engagements/recent/modified""
    params = {""hapikey"": HS_API_TOKEN,""since"":Previous_tstamp,""count"":no_of_tasks}
    headers = {'Content-Type': ""application/json""}
    # Post requests
    res = requests.get(url,headers=headers,params=params)
    
    if res.status_code == 200:
        
        res_json = res.json()

        # Check requests
        try:
            res.raise_for_status()
        except requests.HTTPError as e:
            raise (e)
        res_json = res.json()

        return res_json
    else:
        print(""Task not found"")
   
results = get_task(contact_id,owner_id,time_delay,no_of_tasks)
for key in results[""results""]:
    print(""---------------"")
    print(key['engagement']['id'])
"
10168,EM-DAT - Natural disasters,"import pandas as pd
import plotly.express as px
PATH_CSV = 'path_to_your_file.csv'
df = pd.read_csv(PATH_CSV)
# Types
types_df = df[['Year', 'Disaster Type']]
total_line = types_df[['Year']].value_counts().reset_index(name=""value"")
total_line['Disaster Type'] = ""All""
types_df = types_df.groupby(['Year', 'Disaster Type']).size().reset_index(name=""value"")
types_df = types_df.append(total_line).sort_values(by=[""Year""])

# Countries   
count_by_countries = df[['Year', 'ISO', 'Country']].groupby(['Year', 'ISO', 'Country']).size().reset_index(name='counts')
fig = px.choropleth(
    count_by_countries, locations=""ISO"",
    color=""counts"",
    hover_name=""Country"",
    animation_frame=""Year"",
    title = ""Number of natural disasters per country"",
    color_continuous_scale=px.colors.sequential.OrRd,
    range_color=[0, count_by_countries['counts'].max()]
)

fig.update_layout(
    width=850,
    height=600,
    autosize=False,
    template=""plotly_white"",
    title_x=0.5
)
fig.show()
common_kwargs = {'x': ""Year"", 'y': ""value"", 'title': ""Number of natural disasters per year""}

line_fig = px.line(types_df[types_df['Disaster Type'] == ""All""], **common_kwargs)
lineplt_all = px.line(types_df[types_df['Disaster Type'] == ""All""], **common_kwargs)
lineplt_filtered = {
    disaster_type: px.line(types_df[types_df['Disaster Type'] == disaster_type], **common_kwargs)
    for disaster_type in types_df['Disaster Type'].unique() if disaster_type != ""All""
}
# Add dropdown
line_fig.update_layout(
    updatemenus=[
        dict(
            buttons=list(
                [
                    dict(
                        label=""All disasters"",
                        method=""restyle"",
                        args=[{
                            ""y"": [data.y for data in lineplt_all.data]
                        }]
                    )
                ] + [
                    dict(
                        label=disaster_type,
                        method=""restyle"",
                        args=[
                            {
                                ""y"": [data.y for data in lineplt.data],
                            }
                        ]
                    )
                    for disaster_type, lineplt in lineplt_filtered.items()
                ]
            ),
        ),
    ],
    title_x=0.5,
    plot_bgcolor='rgba(0,0,0,0)',
)
line_fig.update_xaxes(gridcolor=""grey"")
line_fig.update_yaxes(gridcolor=""grey"")
line_fig.show()"
10169,Airtable - Get data,"from naas_drivers import airtable
API_KEY = 'API_KEY'
BASE_KEY = 'BASE_KEY'
TABLE_NAME = 'TABLE_NAME'
df = naas_drivers.airtable.connect(API_KEY,
                                   BASE_KEY, 
                                   TABLE_NAME).get(view='All opportunities',
                                                   maxRecords=20)
df"
10170,Airtable - Insert data,"from naas_drivers import airtable
API_KEY = 'API_KEY'
BASE_KEY = 'BASE_KEY'
TABLE_NAME = 'TABLE_NAME'
data = airtable.connect(API_KEY,
                        BASE_KEY,
                        TABLE_NAME).get(view='All opportunities',
                                        maxRecords=20).insert({'Name': 'Brian'})
data"
10171,Airtable - Search data,"from naas_drivers import airtable
API_KEY = 'API_KEY'
BASE_KEY = 'BASE_KEY'
TABLE_NAME = 'TABLE_NAME'
data = airtable.connect(API_KEY,
                        BASE_KEY,
                        TABLE_NAME).get(view='All opportunities',
                                        maxRecords=20).search('Name', 'Tom')
data"
10172,Airtable - Delete data,"from naas_drivers import airtable
API_KEY = 'API_KEY'
BASE_KEY = 'BASE_KEY'
TABLE_NAME = 'TABLE_NAME'
data = airtable.connect(API_KEY,
                        BASE_KEY,
                        TABLE_NAME).delete_by_field('Name', 'Tom')
data"
10173,ZIP - Extract files,"import zipfile
from io import BytesIO
import re
def extract_zip(filepath):
    i = 0
    with zipfile.ZipFile(filepath, ""r"") as zfile:
        for name in zfile.namelist():
            if re.search(r'\.zip$', name) is not None:
                zfiledata = BytesIO(zfile.read(name))
                with zipfile.ZipFile(zfiledata) as zfile2:
                    for name2 in zfile2.namelist():
                        zfile2.extract(name2, path=""../"", pwd=None)
                        i=i+1
    zfile.close()
    print(""Processing Completed. ""+str(i)+"" file(s) extracted"")
extract_zip('bilans_saisis_20181231.zip')"
10174,Google Drive - Download file,"try:
    import gdown
except:
    !pip install gdown
    import gdown
url = 'https://drive.google.com/uc?id=1-3UlYEPKgL4E197umEh6d58jWknodSm5'
output = 'naas_happy_hour.mp4'
gdown.download(url, output, quiet=False)"
10175,Pillow - Add data to image,"from PIL import Image, ImageDraw, ImageFont
import naas
# Input
input_image = ""layout.png""
input_font = ""ArchivoBlack-Regular.ttf""

# Model
text_position = (50, 900)
text = ""Your text""
font_size = 90

# Output
output_image = ""output_image.png""
def create_image(input_image,
                 output_image,
                 text_position,
                 text,
                 font_size=90,
                 input_font=None):
    img = Image.open(input_image)
    d = ImageDraw.Draw(img)
    
    font = ImageFont.truetype(input_font, font_size)
    fill = (255, 255, 255)
    
    d.text(text_position, text, font=font, fill=fill)
    img.save(output_image)
    print(""üíæ Image saved :"", output_image)
    return img

img = create_image(input_image, output_image, text_position, text, font_size, input_font)
display(img)
naas.asset.add(output_image)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(output_image)"
10176,Celestrak - Satellites over time,"import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
URL = 'http://www.celestrak.com/pub/satcat.csv'
df = pd.read_csv(URL)
df = df[['OPS_STATUS_CODE', 'LAUNCH_DATE', 'DECAY_DATE']]
df['DECAY_DATE'] = df['DECAY_DATE'].mask(df['DECAY_DATE'].isnull(), '9999')
df['LAUNCH_DATE'] = df['LAUNCH_DATE'].str[:4].astype(int)
df['DECAY_DATE'] = df['DECAY_DATE'].str[:4].astype(int)
years = df['LAUNCH_DATE'].unique()
inactive = list()
active = list()

for year in years:
    active.append(len(df[
        ((df['OPS_STATUS_CODE'].isin(['+', 'P', 'B', 'S', 'X'])) & (df['LAUNCH_DATE'] <= year))
        | ((df['DECAY_DATE'] > year) & (df['OPS_STATUS_CODE'] == 'D') & (df['LAUNCH_DATE'] <= year))
    ].index))
    
    inactive.append(len(df[
        ((df['OPS_STATUS_CODE'] == 'D') & (df['DECAY_DATE'] <= year))
        | ((df['OPS_STATUS_CODE'] == '-') & (df['LAUNCH_DATE'] <= year) )
    ].index))
    
fig = go.Figure(data=[
    go.Bar(name='Inactive satellites', x=years, y=inactive),
    go.Bar(name='Active satellites', x=years, y=active)
])
# Change the bar mode
fig.update_layout(
    title=""Number of satellites in space over time"",
    xaxis_title=""Years"",
    yaxis_title=""Number of satellites"",
    barmode='stack'
)
fig.show()
labels = years
widths = [100/len(years) for year in years]

active_percentage = list()
inactive_percentage = list()

for index, _ in np.ndenumerate(active):
    total = active[index[0]] + inactive[index[0]]
    active_percentage.append(active[index[0]]/total*100)
    inactive_percentage.append(inactive[index[0]]/total*100)

data = {
    ""Inactive"": inactive_percentage,
    ""Active"": active_percentage
}

fig = go.Figure()
for key in data:
    fig.add_trace(go.Bar(
        name=key,
        y=data[key],
        x=years,
        offset=0
    ))

fig.update_xaxes(range=[years[0],years[-1]])
fig.update_yaxes(range=[0,100])

fig.update_layout(
    title_text=""Percentage of inactive VS active satellites from 1957 to now"",
    barmode=""stack"",
    uniformtext=dict(mode=""hide"", minsize=10),
)"
10177,WSR - Get daily Covid19 active cases trend JHU,"import pandas as pd
from datetime import datetime
import plotly.graph_objects as go
import naas
# Input URLs of the raw csv dataset
urls = [
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'
]

# Outputs
uid = ""WSR_00002""
name_output = ""Covid19_Activecases_trend""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
title = ""COVID 19 - Active cases (in milions)""
def get_data_url(urls):
    df = pd.DataFrame()
    for url in urls:
        tmp_df = pd.read_csv(url)
        tmp_df[""Indicator""] = url.split(""/time_series_covid19_"")[-1].split(""_global.csv"")[0].capitalize()
        df = pd.concat([df, tmp_df])
    return df

df_init = get_data_url(urls)
df_init
def get_all_data(df_init):
    df = df_init.copy()
    # Cleaning
    df = df.drop(""Province/State"", axis=1)
    
    # Melt data
    df = pd.melt(df,
                 id_vars=[""Country/Region"", ""Lat"", ""Long"", ""Indicator""],
                 var_name=""Date"",
                 value_name=""Value"").fillna(0)
    df[""Date""] = pd.to_datetime(df[""Date""])
    
    # Calc active cases
    df_active = df.copy()
    df_active.loc[df_active[""Indicator""].isin([""Deaths"", ""Recovered""]), ""Value""] = df_active[""Value""] * (-1)
    df_active[""Indicator""] = ""Active cases""
    
    # Concat data
    df = pd.concat([df, df_active])
    
    # Group by country/region
    to_group = [""Country/Region"", ""Lat"", ""Long"", ""Indicator"", ""Date""]
    df = df.groupby(to_group, as_index=False).agg({""Value"": ""sum""})
    
    # Cleaning
    df = df.rename(columns={""Country/Region"": ""COUNTRY""})
    df.columns = df.columns.str.upper()
    return df.reset_index(drop=True)

df_clean = get_all_data(df_init)
df_clean
def get_trend(df_init):
    df = df_init.copy()
    # Filter
    df = df[(df[""INDICATOR""] == ""Active cases"")]
    
    # Groupby date
    df = df.groupby([""DATE""], as_index=False).agg({""VALUE"": 'sum'})
    
    # Calc variation
    for idx, row in df.iterrows():
        if idx == 0:
            value_n1 = 0
        else:
            value_n1 = df.loc[df.index[idx-1], ""VALUE""]
        df.loc[df.index[idx], ""VALUE_COMP""] = value_n1
    df[""VARV""] = df[""VALUE""] - df[""VALUE_COMP""]
    df[""VARP""] = df[""VARV""] / abs(df[""VALUE_COMP""])
    
    # Calc variation
    for idx, row in df.iterrows():
        if idx == 0:
            value_n1 = 0
        else:
            value_n1 = df.loc[df.index[idx-1], ""VARP""]
        df.loc[df.index[idx], ""VARP_COMP""] = value_n1
    df[""VARP_VAR""] = df[""VARP""] - df[""VARP_COMP""]
    
    # Prep data
    df[""VALUE_D""] = df[""VALUE""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[""VARV""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[""VARV""] > 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[""VARP""].map(""{:,.2%}"".format).str.replace("","", "" "")
    df.loc[df[""VARP""] > 0, ""VARP_D""] = ""+"" + df[""VARP_D""]
    
    df[""TEXT""] = (df['VALUE_D'] + "" active cases as of "" + df['DATE'].dt.strftime(""%Y-%m-%d"") + ""<br>""
                  "" ("" + df['VARP_D'] + "" vs yesterday)"")
    return df

df_trend = get_trend(df_clean)
df_trend.tail(30)
def create_linechart(df, label, value, text):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            text=df[text],
            hoverinfo=""text"",
            mode=""lines"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=title,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title='No. of active cases',
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""DATE"", ""VALUE"", ""TEXT"")
# Save your dataframe in CSV
df.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)"
10178,WSR - WHI Create indicator,"import pandas as pd
from PIL import Image, ImageDraw, ImageFont
from datetime import date
# Input extracted from your open source data
data = [
    {'DATE_PROCESSED': '2021-05-28', 'INDICATOR': 'COVID-19 Active Cases', 'VALUE': 0.21, 'WEIGHT': 4},
    {'DATE_PROCESSED': '2021-05-28', 'INDICATOR': 'Sea Level', 'VALUE': 4.951165245651996, 'WEIGHT': 2},
    {'DATE_PROCESSED': '2021-06-10', 'INDICATOR': 'Delta global temperature', 'VALUE': 4.9, 'WEIGHT': 4},
    {'DATE_PROCESSED': '2021-06-10', 'INDICATOR': 'Arctic Sea Ice level (million square km)', 'VALUE': 4.9, 'WEIGHT': 2}
]

# Input image
input_image = ""layout.png""

# Input font
input_font = ""ArchivoBlack-Regular.ttf""

# Output image
output_image = ""WHI.png""
df = pd.DataFrame(data)
df
def whi(df):
    return round((df['VALUE']*df['WEIGHT']).sum() / df['WEIGHT'].sum(), 2)

whi(df)
def create_image(value, datetime):
    img = Image.open(input_image)
    d = ImageDraw.Draw(img)
    
    font = ImageFont.truetype(input_font, 90)
    fill = (255,255,255)
    
    d.text((50,900), ""{indicator}/10, {date}"".format(date=datetime.strftime(""%d/%m/%Y""), indicator=value), font=font, fill=fill)
    return img
img = create_image(f'{whi(df)}' , date.today())
display(img)
img.save(output_image)

naas.asset.add(output_image)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(output_image)"
10179,WSR - Get daily Covid19 active cases worldmap JHU,"import pandas as pd
from datetime import datetime
try:
    from dataprep.clean import clean_country
except:
    !pip install dataprep --user
    from dataprep.clean import clean_country
import plotly.graph_objects as go
import naas
title = ""COVID 19 - Active cases (in milions)""
# Input URLs of the raw csv dataset
urls = [
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv',
    'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'
]

# Output paths
output_image = f""{title}.png""
output_html = f""{title}.html""
# Schedule your job everyday at 8:00 AM (NB: you can choose the time of your scheduling bot)
naas.scheduler.add(cron=""0 8 * * *"")

#-> Uncomment the line below (by removing the hashtag) to remove your scheduler
# naas.scheduler.delete()
def get_data_url(urls):
    df = pd.DataFrame()
    for url in urls:
        tmp_df = pd.read_csv(url)
        tmp_df[""Indicator""] = url.split(""/time_series_covid19_"")[-1].split(""_global.csv"")[0].capitalize()
        df = pd.concat([df, tmp_df])
    return df

df_init = get_data_url(urls)
df_init
def get_all_data(df_init):
    df = df_init.copy()
    # Cleaning
    df = df.drop(""Province/State"", axis=1)
    
    # Melt data
    df = pd.melt(df,
                 id_vars=[""Country/Region"", ""Lat"", ""Long"", ""Indicator""],
                 var_name=""Date"",
                 value_name=""Value"").fillna(0)
    df[""Date""] = pd.to_datetime(df[""Date""])
    
    # Calc active cases
    df_active = df.copy()
    df_active.loc[df_active[""Indicator""].isin([""Deaths"", ""Recovered""]), ""Value""] = df_active[""Value""] * (-1)
    df_active[""Indicator""] = ""Active cases""
    
    # Concat data
    df = pd.concat([df, df_active])
    
    # Group by country/region
    to_group = [""Country/Region"", ""Lat"", ""Long"", ""Indicator"", ""Date""]
    df = df.groupby(to_group, as_index=False).agg({""Value"": ""sum""})
    
    # Cleaning
    df = df.rename(columns={""Country/Region"": ""COUNTRY""})
    df.columns = df.columns.str.upper()
    return df.reset_index(drop=True)

df_clean = get_all_data(df_init)
df_clean
def prep_data(df_init):
    df = df_init.copy()
    # Filter
    date_max = df[""DATE""].max()
    df = df[
        (df[""INDICATOR""] == ""Active cases"") & 
        (df[""DATE""] == date_max)
    ].reset_index(drop=True)


    # Clean country
    df = clean_country(df, 'COUNTRY', output_format='alpha-3').dropna()
    df = df.rename(columns={'COUNTRY_clean': 'COUNTRY_ISO'})
    return df.reset_index(drop=True)

df_worldmap = prep_data(df_clean)
df_worldmap
def create_worldmap(df):
    fig = go.Figure()

    fig = go.Figure(data=go.Choropleth(
        locations=df['COUNTRY_ISO'],
        z=df['VALUE'],
        text=df[""COUNTRY""] + "": "" + df['VALUE'].map(""{:,.0f}"".format).str.replace("","", "" "") + "" active cases"",
        hoverinfo=""text"",
        colorscale='Blues',
        autocolorscale=False,
        reversescale=False,
        marker_line_color='darkgray',
        marker_line_width=0.5,
        colorbar_tickprefix='',
        colorbar_title='Active cases',
    ))

    fig.update_layout(
        title=title,
        plot_bgcolor=""#ffffff"",
        legend_x=1,
        geo=dict(
            showframe=False,
            showcoastlines=False,
            #projection_type='equirectangular'
        ),
        dragmode= False,
        width=1200,
        height=800,

    )
    config = {'displayModeBar': False}
    fig.show(config=config)
    return fig

fig = create_worldmap(df_worldmap)
fig.write_image(output_image, width=1200)
fig.write_html(output_html)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {""inline"":True})

#-> Uncomment the line below to remove your assets
# naas.asset.delete(output_image)
# naas.asset.delete(output_html)"
10180,GitHub - Track notebooks created over time,"import pandas as pd
import requests
import os
from naas_drivers import github
import plotly.graph_objects as go
import pydash as _pd
from urllib.parse import urlencode
from datetime import datetime, timedelta
import naas
# Inputs
REPO_URL = ""https://github.com/jupyter-naas/awesome-notebooks""
GITHUB_TOKEN = ""ghp_COJiJEU4cQR4rjsxxxxx""

# Outputs
chart_title = ""Notebooks created since naas launch""
name_output = f""awesome-notebooks""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df_commits = github.connect(GITHUB_TOKEN).repos.get_commits(REPO_URL)
print(""Total commits:"", len(df_commits))
df_commits.head(1)
def get_commits_merge(df):
    df_pr = df[(df.MESSAGE.str[:5] == ""Merge"") & 
               (df.VERIFICATION_STATUS == True)].reset_index(drop=True)
    print(""Total Merged PR:"", len(df_pr))

    df_pr[""DATE""] = df_pr[""AUTHOR_DATE""].dt.strftime(""%Y-%m-%d"")
    df_pr = df_pr[[""DATE"", ""COMMITTER_NAME"", ""ID""]].drop_duplicates(""DATE"").reset_index(drop=True)
    print(""Total Merged PR (unique date):"", len(df_pr))
    return df_pr

df_pr = get_commits_merge(df_commits)
df_pr.head(1)
def get_notebooks(commit_id):
    notebooks = []
    headers = {'Authorization': f'token {GITHUB_TOKEN}'}
    url = f""https://api.github.com/repos/jupyter-naas/awesome-notebooks/git/trees/{commit_id}?recursive=1""
    res = requests.get(url, headers=headers)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        raise(e)
    res_json = res.json()

    trees = res_json.get(""tree"")
    for file in trees:
        if "".github"" not in file.get(""path"") and "".gitignore"" not in file.get(""path"") and ""/"" in file.get(""path""):
            if file.get(""path"").endswith("".ipynb""):
                temp = file.get(""path"").split(""/"")
                if temp == -1:
                    data = {
                        ""root"": None,
                        ""subdir"": file.get(""path"")
                    }
                    notebooks.append(data)
                else:
                    last_folder = """"
                    file_name = temp[-1]
                    temp.pop()
                    for folder in temp:
                        last_folder += ""/"" + folder
                    root = last_folder[1:]
                    data = {
                        ""root"": root,
                        ""subdir"": file_name
                    }
                    notebooks.append(data)
    return pd.DataFrame(notebooks)

def tracks_notebooks(df):
    df_tracks = pd.DataFrame()

    for _, row in df.iterrows():
        date = row.DATE
        commit_id = row.ID
        try:
            df_track = get_notebooks(commit_id)
            df_track[""DATE""] = date
            df_track[""ID""] = commit_id
            # Concat
            df_tracks = pd.concat([df_tracks, df_track], axis=0)
        except Exception as e:
            print(f""Error on {date} - {commit_id}"", e)
    print(""Total notebooks tracked:"", len(df_tracks))    
    return df_tracks

df_tracks = tracks_notebooks(df_pr)
df_tracks.head(1)
def get_trend(df,
              date_col_name='DATE',
              date_val_name='ID',
              date_order='asc'):
    # Cleaning
    df = df.rename(columns={date_val_name: ""VALUE""})
    
    df = df.groupby(date_col_name, as_index=False).agg({""VALUE"": ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    df = df.reset_index(drop=True)
    for _, row in df.iterrows():
        if _ > 0:
            n_1 = df.loc[df.index[_-1], ""VALUE""]
            n = df.loc[df.index[_], ""VALUE""]
            if n == 0:
                df.loc[_, ""VALUE""] = n_1
    df[""GROUP""] = ""Actual""
    
    # Create line to target
    value_target = 1000
    date_target = ""2022-12-31""
    df_target = df[-1:].reset_index(drop=True)
    d = datetime.strptime(date_target, ""%Y-%m-%d"")
    d2 = df_target.loc[df_target.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    df_target.set_index(date_col_name, drop=True, inplace=True)
    df_target.index = pd.DatetimeIndex(df_target.index)
    df_target = df_target.reindex(idx, fill_value=1000)
    df_target[date_col_name] = pd.DatetimeIndex(df_target.index)
    df_target = df_target.reset_index(drop=True)
    
    # Get target value
    to_go = value_target - df_target.loc[df_target.index[0], ""VALUE""]
    var = to_go / (len(idx) - 1)
    for _, row in df_target.iterrows():
        if _ > 0:
            n_1 = df_target.loc[df_target.index[_-1], ""VALUE""]
            df_target.loc[_, ""VALUE""] = n_1 + var
    df_target[""GROUP""] = ""Forecast""

    # Concat data
    df = pd.concat([df, df_target]).reset_index(drop=True)
    df = df[[date_col_name, ""GROUP"", ""VALUE""]]
    
    # Calc variation
    for idx, row in df.iterrows():
        if idx == 0:
            value_n1 = 0
        else:
            value_n1 = df.loc[df.index[idx-1], ""VALUE""]
        df.loc[df.index[idx], ""VALUE_COMP""] = value_n1
    df[""VARV""] = df[""VALUE""] - df[""VALUE_COMP""]
    df[""VARP""] = df[""VARV""] / abs(df[""VALUE_COMP""])
    
    # Prep data
    df[""VALUE_D""] = df[""VALUE""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df[""VARV_D""] = df[""VARV""].map(""{:,.0f}"".format).str.replace("","", "" "")
    df.loc[df[""VARV""] >= 0, ""VARV_D""] = ""+"" + df[""VARV_D""]
    df[""VARP_D""] = df[""VARP""].map(""{:,.2%}"".format).str.replace("","", "" "")
    df.loc[df[""VARP""] >= 0, ""VARP_D""] = ""+"" + df[""VARP_D""]
    
    df.loc[df[""GROUP""] == ""Forecast"", ""TEXT""] = """"
    df.loc[df[""GROUP""] == ""Actual"", ""TEXT""] = (df['VALUE_D'] + "" notebooks as of "" + df['DATE'].dt.strftime(""%Y-%m-%d"") +
                  "" ("" + df['VARV_D'] + "" vs yesterday)"")
    return df

df_trend = get_trend(df_tracks)
df_trend
def create_linechart(df, label, group, value, text, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    df1 = df[df[group] == ""Actual""]
    last_text = df1.loc[df1.index[-1], text]
    fig.add_trace(
        go.Scatter(
            name=""Actual"",
            x=df1[label],
            y=df1[value],
            mode=""lines"",
            hovertext=df1[text],
            hoverinfo=""text"",
            line=dict(color=""royalblue""),
        )
    )
    df2 = df[df[group] == ""Forecast""]
    fig.add_trace(
        go.Scatter(
            name=""Target"",
            x=df2[label],
            y=df2[value],
            mode=""lines"",
            hovertext=df2[text],
            hoverinfo=""text"",
            line=dict(color=""red"", dash=""dot""),
        )
    )
    fig.update_layout(
#         title=f""<b>Tracks of notebooks created since naas launch </b><br><span style='font-size: 13px;'>Total notebooks as of today: {notebooks} (+{var} vs yesterday)</span>"",
        title=f""<b>{title}</b><br><span style='font-size: 13px;'>{last_text}</span>"",
        title_font=dict(family=""Verdana"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title='Date',
        xaxis_title_font=dict(family=""Verdana"", size=11, color=""black""),
        yaxis_title='No. of notebooks',
        yaxis_title_font=dict(family=""Verdana"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""DATE"", ""GROUP"", ""VALUE"", ""TEXT"", chart_title)
# Save your dataframe in CSV
df_trend.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10181,GitHub - Track issues on projects,"import requests
import pandas as pd
from urllib.parse import urlencode
from datetime import datetime
import plotly.express as px
from naas_drivers import github
# Project URL from web
PROJECT_URL = ""https://github.com/orgs/jupyter-naas/projects""

# GitHub Token
GITHUB_TOKEN = ""ghp_COJiJEU4cQR4rjsxxxxxx""
# Graph : bar order
STATUS_ORDER = [""Backlog"", ""In Progress"", ""Review"", ""Done"", ""Publication""]

# Outputs
csv_output = ""GitHub_Issues.csv""
df_issues = github.connect(GITHUB_TOKEN).projects.get_issues(PROJECT_URL)
df_issues.tail(15)
for index, s in enumerate(STATUS_ORDER):
    print(index, s)
def create_barchart(df, title, labels):
    # Get status
    status = df.issue_status.unique().tolist()
    status_order = []
    for index, s in enumerate(STATUS_ORDER):
        if s in status:
            status_order += [s] 
    
    # Create fig
    fig = px.bar(df,
                 title=title,
                 x=status_order,
                 y=""count"",
                 text=""count"",
                 labels=labels)
    fig.update_traces(marker_color='black')
    fig.update_layout(
        plot_bgcolor=""#ffffff"",
        width=1000,
        height=800,
        font=dict(family=""Arial"", size=14, color=""black""),
        paper_bgcolor=""white"",
        yaxis_title=""No of issues"",
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        xaxis_title=""Status"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig
df_issues.to_csv(csv_output, index=False)
# Dataframe
issues = df_issues.groupby('issue_status').agg({'issue_number':'count'}).reset_index().rename(columns={""issue_number"":""count""})

# Chart title
title =  f""Github Project - {PROJECT_URL.split('/')[-2]} : {df_issues['project_name'].unique()[0]} <br><span style='font-size: 13px;'>Total issues: {issues['count'].sum()}</span>""

# Hover labels
labels = {
    'issue_status':'Issue status',
    'count':""Number of Issues""
}
fig = create_barchart(issues, title, labels)
# Dataframe
open_issues = df_issues[df_issues['issue_state']=='open'].groupby('issue_status').agg({""issue_number"":'count'}).reset_index().rename(columns={'issue_number':'count'})

# Chart title
title =  f""Github Project - {PROJECT_URL.split('/')[-2]} : {df_issues['project_name'].unique()[0]} <br><span style='font-size: 13px;'>Total open issues: {open_issues['count'].sum()}</span>""

# Hover labels
labels = {
               'issue_status':'Issue status',
               'count':""Number of Open issues""
          }
fig = create_barchart(open_issues, title, labels)
# Dataframe
closed_issues = df_issues[df_issues['issue_state']=='closed'].groupby('issue_status').agg({""issue_number"":'count'}).reset_index().rename(columns={'issue_number':'count'})

# Chart title
title =  f""Github Project - {PROJECT_URL.split('/')[-2]} : {df_issues['project_name'].unique()[0]} <br><span style='font-size: 13px;'>Total closed issues: {closed_issues['count'].sum()}</span>""

# Hover labels
labels = {
               'issue_status':'Issue status',
               'count':""Number of Closed issues""
          }

fig = create_barchart(closed_issues, title, labels)
stale_issues = []
for item in df_issues.stale_issue:
    if item!='None':
        stale_issues.append(int(item.split()[-2])>=7)
    else:
         stale_issues.append(False)
            
df_issues['stale_bool'] = stale_issues
temp = df_issues[df_issues['stale_bool']==True]
temp[temp['issue_state']=='open']

# Dataframe
open_stale_issues = temp[temp['issue_state']=='open'].groupby('issue_status').agg({""stale_bool"":'count'}).reset_index().rename(columns={'stale_bool':'count'})

# Chart title
title =  f""Github Project - {PROJECT_URL.split('/')[-2]} : {df_issues['project_name'].unique()[0]} <br><span style='font-size: 13px;'>Total open stale issues: {open_stale_issues['count'].sum()}</span>""

# Hover labels
labels = {
               'issue_status':'Issue status',
               'count':""Number of Open issues with no activity since more than 7 days""
          }
fig = create_barchart(open_stale_issues, title, labels)
"
10182,GitHub - Get profile from user,"import pandas as pd
import requests
import os
from urllib.parse import urlencode
from naas_drivers import github
USER_URL = ""https://github.com/FlorentLvr""
GITHUB_TOKEN = ""ghp_Stz3qlkR3b00nKUW8rxJoxxxxxxxxxxxx""
df_user = github.connect(GITHUB_TOKEN).users.get_profile(USER_URL)
df_user"
10183,GitHub - Get issues from repo,"import requests
import pandas as pd
from urllib.parse import urlencode
from datetime import datetime
import plotly.express as px
from naas_drivers import github
# Github repository url
REPO_URL = ""https://github.com/jupyter-naas/awesome-notebooks""

# Github token
GITHUB_TOKEN = ""ghp_COJiJEU4cQR4rjslUxxxxxxxxxxxxxxxx""
df_issues = github.connect(GITHUB_TOKEN).repos.get_issues(REPO_URL)
print(""Nb issues:"", len(df_issues))
df_issues.head(15)"
10184,GitHub - Get weekly commits from repository,"import pandas as pd
import plotly.express as px
from naas_drivers import github
import naas
# GitHub token
GITHUB_TOKEN = ""ENTER_YOUR_GITHUB_TOKEN_HERE"" # EXAMPLE : ""ghp_fUYP0Z5i29AG4ggX8owctGnHU**********"" 

# Github repo on which we want to create issues.
REPO_URL = ""ENTER_YOUR_REPO_URL_HERE"" # EXAMPLE : ""https://github.com/jupyter-naas/awesome-notebooks/""
df_commits = github.connect(GITHUB_TOKEN).repos.get_commits(REPO_URL)
df_commits
def get_weekly_commits(df):
    # Exclude Github commits
    df = df[(df.COMMITTER_EMAIL.str[-10:] != ""github.com"")]
    
    # Groupby and count
    df = df.groupby(pd.Grouper(freq='W', key='AUTHOR_DATE')).agg({""ID"": ""count""}).reset_index()
    df[""WEEKS""] = df[""AUTHOR_DATE""].dt.strftime(""W%U-%Y"")
    
    # Cleaning
    df = df.rename(columns={""ID"": ""NB_COMMITS""})
    return df

df_weekly = get_weekly_commits(df_commits)
df_weekly
def create_barchart(df, repository):
    # Get repository
    repository = repository.split(""/"")[-1]
    
    # Calc commits
    commits = df.NB_COMMITS.sum()
    
    # Create fig
    fig = px.bar(df,
           title=f""GitHub - {repository} : Weekly user commits <br><span style='font-size: 13px;'>Total commits: {commits}</span>"",
           x=""WEEKS"",
           y=""NB_COMMITS"",
           labels={
               'WEEKS':'Weeks committed',
               'NB_COMMITS':""Nb. commits""
          })
    fig.update_traces(marker_color='black')
    fig.update_layout(
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        font=dict(family=""Arial"", size=14, color=""black""),
        paper_bgcolor=""white"",
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_barchart(df_weekly, REPO_URL)
output_path = f""{REPO_URL.split('/')[-1]}_weekly_commits.html""
fig.write_html(output_path)
naas.asset.add(output_path, params={""inline"": True})
"
10185,GitHub - Get stargazers from repository,"import pandas as pd
import requests
import os
from urllib.parse import urlencode
from datetime import datetime
import plotly.graph_objects as go
from naas_drivers import github
import naas
# GitHub token
GITHUB_TOKEN = ""ENTER_YOUR_GITHUB_TOKEN_HERE"" # EXAMPLE : ""ghp_fUYP0Z5i29AG4ggX8owctGnHU**********"" 

# Github repo on which we want to create issues.
REPO_URL = ""ENTER_YOUR_REPO_URL_HERE"" # EXAMPLE : ""https://github.com/jupyter-naas/awesome-notebooks/""
# Outputs
name_output = f""awesome-notebooks""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df_stargazers = github.connect(GITHUB_TOKEN).repos.get_stargazers(REPO_URL)
df_stargazers
def get_trend(df,
              date_col_name='STARRED_AT',
              value_col_name=""ID"",
              date_order='asc'):
    
    # Format date
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    
    # Calc sum cum
    df[""value_cum""] = df.agg({value_col_name: ""cumsum""})
    return df.reset_index(drop=True)

df_trend = get_trend(df_stargazers)
df_trend.tail(1)
def create_linechart(df, date, value, repo_url):
    # Get repo name
    repo_name = repo_url.split(""https://github.com/"")[-1].split(""/"")[-1]
    
    # Get last value
    last_value = df.loc[df.index[-1], value]
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date].to_list(),
            y=df[value].to_list(),
            mode=""lines+text"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""‚≠ê<b> Stars - {repo_name}</b><br><span style='font-size: 13px;'>Total stars as of today: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title='No. of stars',
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""STARRED_AT"", ""value_cum"", REPO_URL)
# Save your dataframe in CSV
df_trend.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10186,GitHub - Get commits ranking from repository,"import pandas as pd
import plotly.express as px
from naas_drivers import github
import naas
# Github repository url
REPO_URL = ""https://github.com/jupyter-naas/awesome-notebooks""

# Github token
GITHUB_TOKEN = ""ghp_CEvqR7QauDbNLRiIiwAC1v4xxxxxxxxxxxxx""
df_commits = github.connect(GITHUB_TOKEN).repos.get_commits(REPO_URL)
df_commits
def get_commits(df):
    # Exclude Github commits
    df = df[(df.COMMITTER_EMAIL.str[-10:] != ""github.com"")]
    
    # Groupby and count
    df = df.groupby([""AUTHOR_NAME""], as_index=False).agg({""ID"": ""count""})
    
    # Cleaning
    df = df.rename(columns={""ID"": ""NB_COMMITS""})
    return df.sort_values(by=""NB_COMMITS"", ascending=False).reset_index(drop=True)

df = get_commits(df_commits)
df
def create_barchart(df, repository):
    # Get repository
    repository = repository.split(""/"")[-1]
    
    # Sort df
    df = df.sort_values(by=""NB_COMMITS"")
    
    # Calc commits
    commits = df.NB_COMMITS.sum()
    
    # Create fig
    fig = px.bar(df,
                 y=""AUTHOR_NAME"",
                 x=""NB_COMMITS"",
                 orientation='h',
                 title=f""GitHub - {repository} : Commits by user <br><span style='font-size: 13px;'>Total commits: {commits}</span>"",
                 text=""NB_COMMITS"",
                 labels={""AUTHOR_NAME"": ""Author"",
                         ""NB_COMMITS"": ""Nb commits""}
                 )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        font=dict(family=""Arial"", size=14, color=""black""),
        paper_bgcolor=""white"",
        xaxis_title=None,
        xaxis_showticklabels=False,
        yaxis_title=None,
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_barchart(df, REPO_URL)
output_path = f""{REPO_URL.split('/')[-1]}_commits_ranking.html""
fig.write_html(output_path)
naas.asset.add(output_path, params={""inline"": True})"
10187,GitHub - Add new member to team,"import requests
from naas_drivers import github
import pandas as pd
import naas
# GitHub token
GITHUB_TOKEN = ""ENTER_YOUR_GITHUB_TOKEN_HERE"" # EXAMPLE : ""ghp_fUYP0Z5i29AG4ggX8owctGnHU**********"" 

# GitHub teams url
GITHUB_TEAMS_URL = ""ENTER_YOUR_GITHUB_TEAMS_URL_HERE"" # EXAMPLE : ""https://github.com/orgs/jupyter-naas/teams/opensource-contributors""

# New members to add : str or list of members
GITHUB_NEW_MEMBERS = ""ENTER_YOUR_NEW_USERS_HERE"" # EXAMPLE : ""FlorentLvr"" or [""FlorentLvr"", ""Dr0p42""]
def add_members(team_url, new_members):
    # Init output
    df = pd.DataFrame()
    
    # Get org id and team id
    team_id = team_url.split(""/teams/"")[-1].split(""/"")[0]
    team_org = team_url.split(""https://github.com/orgs/"")[-1].split(""/"")[0]
    
    # If a particular member already is present in the team,
    # then it does not create a copy of that member. No need to worry :)
    if isinstance(new_members, str):
        new_members = [new_members]
    
    # Add new members to team
    data = []
    for member in new_members:
        member = member.split(""https://github.com/"")[-1].split(""/"")[0]
        req_url = f""https://api.github.com/orgs/{team_org}/teams/{team_id}/memberships/{member}""
        headers = {'Authorization': f'token {GITHUB_TOKEN}',
                   ""Accept"": ""application/vnd.github.v3+json""}
        res = requests.put(req_url, headers=headers)
        res.raise_for_status()
        
        if res.status_code == 200:
            print(f'‚úÖ Member {member} successfully invited to your team {team_id}')
            res_json = res.json()
            data.append(res_json)
            
    # Send result as dataframe
    df = pd.DataFrame(data)
    return df.reset_index(drop=True)

df_new = add_members(GITHUB_TEAMS_URL, GITHUB_NEW_MEMBERS)
df_new"
10188,GitHub - Add new issues as page in Notion database,"import naas
from naas_drivers import notion, github
# GitHub token
GITHUB_TOKEN = ""ENTER_YOUR_GITHUB_TOKEN_HERE"" # EXAMPLE : ""ghp_fUYP0Z5i29AG4ggX8owctGnHU**********"" 

# Github repo on which we want to create issues.
REPO_URL = ""ENTER_YOUR_REPO_URL_HERE"" # EXAMPLE : ""https://github.com/jupyter-naas/awesome-notebooks/""
# Notion token
NOTION_TOKEN = ""ENTER_YOUR_NOTION_TOKEN_HERE"" # EXAMPLE : ""secret_xxskqjlodshfiqs""

# Notion database URL
DATABASE_URL = ""https://www.notion.so/naas-official/f42d6592949axxxxxxxxxxxxx"" # EXAMPLE : ""https://www.notion.so/naas-official/f42d6592949axxxxxxxxxxxxx"" 
# Schedule your notebook every 15 minutes.
naas.scheduler.add(cron=""*/15 * * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
notion_database_id = DATABASE_URL.split(""/"")[-1].split(""?v="")[0]
db_notion = notion.connect(NOTION_TOKEN).database.get(notion_database_id)
df_notion = db_notion.df()

for idx, row in df_notion.iterrows():
    if row.Name=='':
        df_notion.drop(index= idx, inplace=True)
    try:
        df_notion.drop(columns=['Tags'], inplace=True)
    except KeyError:
        pass

df_notion
df_issues = github.connect(GITHUB_TOKEN).repos.get_issues(REPO_URL)
df_issues.head()
def notion_page_from_gh_issue(issue, notion_database_id):
    if len(df_notion) == 0:
        pass
    elif len(df_notion) != 0 and str(issue.issue_id) in df_notion.Issue_id.to_list():
        return ""issue already exists in database""
    
    page = notion.connect(NOTION_TOKEN).page.create(database_id=notion_database_id,
                                                    title=issue.issue_title)
    page.rich_text(""URL"",issue.link_to_the_issue)
    page.rich_text(""Assignees"",issue.issue_assignees)
    page.number('Issue_id',issue.issue_id)
    page.date('Last_created',str(issue.last_created_date)+'T'+str(issue.last_created_time))
    page.update()
    return ""Done""
for idx, issue in df_issues.iterrows():
    val = notion_page_from_gh_issue(issue, notion_database_id)
    if val == ""issue already exists in database"":
        print(""Database up to date!"")
        break
    print(f'‚úÖ Notion page created for issue {issue.link_to_the_issue}')"
10189,GitHub - Close issue,"from github import Github
repo_name = ""**********""                # Repository path
git_key = ""**********""                  # Settings/Developer settings
g = Github(git_key)   
import pandas as pd
repo = g.get_repo(repo_name)
open_issues = repo.get_issues(state='open')
title = []
number = []
for issue in open_issues:
    title.append(issue.title)
    number.append(issue.number)
data = {'Issue':title, 'Number':number} 
df = pd.DataFrame(data)
df
# num = 447
# for issue in open_issues:
#     if issue.number == num:
#         issue.edit(state='closed')
repo = g.get_repo(repo_name)
open_issues = repo.get_issues(state='open')
for issue in open_issues:
    issue.edit(state='closed')"
10190,GitHub - Download file from url,"import requests
import naas
import uuid
target = ""https://github.com/jupyter-naas/awesome-notebooks/blob/master/Plotly/Create%20Candlestick%20chart.ipynb""
# https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/Dataviz/Plotly/Create%20Candlestick%20chart.ipynb
raw_target = target.replace(""https://github.com/"", ""https://raw.githubusercontent.com/"")
raw_target = raw_target.replace(""/blob/"", ""/"")
print(raw_target)
import urllib.parse
r = requests.get(raw_target)
uid = uuid.uuid4().hex

file_name = raw_target.split('/')[-1]
file_name = urllib.parse.unquote(file_name)

with open(file_name, 'wb') as f:
    f.write(r.content)"
10191,GitHub - Get pull requests from repository,"import requests
import pandas as pd
from urllib.parse import urlencode
from datetime import datetime
import plotly.express as px
from naas_drivers import github
# GitHub token
GITHUB_TOKEN = ""ENTER_YOUR_GITHUB_TOKEN_HERE"" # EXAMPLE : ""ghp_fUYP0Z5i29AG4ggX8owctGnHU**********"" 

# Github repo on which we want to create issues.
REPO_URL = ""ENTER_YOUR_REPO_URL_HERE"" # EXAMPLE : ""https://github.com/jupyter-naas/awesome-notebooks/""
df_pulls = github.connect(GITHUB_TOKEN).repos.get_pulls(REPO_URL)
print(""Opened PR:"", len(df_pulls))
df_pulls"
10192,GitHub - Peform basic actions,"from git_lib import Git
config = {
            'username':'< Your github username >',
            'password':'< Your github password >',
            'github_url':'< Github url >',
            'branch':'< Github branch name >',
            'target_folder':'< Folder name >',
            'action':'< Github action >',
            'commit_message':'< Your message for commit >'
         }
git_instance = Git(config)"
10193,GitHub - Read issue,"from github import Github
repo_name = ""**********""                # Repository path
git_key = ""**********""                  # Settings/Developer settings
g = Github(git_key)   
import pandas as pd
repo = g.get_repo(repo_name)
open_issues = repo.get_issues(state='open')
title = []
number = []
for issue in open_issues:
    title.append(issue.title)
    number.append(issue.number)
data = {'Title':title,'Description':title, 'ID':number} 
df = pd.DataFrame(data)
df
df"
10194,GitHub - Create issue,"from github import Github
repo_name = ""**********""                                           # Repository path
git_key = ""**********""                                             # Settings/Developer settings
assignee = ""**********""                                            # Asignee name (optional) or put """"
issue_title = ""This is a another issue""                            # Issue title
issue_description = ""This is another issue body created using api"" # Issue description
g = Github(git_key)   
repo = g.get_repo(repo_name)
repo.create_issue(title = issue_title, body = issue_description, assignee = assignee)"
10195,GitHub - Get profiles from teams,"import pandas as pd
import requests
import naas
from urllib.parse import urlencode
from naas_drivers import github
TEAM_URL = ""https://github.com/orgs/jupyter-naas/teams""
GITHUB_TOKEN = ""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""
df_teams = github.connect(GITHUB_TOKEN).teams.get_profiles(TEAM_URL)
print(f'Dataset size -> {df_teams.shape}')
df_teams.head(25)"
10196,GitHub - Get active projects,"import requests
import pandas as pd
from urllib.parse import urlencode
from datetime import datetime
from naas_drivers import github
# Github project url
PROJECT_URL = ""https://github.com/orgs/jupyter-naas/projects""

# Github token
GITHUB_TOKEN = ""ghp_COJiJEU4cQR4rjslUjM9Duxxxxxx""
df_projects = github.connect(GITHUB_TOKEN).projects.get(PROJECT_URL)
df_projects.head()"
10197,Google Search - Get LinkedIn profile url from name,"try:
    from googlesearch import search
except:
    !pip install google
    from googlesearch import search
import re
firstname = ""Florent""
lastname = ""Ravenel""
def get_linkedin_url(firstname, lastname):
    # Init linkedinbio
    linkedinbio = None
    
    # Create query
    query = f""{firstname}+{lastname}+Linkedin""
    print(""Google query: "", query)
    
    # Search in Google
    for i in search(query, tld=""com"", num=10, stop=10, pause=2):
        pattern = ""https:\/\/.+.linkedin.com\/in\/.([^?])+""
        result = re.search(pattern, i)

        # Return value if result is not None
        if result != None:
            linkedinbio = result.group(0).replace("" "", """")
            return linkedinbio
    return linkedinbio

linkedin_url = get_linkedin_url(firstname, lastname)
linkedin_url"
10198,Google Search - Perform search,"try:
    from googlesearch import search
except:
    !pip install google
    from googlesearch import search
query = ""telsa""
for i in search(query, tld=""co.in"", num=30, stop=10, pause=2):
    print(i)"
10199,Google Search - Get LinkedIn company url from name,"try:
    from googlesearch import search
except:
    !pip install google
    from googlesearch import search
import re
company = ""Tesla""
def get_linkedin_url(company):
    # Init linkedinbio
    linkedinbio = None
    
    # Create query
    query = f""{company}+Linkedin""
    print(""Google query: "", query)
    
    # Search in Google
    for i in search(query, tld=""com"", num=10, stop=10, pause=2):
        pattern = ""https:\/\/.+.linkedin.com\/company\/.([^?])+""
        result = re.search(pattern, i)

        # Return value if result is not None
        if result != None:
            linkedinbio = result.group(0).replace("" "", """")
            return linkedinbio
    return linkedinbio

linkedin_url = get_linkedin_url(company)
linkedin_url"
10200,CSV - Read file,"import pandas
csv_path = ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv""
df = pandas.read_csv(csv_path)
df.head(5) #read the first 5 lines"
10201,CSV - Save file,"import pandas
# Input
csv_path = ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv""

# Output
csv_output_path = ""data.csv""
df = pandas.read_csv(csv_path)
df
df.to_csv(csv_output_path)
print(f'üíæ Excel '{csv_output_path}' successfully saved in Naas.')"
10202,MySQL - Query database,"import os
import pymysql
import pandas as pd
host = os.getenv('MYSQL_HOST')
port = os.getenv('MYSQL_PORT')
user = os.getenv('MYSQL_USER')
password = os.getenv('MYSQL_PASSWORD')
database = os.getenv('MYSQL_DATABASE')
conn = pymysql.connect(
    host=host,
    port=int(port),
    user=user,
    passwd=password,
    db=database,
    charset='utf8mb4')
df = pd.read_sql_query(
    ""SELECT DATE(created_at) AS date, COUNT(*) AS count FROM user GROUP BY date HAVING date >= '2017-04-01' "",
    conn)
df.tail(10)
%matplotlib inline

df.index = df['date']
p = df.tail(10).plot.bar()
conn.close()"
10203,Telegram - Create crypto sentiment bot,"!pip install python-telegram-bot --user
import logging
from telegram.ext import *
import numpy as np
from naas_drivers import newsapi
from naas_drivers import sentiment
from datetime import datetime, timedelta
TELEGRAM_API_KEY = ""***********""
NEWSAPI_API_KEY = ""***********""
# Set up the logging
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logging.info('Starting Bot...')
#parameters for newsapi
FROM_DATE = (datetime.now()-timedelta(days=1)).replace(microsecond=0).isoformat()
TO_DATE = datetime.now().replace(microsecond=0).isoformat()

#top marketcap coins according to coinmarketcap
coins= [""bitcoin"",""ethereum"", ""ripple"",""dogecoin"",""cardano"", 
        ""polygon"",""binance"",""polkadot"",""uniswap"",""litecoin"",
        ""chainlink"",""solana"",""stellar""]

#connect to Newsapi Naas drivers
newsapi.connect(NEWSAPI_API_KEY)
newsapi.connect()
#user_response
def get_response(user_message):
    
    usr_msg_text = user_message.lower()
    
    if usr_msg_text in coins: #check if returned message has the top coins 
        data = newsapi.get(q=usr_msg_text,
                                       from_param=FROM_DATE,
                                       to = TO_DATE,
                                       language =""en"")
        if len(data)==0:#extra check to returned data
            sentiment = ""No news for this ticker in last 24 hours, try another""
            return sentiment
        else:
            sentiment_df =  sentiment.get(data, column_name=""title"")#sentiment calculation
            sentiment = sentiment_df[""Score""].mean()
    else:
        sentiment = 'There is no data for this ticker\n use \help command'
    
    return sentiment
#commands
def start_command(update, context):
    update.message.reply_text('''Hello there! I\'m a crypto sentiment bot.
    Send me the coin name, I\'ll send you sentiment!''')


def help_command(update, context):
    update.message.reply_text('''check the coin names from the list below!
    bitcoin\n etherium\n ripple\n dogecoin\n cardano\n polygon
    binance\n polkadot\n uniswap\n litecoin\n chainlink\n solana\n stellar''')

#message handler
def handle_message(update, context):
    text = str(update.message.text).lower()
    logging.info(f'User ({update.message.chat.id}) says: {text}')

    # Bot response
    sentiment_naas = get_response(text)
    update.message.reply_text(sentiment_naas)

#error logs
def error(update, context):
    # Logs errors
    logging.error(f'Update {update} caused error {context.error}')
# Run the programme

# create an object ""bot""
updater = Updater(TELEGRAM_API_KEY, use_context=True)
dp = updater.dispatcher

#  bot's command handlers
dp.add_handler(CommandHandler('start', start_command))
dp.add_handler(CommandHandler('help', help_command))

# bot's text handlers 
dp.add_handler(MessageHandler(Filters.text, handle_message))

# bot's error handler
dp.add_error_handler(error)

    
# Run the bot
updater.start_polling(1.0)
updater.idle()"
10204,IMDB - Top  Movie,"try:
    import scrapy
except:
    !pip install scrapy
    import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.crawler import CrawlerRunner
try:
    from crochet import setup, wait_for
except:
    !pip install crochet
    from crochet import setup, wait_for
setup()
class IMDB(scrapy.Spider):
    """"""The scraping class""""""
    name=""movies""
    # Writing the output to a csv file and saving it as sample.csv
    custom_settings = {'FEEDS': {'sample.csv': {""format"": 'csv','overwrite': True}}}
    
    def start_requests(self):
        """"""The start requests method that holds the url and processes it then send it to the parse method""""""
        urls=[""https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=action"",
        ""https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=adventure"",
        ""https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=animation"",
        ""https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=fantasy"",
        ""https://www.imdb.com/search/title/?title_type=feature&num_votes=25000,&genres=romance""]
        for url in urls:
            yield scrapy.Request(url=url,callback=self.parse)
            
    def parse(self, response):
        """"""The method that is used for subseting and scraping the websites into acceptable formats"""""" 
        movies=response.css(""div.lister-item-content"")
        for movie in movies:
            items={
            ""title"" :movie.css(""h3.lister-item-header"").css(""a::text"").get(),
            ""year"":movie.css(""span.lister-item-year.text-muted.unbold::text"").get().replace(""("","""").replace("")"","""").replace(""I"",""""),
            ""rating"":movie.css(""span.certificate::text"").get(),
            ""duration"":movie.css(""span.runtime::text"").get(),
            ""genre"":movie.css(""span.genre::text"").get().strip(),
            ""Total vote rating"":movie.css(""div.inline-block.ratings-imdb-rating>strong::text"").get(),
            ""Number of votes"":movie.css(""p.sort-num_votes-visible>span:nth-of-type(2)::text"").get(),
            ""Director"":movie.css(""p:nth-of-type(3)>a:nth-of-type(1)::text"").get()
            }
            yield items
            #You can delete the next 3 lines if you need just the first page and not all pages.
        next_page=response.css(""div.desc>a::attr(href)"").get()
        if next_page is not None:
            yield response.follow(next_page,callback=self.parse)
@wait_for(10) # To avoid reactor time error
def run_spider():
    """"""run spider""""""
    crawler = CrawlerRunner()
    result = crawler.crawl(IMDB)
    return result
run_spider()"
10205,Reddit - Get Hot Posts From Subreddit,"!pip install praw
import praw
import pandas as pd
import numpy as np
from datetime import datetime
SUBREDDIT = ""Python"" #example: ""CryptoCurrency""
MY_CLIENT_ID = 'EtAr0o-oKbVuEnPOFbrRqQ'
MY_CLIENT_SECRET = 'LmNpsZuFM-WXyZULAayVyNsOhMd_ug'
MY_USER_AGENT = 'script by u/naas'
reddit = praw.Reddit(client_id=MY_CLIENT_ID, client_secret=MY_CLIENT_SECRET, user_agent=MY_USER_AGENT)
posts =[]
for post in reddit.subreddit(SUBREDDIT).hot(limit=50):
    
    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])
posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])
posts['created']=pd.to_datetime(posts[""created""],unit='s')
posts.head()"
10206,Remotive - Post daily jobs on slack,"import pandas as pd
from bs4 import BeautifulSoup
import requests
from datetime import datetime
import time
from naas_drivers import gsheet, slack
import naas
SLACK_TOKEN = ""xoxb-1481042297777-3085654341191-xxxxxxxxxxxxxxxxxxxxxxxxx""
SLACK_CHANNEL = ""05_work""
spreadsheet_id = ""1EBefhkbmqaXMZLRCiafabf6xxxxxxxxxxxxxxxxxxx""
sheet_name = ""SLACK_CHANNEL_POSTS""
def get_remotejob_categories():
    req_url = f""https://remotive.io/api/remote-jobs/categories""
    res = requests.get(req_url)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        return e
    res_json = res.json()
    
    # Get categories
    jobs = res_json.get('jobs')
    return pd.DataFrame(jobs)

df_categories = get_remotejob_categories()
df_categories
categories = ['data'] # Pick the list of categories in columns ""slug""
date_from = - 10 # Choose date difference in days from now => must be negative
naas.scheduler.add(recurrence=""0 9 * * *"")
# # naas.scheduler.delete() # Uncomment this line to delete your scheduler if needed
df_jobs_log = gsheet.connect(spreadsheet_id).get(sheet_name=sheet_name)
df_jobs_log
REMOTIVE_DATETIME = ""%Y-%m-%dT%H:%M:%S""
NAAS_DATETIME = ""%Y-%m-%d %H:%M:%S""

def get_remotive_jobs_since(jobs, date):
    ret = []
    for job in jobs:
        publication_date = datetime.strptime(job['publication_date'], REMOTIVE_DATETIME).timestamp()
        if publication_date > date:
            ret.append({
                'URL': job['url'],
                'TITLE': job['title'],
                'COMPANY': job['company_name'],
                'PUBLICATION_DATE': datetime.fromtimestamp(publication_date).strftime(NAAS_DATETIME)
            })
    return ret

def get_category_jobs_since(category, date, limit):
    url = f""https://remotive.io/api/remote-jobs?category={category}&limit={limit}""
    res = requests.get(url)
    if res.json()['jobs']:
        publication_date = datetime.strptime(res.json()['jobs'][-1]['publication_date'], REMOTIVE_DATETIME).timestamp()
        if len(res.json()['jobs']) < limit or date > publication_date:
            print(f""Jobs from catgory {category} fetched ‚úÖ"")
            return get_remotive_jobs_since(res.json()['jobs'], date)
        else:
            return get_category_jobs_since(category, date, limit + 5)
    return []

def get_jobs_since(categories: list,
                   date_from: int):
    if date_from >= 0:
        return(""'date_from' must be negative. Please update your parameter."")
    # Transform datefrom int to
    search_jobs_from = date_from * 24 * 60 * 60   # days in seconds
    timestamp_date = time.time() + search_jobs_from

    jobs = []
    for category in categories:
        jobs += get_category_jobs_since(category, timestamp_date, 5)
    print(f'- All job since {datetime.fromtimestamp(timestamp_date)} have been fetched -')
    return pd.DataFrame(jobs)

df_jobs = get_jobs_since(categories, date_from=date_from)
df_jobs
def remove_duplicates(df1, df2):
    # Get jobs log
    jobs_log = df1.URL.unique()
    
    # Exclude jobs already log from jobs
    df2 = df2[~df2.URL.isin(jobs_log)]
    return df2.sort_values(by=""PUBLICATION_DATE"")

df_new_jobs = remove_duplicates(df_jobs_log, df_jobs)
df_new_jobs
gsheet.connect(spreadsheet_id).send(sheet_name=sheet_name,
                                    data=df_new_jobs,
                                    append=True)
if len(df_new_jobs) > 0:
    for _, row in df_new_jobs.iterrows():
        url = row.URL
        slack.connect(SLACK_TOKEN).send(SLACK_CHANNEL, f""<{url}>"")
else:
    print(""Nothing to published in Slack !"")"
10207,Remotive - Get categories from job,"import pandas as pd
import requests
def get_remotejob_categories():
    req_url = f""https://remotive.io/api/remote-jobs/categories""
    res = requests.get(req_url)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        return e
    res_json = res.json()
    
    # Get categories
    jobs = res_json.get('jobs')
    return pd.DataFrame(jobs)

df_categories = get_remotejob_categories()
df_categories"
10208,Remotive - Get jobs from categories,"import pandas as pd
import requests
import time
from datetime import datetime
def get_remotejob_categories():
    req_url = f""https://remotive.io/api/remote-jobs/categories""
    res = requests.get(req_url)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        return e
    res_json = res.json()
    
    # Get categories
    jobs = res_json.get('jobs')
    return pd.DataFrame(jobs)

df_categories = get_remotejob_categories()
df_categories
categories = ['data'] # Pick the list of categories in columns ""slug""
date_from = - 10 # Choose date difference in days from now => must be negative
csv_output = ""REMOTIVE_JOBS.csv""
REMOTIVE_DATETIME = ""%Y-%m-%dT%H:%M:%S""
NAAS_DATETIME = ""%Y-%m-%d %H:%M:%S""

def get_remotive_jobs_since(jobs, date):
    ret = []
    for job in jobs:
        publication_date = datetime.strptime(job['publication_date'], REMOTIVE_DATETIME).timestamp()
        if publication_date > date:
            ret.append({
                'URL': job['url'],
                'TITLE': job['title'],
                'COMPANY': job['company_name'],
                'PUBLICATION_DATE': datetime.fromtimestamp(publication_date).strftime(NAAS_DATETIME)
            })
    return ret

def get_category_jobs_since(category, date, limit):
    url = f""https://remotive.io/api/remote-jobs?category={category}&limit={limit}""
    res = requests.get(url)
    if res.json()['jobs']:
        publication_date = datetime.strptime(res.json()['jobs'][-1]['publication_date'], REMOTIVE_DATETIME).timestamp()
        if len(res.json()['jobs']) < limit or date > publication_date:
            print(f""Jobs from catgory {category} fetched ‚úÖ"")
            return get_remotive_jobs_since(res.json()['jobs'], date)
        else:
            return get_category_jobs_since(category, date, limit + 5)
    return []

def get_jobs_since(categories: list,
                   date_from: int):
    if date_from >= 0:
        return(""'date_from' must be negative. Please update your parameter."")
    # Transform datefrom int to
    search_jobs_from = date_from * 24 * 60 * 60   # days in seconds
    timestamp_date = time.time() + search_jobs_from

    jobs = []
    for category in categories:
        jobs += get_category_jobs_since(category, timestamp_date, 5)
    print(f'- All job since {datetime.fromtimestamp(timestamp_date)} have been fetched:', len(jobs))
    return pd.DataFrame(jobs)

df_jobs = get_jobs_since(categories, date_from=date_from)
df_jobs.head(5)
df_jobs.to_csv(csv_output, index=False)"
10209,Remotive - Send jobs to gsheet,"import pandas as pd
import requests
from datetime import datetime
import time
from naas_drivers import gsheet
import naas
spreadsheet_id = ""1EBefhkbmqaXMZLRCiafaxxxxxxxxxxxxxxxx""
sheet_name = ""SLACK_CHANNEL_POSTS""
def get_remotejob_categories():
    req_url = f""https://remotive.io/api/remote-jobs/categories""
    res = requests.get(req_url)
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        return e
    res_json = res.json()
    
    # Get categories
    jobs = res_json.get('jobs')
    return pd.DataFrame(jobs)

df_categories = get_remotejob_categories()
df_categories
categories = ['data'] # Pick the list of categories in columns ""slug""
date_from = - 10 # Choose date difference in days from now => must be negative
naas.scheduler.add(recurrence=""0 9 * * *"")
# # naas.scheduler.delete() # Uncomment this line to delete your scheduler if needed
df_jobs_log = gsheet.connect(spreadsheet_id).get(sheet_name=sheet_name)
df_jobs_log
REMOTIVE_DATETIME = ""%Y-%m-%dT%H:%M:%S""
NAAS_DATETIME = ""%Y-%m-%d %H:%M:%S""

def get_remotive_jobs_since(jobs, date):
    ret = []
    for job in jobs:
        publication_date = datetime.strptime(job['publication_date'], REMOTIVE_DATETIME).timestamp()
        if publication_date > date:
            ret.append({
                'URL': job['url'],
                'TITLE': job['title'],
                'COMPANY': job['company_name'],
                'PUBLICATION_DATE': datetime.fromtimestamp(publication_date).strftime(NAAS_DATETIME)
            })
    return ret

def get_category_jobs_since(category, date, limit):
    url = f""https://remotive.io/api/remote-jobs?category={category}&limit={limit}""
    res = requests.get(url)
    if res.json()['jobs']:
        publication_date = datetime.strptime(res.json()['jobs'][-1]['publication_date'], REMOTIVE_DATETIME).timestamp()
        if len(res.json()['jobs']) < limit or date > publication_date:
            print(f""Jobs from catgory {category} fetched ‚úÖ"")
            return get_remotive_jobs_since(res.json()['jobs'], date)
        else:
            return get_category_jobs_since(category, date, limit + 5)
    return []

def get_jobs_since(categories: list,
                   date_from: int):
    if date_from >= 0:
        return(""'date_from' must be negative. Please update your parameter."")
    # Transform datefrom int to
    search_jobs_from = date_from * 24 * 60 * 60   # days in seconds
    timestamp_date = time.time() + search_jobs_from

    jobs = []
    for category in categories:
        jobs += get_category_jobs_since(category, timestamp_date, 5)
    print(f'- All job since {datetime.fromtimestamp(timestamp_date)} have been fetched -')
    return pd.DataFrame(jobs)

df_jobs = get_jobs_since(categories, date_from=date_from)
df_jobs
def remove_duplicates(df1, df2):
    # Get jobs log
    jobs_log = df1.URL.unique()
    
    # Exclude jobs already log from jobs
    df2 = df2[~df2.URL.isin(jobs_log)]
    return df2.sort_values(by=""PUBLICATION_DATE"")

df_new_jobs = remove_duplicates(df_jobs_log, df_jobs)
df_new_jobs
gsheet.connect(spreadsheet_id).send(sheet_name=sheet_name,
                                    data=df_new_jobs,
                                    append=True)"
10210,SAP-HANA - Query data,"!pip install hdbcli
import sap_hana_connector
type: 'SapHana'
name: 'JPAK_LIVE'
user: 'USER'
password: 'PASSWORD'
port: 30015
host: 'HOST'
query: 'SELECT * FROM JPAK_LIVE.OINV T0'
df = query"
10211,Cityfalcon - Get data from API,"from naas_drivers import cityfalcon
API_KEY = ""YOUR_API_KEY""
CF = cityfalcon.connect(API_KEY).get(""TSLA"")
CF"
10212,Matplotlib - Create Waterfall chart,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
#Use python 2.7+ syntax to format currency
def money(x, pos):
    'The two args are the value and tick position'
    return ""${:,.0f}"".format(x)
formatter = FuncFormatter(money)

#Data to plot. Do not include a total, it will be calculated
index = ['sales','returns','credit fees','rebates','late charges','shipping']
data = {'amount': [350000,-30000,-7500,-25000,95000,-7000]}

#Store data and create a blank series to use for the waterfall
trans = pd.DataFrame(data=data,index=index)
blank = trans.amount.cumsum().shift(1).fillna(0)

#Get the net total number for the final element in the waterfall
total = trans.sum().amount
trans.loc[""net""]= total
blank.loc[""net""] = total

#The steps graphically show the levels as well as used for label placement
step = blank.reset_index(drop=True).repeat(3).shift(-1)
step[1::3] = np.nan

#When plotting the last element, we want to show the full bar,
#Set the blank to 0
blank.loc[""net""] = 0

#Plot and label
my_plot = trans.plot(kind='bar', stacked=True, bottom=blank,legend=None, figsize=(10, 5), title=""2014 Sales Waterfall"")
my_plot.plot(step.index, step.values,'k')
my_plot.set_xlabel(""Transaction Types"")

#Format the axis for dollars
my_plot.yaxis.set_major_formatter(formatter)

#Get the y-axis position for the labels
y_height = trans.amount.cumsum().shift(1).fillna(0)

#Get an offset so labels don't sit right on top of the bar
max = trans.max()
neg_offset = max / 25
pos_offset = max / 50
plot_offset = int(max / 15)

#Start label loop
loop = 0
for index, row in trans.iterrows():
    # For the last item in the list, we don't want to double count
    if row['amount'] == total:
        y = y_height[loop]
    else:
        y = y_height[loop] + row['amount']
    # Determine if we want a neg or pos offset
    if row['amount'] > 0:
        y += pos_offset
    else:
        y -= neg_offset
    my_plot.annotate(""{:,.0f}"".format(row['amount']),(loop,y),ha=""center"")
    loop+=1
#Scale up the y axis so there is room for the labels
my_plot.set_ylim(0,blank.max()+int(plot_offset))
#Rotate the labels
my_plot.set_xticklabels(trans.index,rotation=0)
my_plot.get_figure().savefig(""waterfall.png"",dpi=200,bbox_inches='tight')"
10213,Quandl - Get data from API,"!pip install quandl
import quandl
import matplotlib.pyplot as plt
data = quandl.get('EIA/PET_RWTC_D')
data
%matplotlib inline
data.plot()"
10214,Quandl - Get data from CSV,"import matplotlib.pyplot as plt
import pandas as pd
import os 
data = pd.read_csv('https://www.quandl.com/api/v3/datasets/BITFINEX/BTCUSD.csv?api_key=bxrSXWimkiknuCcV71uL')
data
%matplotlib inline
data.plot()"
10215,Jupyter Notebooks - Get installs,"import json
from pprint import pprint
# Input
notebook_path = ""../template.ipynb""
def get_installs(notebook_path):
    with open(notebook_path) as f:
        nb = json.load(f)
    data = []
    
    cells = nb.get(""cells"")
    # Check each cells
    for cell in cells:
        cell_type = cell.get('cell_type')
        sources = cell.get('source')
        for source in sources:
            if cell_type == ""code"":
                if ""pip install"" in source:
                    install = source.split(""pip install"")[-1].replace(""\n"", """").strip()
                    data.append(install)
    if len(data) == 0:
        print(""‚ùé No install found in notebook:"", notebook_path)
    else:
        print(f""‚úÖ {len(data)} install(s) found in notebook:"", notebook_path)
    return data
installs = get_installs(notebook_path)
print(installs)
"
10216,Jupyter Notebooks - Count code characters,"import json
# Input
notebook_path = ""../template.ipynb""
def count_characters(notebook_path):
    with open(notebook_path) as f:
        nb = json.load(f)
    data = 0
    
    cells = nb.get(""cells"")
    # Check each cells
    for cell in cells:
        cell_type = cell.get('cell_type')
        sources = cell.get('source')
        for source in sources:
            if cell_type == ""code"":
                if not source.startswith('\n') and not source.startswith('#'):
                    char = source.replace("" "", """")
                    data += len(char)
    if data == 0:
        print(""‚ùé No character of code wrote in notebook:"", notebook_path)
    else:
        print(f""‚úÖ {data} character(s) of code wrote in notebook:"", notebook_path)
    return data
no_characters = count_characters(notebook_path)
no_characters"
10217,Jupyter Notebooks - Read file json,"import json
from pprint import pprint
# Input
notebook_path = ""../template.ipynb""
with open(notebook_path) as f:
    nb = json.load(f)
pprint(nb)"
10218,Jupyter Notebooks - Add tags in cells,"import os
import json
from pprint import pprint
# Notebook path to add tags
notebook_path = ""AWS/AWS_Upload_file_to_S3_bucket.ipynb""

# Tags to added
new_tags = [f""awesome-notebooks/{notebook_path}""]
def add_tags(notebook_path, new_tags):
    with open(notebook_path) as f:
        nb = json.load(f)
        
    new_cells = []
    cells = nb.get(""cells"")
    
    # Apply change
    for cell in cells:
        tags = cell.get('metadata').get('tags')
        if len(tags) == 0:
            tags = new_tags
        else:
            tags += new_tags
        cell[""metadata""][""tags""] = tags
        new_cells.append(cell)
        
    # Save notebook
    nb_new = nb.copy()
    nb_new[""cells""] = new_cells
    with open(notebook_path, 'w') as f:
        json.dump(nb_new, f)
    print(f""‚úîÔ∏è {notebook_path} saved in Naas."")
    return nb_new

nb = add_tags(notebook_path, new_tags)
pprint(nb)"
10219,Jupyter Notebooks - Count code lines,"import json
# Input
notebook_path = ""../template.ipynb""
def count_codes(notebook_path):
    with open(notebook_path) as f:
        nb = json.load(f)
    data = 0
    
    cells = nb.get(""cells"")
    # Check each cells
    for cell in cells:
        cell_type = cell.get('cell_type')
        sources = cell.get('source')
        for source in sources:
            if cell_type == ""code"":
                if not source.startswith('\n') and not source.startswith('#'):
                    data += 1
    if data == 0:
        print(""‚ùé No line of code wrote in notebook:"", notebook_path)
    else:
        print(f""‚úÖ {data} line(s) of code wrote in notebook:"", notebook_path)
    return data
no_lines = count_codes(notebook_path)
no_lines
"
10220,Jupyter Notebooks - Save file ipynb,"import json
from pprint import pprint
# Input
notebook_path = ""../template.ipynb""

# Output
notebook_output = ""new_template.ipynb""
with open(notebook_path) as f:
    nb = json.load(f)
with open(notebook_output, 'w') as f:
    json.dump(nb, f)
pprint(nb)"
10221,Jupyter Notebooks - Get libraries,"import json
from pprint import pprint
# Input
notebook_path = ""../template.ipynb""
def get_libraries(notebook_path):
    with open(notebook_path) as f:
        nb = json.load(f)
    data = []
    
    cells = nb.get(""cells"")
    # Check each cells
    for cell in cells:
        cell_type = cell.get('cell_type')
        sources = cell.get('source')
        for source in sources:
            if cell_type == ""code"":
                if ""from"" in source and ""import"" in source:
                    lib = source.replace(""\n"", """").split(""from"")[-1].split(""import"")[0].strip()
                    module = source.replace(""\n"", """").split(""import"")[-1].split("" as "")[0].strip()
                    data.append(f""{lib}.{module}"")
                if ""from"" not in source and ""import"" in source:
                    library = source.replace(""\n"", """").split(""import"")[-1].split("" as "")[0].strip()
                    data.append(library)
    if len(data) == 0:
        print(""‚ùé No library found in notebook:"", notebook_path)
    else:
        print(f""‚úÖ {len(data)} librarie(s) found in notebook:"", notebook_path)
    return data
libraries = get_libraries(notebook_path)
print(libraries)
"
10222,Jupyter Notebooks - Add cells in notebook json,"import json
from pprint import pprint
notebook_path = ""Jupyter_Notebooks_Get_libraries.ipynb""
def add_cells(notebook_path):
    notebook_path
    with open(notebook_path) as f:
        nb = json.load(f)
    root = notebook_path.split(""/"")[0]
    check_logo = False
    check_title = False
    check_download = False
    check_tags = False
    check_author = False
    check_input = False
    check_model = False 
    check_output = False
    no_markdown = 0
    
    cells = nb.get(""cells"")
    # Check each cells
    for cell in cells:
        cell_type = cell.get('cell_type')
        sources = cell.get('source')    
        if cell_type == ""markdown"":
            for source in sources:
                if source.startswith(""<img"") and no_markdown == 0:
                    check_logo = True
                if source.startswith(""# "") and root in source:
                    check_title = True
                if ""https://app.naas.ai/user-redirect/naas/downloader"" in source:
                    check_download = True
                if ""**Tags:**"" in source:
                    check_tags = True
                    tags = source[9:].strip()
                if ""**Author:**"" in source:
                    check_author = True
                    author = source[11:].split(""["")[-1].split(""]"")[0]
                if source.startswith(""## Input""):
                    check_input = True
                if source.startswith(""## Model""):
                    check_model = True
                if source.startswith(""## Output""):
                    check_output = True
                
    # Check
    add_logo = False
    add_author = False
    if not check_logo:
        print(""Logo to be added"")
        add_logo = True
    if not check_author and check_tags:
        print(""Author to be added below tags"")
        add_author = True
        
    # Apply change
    new_cells = []
    if add_logo:
        cell_logo = {'cell_type': 'markdown',
                     'id': 'naas-logo',
                     'metadata': {'papermill': {}, 'tags': [""naas""]},
                     'source': '<img width=""10%"" alt=""Naas"" src=""https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160""/>'}
        new_cells.append(cell_logo)
    for cell in cells:
        new_cells.append(cell)
        cell_type = cell.get('cell_type')
        source = cell.get('source')

        if cell_type == ""markdown"":
            if ""**Tags:**"" in source and add_author:
                cell_author = {'cell_type': 'markdown',
                               'id': 'naas-author',
                               'metadata': {'papermill': {}, 'tags': [""naas""]},
                               'source': '**Author:** [Unknown](https://www.linkedin.com/company/naas-ai/)'}
                new_cells.append(cell_author)
    if add_logo or add_author:
        nb_new = nb.copy()
        nb_new[""cells""] = new_cells
        with open(notebook_path, 'w') as f:
            json.dump(nb_new, f)
        print(f""{notebook_path} saved in Naas."")
        return nb_new
    else:
        print(""Nothing to be added in notebooks !"")
nb = add_cells(notebook_path)
pprint(nb)
"
10223,Slack - Follow number of users in workspace,"!pip install slack-sdk --user
import naas
import pandas as pd
import plotly.graph_objects as go
from slack_sdk import WebClient
from datetime import datetime
SLACK_BOT_TOKEN = ""xoxb-232887839156-1673274923699-vTF6xxxxxxxxxx""
client = WebClient(token=SLACK_BOT_TOKEN)
def get_channel_ids():
    channel_id = {}
    for channel_info in client.conversations_list().data['channels']:
        key, value = channel_info['name'], channel_info['id']
        channel_id[key] = value
    return channel_id

channel_dict = get_channel_ids()
channel_dict
# Outputs
title = ""Title of your chart""
name_output = ""My_output""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
# Schedule your notebook every hour
naas.scheduler.add(cron=""0 * * * *"")

#-> Uncomment the line below and execute this cell to delete your scheduler
# naas.scheduler.delete()
def list_users():
    df = pd.DataFrame()
    idx=0
    for user_data in client.users_list().data['members']:
        if ('real_name' in user_data and user_data['real_name'] != 'Slackbot') and not user_data['is_bot']:
            df.loc[idx,'NAME'] = user_data['profile']['real_name']
            df.loc[idx,'ID'] = user_data['id']
            df.loc[idx,'FIRST_VIEWED_AT'] = datetime.fromtimestamp(user_data['updated'])
            idx+=1
    
    return df

df_slack = list_users()
df_slack
def list_users_histo():
    # Load csv file containing already seen users
    try:
        df = pd.read_csv(csv_output)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()

    return df

df_slack_histo = list_users_histo()
df_slack_histo 
# Append new users to historical data with today's date.

def merge_dataframes(df_slack, df_slack_histo):
	# Add new users + date. It could be a two columns dataframe ['EMAIL', 'DATE_EXTRACT']
	
    if len(df_slack_histo) == 0:
        return df_slack
    else:
        historical_data = df_slack_histo.ID.to_list()
        for idx, row in df_slack.iterrows():
            if row['ID'] not in historical_data:
                df_slack_histo = df_slack_histo.append(row)
        
        return df_slack_histo

merged_df = merge_dataframes(df_slack, df_slack_histo)
merged_df
def get_trend(df,
              date_col_name='FIRST_VIEWED_AT',
              value_col_name=""ID"",
              date_order='asc'):
    
    # Format date
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    # Calc sum cum
    df[""value_cum""] = df.agg({value_col_name: ""cumsum""})
    return df.reset_index(drop=True)

df_trend = get_trend(merged_df)
df_trend
def create_linechart(df, label, value, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            mode=""lines"",
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=title,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, label=""FIRST_VIEWED_AT"", value=""value_cum"", title=title)
# Save your dataframe in CSV
merged_df.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10224,Slack - Send message,"from naas_drivers import slack
SLACK_TOKEN = ""xoxb-232887839156-1673274923699-vTF6NOKOMosoPFI7qfnkCdRF""
channel = ""-team-tech""
message = ""second test""
image_url = ""http://i.imgur.com/c4jt321l.png"" # Set to None if you don't need it
slack.connect(token).send(channel, ""second test"", image=image_url)"
10225,Slack - Add new user to Google Sheets,"!pip install slack-sdk --user
from naas_drivers import gsheet
import naas
import pandas as pd
from slack_sdk import WebClient
from datetime import datetime
SLACK_BOT_TOKEN = ""xoxb-232887839156-1673274923699-vTF6xxxxxxxxxx""
client = WebClient(token=SLACK_BOT_TOKEN)
SPREADSHEET_URL = ""---""
SHEET_NAME = ""Sheet1""
# Schedule your notebook every hour
naas.scheduler.add(cron=""0 * * * *"")

#-> Uncomment the line below and execute this cell to delete your scheduler
# naas.scheduler.delete()
def list_users():
    df = pd.DataFrame()
    idx=0
    for user_data in client.users_list().data['members']:
        if ('real_name' in user_data and user_data['real_name'] != 'Slackbot') and not user_data['is_bot']:
            df.loc[idx,'NAME'] = user_data['profile']['real_name']
            df.loc[idx,'ID'] = user_data['id']
            df.loc[idx,'FIRST_VIEWED_AT'] = datetime.fromtimestamp(user_data['updated'])
            idx+=1
    
    return df

df_slack = list_users()
df_slack
df_gsheet = gsheet.connect(SPREADSHEET_URL).get(sheet_name=SHEET_NAME)
df_gsheet
def get_new_users(df_slack, df_gsheet):
    if len(df_gsheet) == 0:
        return df_slack
    else:
        historical_data = df_gsheet.ID.to_list()
        df_new=pd.DataFrame()
        for idx, row in df_slack.iterrows():
            if row['ID'] not in historical_data:
                df_new = df_new.append(row)
        return df_new

df_new = get_new_users(df_slack, df_gsheet)
df_new
# Send data to Google Sheets
gsheet.connect(SPREADSHEET_URL).send(
    sheet_name=SHEET_NAME,
    data=df_new,
	append=True
)"
10226,AWS - Daily biling notification to slack,"!pip install boto3
#naas.secret.add(name=""AWS_ACCESS_KEY_ID"", secret=""***"") 
#naas.secret.add(name=""AWS_SECRET_ACCESS_KEY"", secret=""***"")
#naas.secret.add(name=""SLACK_TOKEN"", secret=""***"")
import datetime
import boto3
import naas
import dateutil.relativedelta
import pandas as pd
import naas_drivers
# AWS account
AWS_ACCESS_KEY_ID = naas.secret.get(name=""AWS_ACCESS_KEY_ID"")
AWS_SECRET_ACCESS_KEY = naas.secret.get(name=""AWS_SECRET_ACCESS_KEY"")

# Slack
SLACK_TOKEN = naas.secret.get(name=""SLACK_TOKEN"")
SLACK_CHANNEL = ""-aws-billing""
# Compute dates
def last_day_of_month(any_day):
    # this will never fail
    # get close to the end of the month for any day, and add 4 days 'over'
    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)
    # subtract the number of remaining 'overage' days to get last day of current month, or said programattically said, the previous day of the first of next month
    return next_month - datetime.timedelta(days=next_month.day)

today = datetime.date.today()
lastDay = last_day_of_month(today)
start_month_date = (today - dateutil.relativedelta.relativedelta(months=12))

start_date = ""{}-{:02d}-{:02d}"".format(today.year, today.month, 1)
end_date = ""{}-{:02d}-{:02d}"".format(today.year, today.month, today.day)
last_day = ""{}-{:02d}-{:02d}"".format(lastDay.year, lastDay.month, lastDay.day)
client = boto3.client('ce',
                      aws_access_key_id=AWS_ACCESS_KEY_ID,
                      aws_secret_access_key=AWS_SECRET_ACCESS_KEY)
result = client.get_cost_and_usage(
    TimePeriod = {
        'Start': start_date,
        'End': end_date
    },
    Granularity = 'MONTHLY',
    Filter = {
        ""Dimensions"": {
                    ""Key"": ""RECORD_TYPE"",
                    ""Values"": [""Credit"", ""Refund""]
                }
    },
    Metrics = [""BlendedCost""],
    GroupBy = [
        {
            'Type': 'DIMENSION',
            'Key': 'SERVICE'
        },
        {
            'Type': 'DIMENSION',
            'Key': 'USAGE_TYPE'
        }
    ]
)
df_billing = pd.DataFrame()

for t in result[""ResultsByTime""]:
    for r in t[""Groups""]:
        dimension = r[""Keys""][0]
        usage_type = r[""Keys""][1]
        amount = r[""Metrics""][""BlendedCost""][""Amount""]
        period_start = t[""TimePeriod""][""Start""]
        period_end = t[""TimePeriod""][""End""]
        df_billing = df_billing.append({
            ""Dimension"": dimension,
            ""UsageType"": usage_type,
            ""Amount"": amount,
            ""PeriodStart"": period_start,
            ""PeriodEnd"": period_end
        }, ignore_index=True)
df_billing = df_billing.astype({'Amount': 'float'})

# Display result
df_billing.tail(5)
ce_forecast = client.get_cost_forecast(
    TimePeriod={
        'Start': end_date,
        'End': last_day
    },
    Metric='BLENDED_COST',
    Granularity='MONTHLY'
)
df_billing.to_csv('current_month_data.csv')
naas.asset.add(path='current_month_data.csv')
current_amount = df_billing[""Amount""].sum()
forecast = float(ce_forecast[""Total""][""Amount""])
asset_link = ""Copy URL from generate asset above""
message = """"""
Hey there, 

This is your daily AWS billing notification.

- Current spending: [*{:.2f}$*]
- Forecast left to spent: [*{:.2f}$*]
- End of month estimate: [*{:.2f}$*]

Download the detailed csv file {}
"""""".format(float(current_amount), float(forecast), float(current_amount - forecast), asset_link)
image_url = None # Set to None if you don't need it
print(message)
naas_drivers.slack.connect(SLACK_TOKEN).send(SLACK_CHANNEL, message)
naas.scheduler.add(cron=""0 9 * * *"")"
10227,AWS - Read dataframe from S3,"try:
    import awswrangler as wr
except:
    !pip install awswrangler --user
    import awswrangler as wr
# Credentials
AWS_ACCESS_KEY_ID = ""YOUR_AWS_ACCESS_KEY_ID""
AWS_SECRET_ACCESS_KEY = ""YOUR_AWS_SECRET_ACCESS_KEY""
AWS_DEFAULT_REGION = ""YOUR_AWS_DEFAULT_REGION""

# Bucket
BUCKET_PATH = f""s3://naas-data-lake/dataset/""
%env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
%env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
%env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION
df = wr.s3.read_parquet(BUCKET_PATH, dataset=True)
df"
10228,AWS - Upload file to S3 bucket,"try:
    import boto3
except:
    !pip install boto3 getpass4
    import boto3
ACCESS_KEY_ID = ""**********""
SECRET_ACCESS_KEY = ""**********""

BUCKET_NAME = ""naas-example""
BUCKET_OBJECT_KEY = 'naas_happy_hour.mp3'
s3 = boto3.client('s3',
                  aws_access_key_id=ACCESS_KEY_ID,
                  aws_secret_access_key=SECRET_ACCESS_KEY)
with open(BUCKET_OBJECT_KEY, ""rb"") as f:
    s3.upload_fileobj(f, BUCKET_NAME, BUCKET_OBJECT_KEY)"
10229,AWS - Send dataframe to S3,"try:
    import awswrangler as wr
except:
    !pip install awswrangler --user
    import awswrangler as wr
import pandas as pd
from datetime import date
# Credentials
AWS_ACCESS_KEY_ID = ""YOUR_AWS_ACCESS_KEY_ID""
AWS_SECRET_ACCESS_KEY = ""YOUR_AWS_SECRET_ACCESS_KEY""
AWS_DEFAULT_REGION = ""YOUR_AWS_DEFAULT_REGION""

# Bucket
BUCKET_PATH = f""s3://naas-data-lake/dataset/""
%env AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
%env AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
%env AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION
df = pd.DataFrame({
    ""id"": [1, 2],
    ""value"": [""foo"", ""boo""],
    ""date"": [date(2020, 1, 1), date(2020, 1, 2)]
})

# Display dataframe
df
wr.s3.to_parquet(
    df=df,
    path=BUCKET_PATH,
    dataset=True,
    mode=""overwrite""
)"
10230,AWS - Get files from S3 bucket,"!pip install boto3 getpass4
import boto3
ACCESS_KEY_ID = ""**********""
SECRET_ACCESS_KEY = ""**********""

BUCKET_NAME = ""naas-example""
BUCKET_OBJECT_KEY = 'naas_happy_hour.mp3'
s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY_ID, aws_secret_access_key=SECRET_ACCESS_KEY)
fileObj = s3.get_object(Bucket=bucketname, Key=filename)
file_url = s3.generate_presigned_url(""get_object"", Params={""Bucket"": BUCKET_NAME, ""Key"": BUCKET_OBJECT_KEY}, ExpiresIn=604800)
fileOBJ
file_url"
10231,Naas - Get number of downloads naas drivers package,"try:
    import pypistats
except:
    !pip install -U pypistats --user
    !pip install --upgrade pypistats
    import pypistats
# from pprint import pprint
from datetime import datetime
import plotly.graph_objects as go
import naas
# Inputs
package = ""naas-drivers""

# Outputs
name_output = f""{package}_downloads""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df = pypistats.overall(package, total=False, format=""pandas"")
df.head()
# Gives us the cumulative number of downloads over a period of 180 days
def get_cumulative_dataframe(df):
    
    data = df.groupby('category').get_group('with_mirrors').sort_values(
        'date').reset_index(drop='index').groupby(
        'date').agg({'downloads':'sum'}).reset_index()
    
    cum_sum = 0
    for idx, num in enumerate(data['downloads']):
        cum_sum+=num
        data.loc[idx, 'cumulative_downloads'] = cum_sum

    data['cumulative_downloads'] = data.cumulative_downloads.astype('int')
    data.drop(columns = 'downloads', inplace=True)
    
    return data

df_downloads = get_cumulative_dataframe(df)
df_downloads.tail(5)
def create_linechart(df, package, date, value):
    # Get last value
    last_value = ""{:,.0f}"".format(df.loc[df.index[-1], value]).replace("","", "" "")
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date].to_list(),
            y=df[value].to_list(),
            mode=""lines+text"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""‚≠ê<b> Number of downloads for {package} </b><br><span style='font-size: 13px;'> Total Downloads as of today: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        yaxis_title='No. of downloads',
        yaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_downloads, package, ""date"", ""cumulative_downloads"")
# Save your dataframe in CSV
df_downloads.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10232,Naas - Configure Github with ssh,"import os
if not (os.path.isfile('/home/ftp/.ssh/id_rsa') and os.path.isfile('/home/ftp/.ssh/id_rsa.pub')):
    print(f'üí° We need to create an SSH key.')
    !mkdir -p /home/ftp/.ssh/
    !ssh-keygen -b 2048 -t rsa -f /home/ftp/.ssh/id_rsa -q -N """"
else:
    print('‚úÖ All good you already have an RSA key pair.')
!cat /home/ftp/.ssh/id_rsa.pub
! git clone git@github.com:jupyter-naas/awesome-notebooks.git"
10233,Naas - Set timezone,"import naas
new_timezone = ""Europe/Paris""
naas.get_remote_timezone()
naas.set_remote_timezone(new_timezone)"
10234,Naas - Scheduler demo,"import naas
# Email receiver
email_to = ""hello@naas.ai""

# Email subject
subject = ""Naas Scheduler Test""

# Email content
content ='''<p>If i put html in there..&nbsp;</p>
<p><img src=""https://specials-images.forbesimg.com/imageserve/5f1f37a40a5db2c8275972c0/960x0.jpg?fit=scale"" alt="""" width=""959"" height=""663"" /></p>'''
# Schedule your notebook everyday at 1:00 AM
naas.scheduler.add(cron=""0 1 * * *"")

# To delete your scheduler, uncomment the line below and execute this cell
# naas.scheduler.delete()
result = naas.notification.send(email_to, subject, content)
result"
10235,Naas - Asset demo,"import naas 
# Add the path of the document you want to share (don't forget to put the extension)
PATH = ""PATH_OF_YOUR_DOCUMENT.png""
url = naas.asset.add(PATH)

# To delete your asset, uncomment the line below and execute this cell
# naas.asset.delete(PATH)
url"
10236,Naas - Get help,"import naas
naas.open_help()
naas.close_help()"
10237,Naas - Get total downloads naas libraries,"try:
    import pypistats
except:
    !pip install -U pypistats --user
    !pip install --upgrade pypistats
    import pypistats
# from pprint import pprint
from datetime import datetime
import plotly.graph_objects as go
import naas
import pandas as pd
packages = [""naas"", ""naas-drivers""]
df_final = pd.DataFrame()
for package in packages:
    df = pypistats.overall(package, total=False, format=""pandas"")
    df_final = df_final.append(df, ignore_index=True)
df_final
# Gives us the cumulative number of downloads over a period of 180 days
def get_cumulative_dataframe(df):
    
    data = df.groupby('category').get_group('with_mirrors').sort_values(
        'date').reset_index(drop='index').groupby(
        'date').agg({'downloads':'sum'}).reset_index()
    
    cum_sum = 0
    for idx, num in enumerate(data['downloads']):
        cum_sum+=num
        data.loc[idx, 'cumulative_downloads'] = cum_sum

    data['cumulative_downloads'] = data.cumulative_downloads.astype('int')
    data.drop(columns = 'downloads', inplace=True)
    
    return data

df_downloads = get_cumulative_dataframe(df_final)
df_downloads.head()
def create_linechart(df, package, date, value):
    # Get last value
    last_value = ""{:,.0f}"".format(df.loc[df.index[-1], value]).replace("","", "" "")
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date].to_list(),
            y=df[value].to_list(),
            mode=""lines+text"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""‚≠ê<b> Total number of downloads for naas libraries </b><br><span style='font-size: 13px;'> Total Downloads as of today: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        yaxis_title='No. of downloads',
        yaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_downloads, packages, ""date"", ""cumulative_downloads"")
# Save your dataframe in CSV
df_downloads.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10238,Naas - Remove Scheduler Outputs,"import glob
import os
import naas
# Schedule your notebook everyday at 9 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
def remove_scheduler_outputs():
    for scheduler_output in glob.glob('/home/ftp/.naas/home/ftp/**/*__output_*.ipynb', recursive=True):
        os.remove(scheduler_output)
        print(f'‚úÖ DELETED: {scheduler_output}')
remove_scheduler_outputs()"
10239,Naas - Domain demo,"

"
10240,Naas - Doc demo,"import naas

naas.doc()"
10241,Naas - Get number of downloads naas package,"try:
    import pypistats
except:
    !pip install -U pypistats --user
    !pip install --upgrade pypistats
    import pypistats
# from pprint import pprint
from datetime import datetime
import plotly.graph_objects as go
import naas
# Inputs
package = ""naas""

# Outputs
name_output = f""{package}_downloads""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df = pypistats.overall(package, total=False, format=""pandas"")
df.head()
# Gives us the cumulative number of downloads over a period of 180 days
def get_cumulative_dataframe(df):
    
    data = df.groupby('category').get_group('with_mirrors').sort_values(
        'date').reset_index(drop='index').groupby(
        'date').agg({'downloads':'sum'}).reset_index()
    
    cum_sum = 0
    for idx, num in enumerate(data['downloads']):
        cum_sum+=num
        data.loc[idx, 'cumulative_downloads'] = cum_sum

    data['cumulative_downloads'] = data.cumulative_downloads.astype('int')
    data.drop(columns = 'downloads', inplace=True)
    
    return data

df_downloads = get_cumulative_dataframe(df)
df_downloads.tail(5)
def create_linechart(df, package, date, value):
    # Get last value
    last_value = ""{:,.0f}"".format(df.loc[df.index[-1], value]).replace("","", "" "")
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date].to_list(),
            y=df[value].to_list(),
            mode=""lines+text"",
            line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""‚≠ê<b> Number of downloads for {package} </b><br><span style='font-size: 13px;'> Total Downloads as of today: {last_value}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        yaxis_title='No. of downloads',
        yaxis_title_font=dict(family=""Arial"", size=13, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_downloads, package, ""date"", ""cumulative_downloads"")
# Save your dataframe in CSV
df_downloads.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10242,Naas - Automate GitHub Auth,"!ssh-keygen -b 2048 -t rsa -f /home/ftp/.ssh/id_rsa -q -N """"
cat /home/ftp/.ssh/id_rsa.pub"
10243,Naas - Secret demo,"import naas
API_NAME = ""HUBSPOT""
API_KEY = ""123456789""
naas.secret.add(name=""API_NAME"", secret=""API_KEY"")"
10244,Naas - Dependency demo,"import naas 
PATH = ""PATH_OF_YOUR_DOCUMENT.png""
naas.dependency.add(""YOUR_DOCUMENT.png"")"
10245,Naas - Webhook demo,"import naas 
naas.webhook.add()
naas.webhook.delete()
import naas
# Parameters
params = {}
body = {
    ""Domain"": ""www.naas.ai"",
    ""Email"": ""jeremy.naas42@gmail.com"",
    ""Name"": ""Jeremy"",
    ""Site_ID"": ""jtci2pxwjczr"",
    ""Submitted_At"": ""March 20, 2021 16:55 UTC"",
}
headers = {
    ""host"": ""public.naas.ai"",
    ""x-request-id"": ""e6b994eb9a83b9cac794c0d9e57c1533"",
    ""x-real-ip"": ""10.0.87.40"",
    ""x-forwarded-for"": ""10.0.87.40"",
    ""x-forwarded-host"": ""public.naas.ai"",
    ""x-forwarded-port"": ""443"",
    ""x-forwarded-proto"": ""https"",
    ""x-scheme"": ""https"",
    ""accept"": ""application/json"",
    ""content-type"": ""application/json"",
    ""user-agent"": ""landen-webhook-agent"",
    ""accept-encoding"": ""gzip"",
    ""connection"": ""close"",
    ""content-length"": ""142"",
    ""accept-charset"": ""utf-8"",
}
naas.webhook.add()
naas.webhook.get_output()
EMAIL = body.get(""Email"")
EMAIL
import naas
email_to = [""jeremy.ravenel@cashstory.com"", EMAIL]
subject = ""Get Naas Webhook tutorial template""
content = f""Hey {EMAIL},<br>Here is the Naas webhook tutorial template : https://public.naas.ai/amVyZW15LTJFbmFhczQyLTQwZ21haWwtMkVjb20=/asset/7c9359cbc967afd01d8e45b68659b3b0db4179582561f6fab70f156c460a""
naas.notifications.send(email_to=email_to, subject=subject, html=content)
naas.assets.add()"
10246,Naas - Emailbuilder demo,"import naas_drivers
import naas
import pandas as pd
# List to emails address of the receiver(s)
email_to = [""""]

# Email sender : Can only take your email account or notification@naas.ai
email_from = """"

# Email subject
subject = ""My Object""
table = pd.DataFrame({
    ""Table Header 1"": [""Left element 1"", ""Left element 2"", ""Left element 3""],
    ""Table Header 2"": [""Right element 1"", ""Right element 2"", ""Right element 3""]
})

link = ""https://www.naas.ai/""

img = ""https://gblobscdn.gitbook.com/spaces%2F-MJ1rzHSMrn3m7xaPUs_%2Favatar-1602072063433.png?alt=media""

list_bullet = [""First element"",
               ""Second element"",
               ""Third element"",
               naas_drivers.emailbuilder.link(link, ""Fourth element""),
              ]

footer_icons = [{
    ""img_src"": img,
    ""href"": link
}]

email_content = {
    'element': naas_drivers.emailbuilder.title(""This is a title""),
    'heading': naas_drivers.emailbuilder.heading(""This is a heading""),
    'subheading': naas_drivers.emailbuilder.subheading(""This is a subheading""),
    'text': naas_drivers.emailbuilder.text(""This is a text""),
    'link': naas_drivers.emailbuilder.link(link, ""This is a link""),
    'button': naas_drivers.emailbuilder.button(link, ""This is a button""),
    'list': naas_drivers.emailbuilder.list(list_bullet),
    'table': naas_drivers.emailbuilder.table(table, header=True, border=True),
    'image': naas_drivers.emailbuilder.image(img),
    'footer': naas_drivers.emailbuilder.footer_company(networks=footer_icons, company=[""Company informations""], legal=[""Legal informations""])
}
content = naas_drivers.emailbuilder.generate(display='iframe',
                              **email_content)
naas.notification.send(email_to=email_to,
                       subject=subject,
                       html=content,
                       email_from=email_from)"
10247,Naas - Credits Get Balance,"from naas_drivers import naascredits
balance = naascredits.connect().get_balance()
balance"
10248,Naas - Notification demo,"import naas
email_to = ""jeremy@acme.com""
subject = ""üõéÔ∏è Naas Notification Test üö®""
content ='''<p>If i can see an image below this text...&nbsp;</p>
<p><img src=""https://specials-images.forbesimg.com/imageserve/5f1f37a40a5db2c8275972c0/960x0.jpg?fit=scale"" alt="""" width=""959"" height=""663"" /></p><br>
...it means everything goes well.'''
naas.notification.send(email_to, subject, content)"
10249,Naas - Reset Instance,"import naas
def reset_naas_instance():
    ! rm -rf ~/.clean || true
    ! mkdir ~/.clean || true
    ! mv ~/.* ~/.clean
    naas.update()
reset_naas_instance()"
10250,Naas - Get Transactions,"from naas_drivers import naascredits
import pandas as pd
page_size = ""100000""
df = pd.DataFrame(naascredits.connect().transactions.get(page_size=page_size))
df
df.to_csv('billing.csv')"
10251,Naas - NLP Examples,"from naas_drivers import nlp
nlp.get(""text-generation"", model=""gpt2"", tokenizer=""gpt2"")(""What is the most important thing in your life right now?"")
nlp.get(""summarization"", model=""t5-small"", tokenizer=""t5-small"")('''

There will be fewer and fewer jobs that a robot cannot do better. 
What to do about mass unemployment this is gonna be a massive social challenge and 
I think ultimately we will have to have some kind of universal basic income.

I think some kind of a universal basic income is going to be necessary 
now the output of goods and services will be extremely high 
so with automation they will they will come abundance there will be or almost everything will get very cheap.

The harder challenge much harder challenge is how do people then have meaning like a lot of people 
they find meaning from their employment so if you don't have if you're not needed if 
there's not a need for your labor how do you what's the meaning if you have meaning 
if you feel useless these are much that's a much harder problem to deal with. 

''')
nlp.get(""text-classification"", 
        model=""distilbert-base-uncased-finetuned-sst-2-english"",
        tokenizer=""distilbert-base-uncased-finetuned-sst-2-english"")('''

It was a weird concept. Why would I really need to generate a random paragraph? 
Could I actually learn something from doing so? 
All these questions were running through her head as she pressed the generate button. 
To her surprise, she found what she least expected to see.

''')
nlp.get(""fill-mask"",
        model=""distilroberta-base"",
        tokenizer=""distilroberta-base"")('''

It was a beautiful <mask>.

''')
nlp.get(""feature-extraction"", model=""distilbert-base-cased"", tokenizer=""distilbert-base-cased"")(""Life is a super cool thing"")
nlp.get(""token-classification"", model=""dslim/bert-base-NER"", tokenizer=""dslim/bert-base-NER"")('''

My name is Wolfgang and I live in Berlin

''')"
10252,Trello - Get board data,"import trello_connector
token = """"
key = """"
board_id = ""VCmIpC16""
export = ""xls""
df = trello_connector.main(key,token,board_id, export)"
10253,Plaid - Get transactions,"pip install plaid-python
import os
import plaid
import naas
import IPython.core.display
import uuid
import json
PLAID_CLIENT_ID = ""*************"" 
PLAID_SECRET = ""*************"" 
PLAID_ENV = 'sandbox'

PLAID_PRODUCTS = ['transactions']
PLAID_COUNTRY_CODES = ['FR']
start_transaction = ""2020-09-01""
end_transaction = ""2020-10-01""
client = plaid.Client(client_id=PLAID_CLIENT_ID,
                      secret=PLAID_SECRET,
                      environment=PLAID_ENV)
def create_link_token():
    response = client.LinkToken.create(
      {
        'user': {
          # This should correspond to a unique id for the current user.
          'client_user_id': 'user-id',
        },
        'client_name': ""Plaid Quickstart"",
        'products': PLAID_PRODUCTS,
        'country_codes': PLAID_COUNTRY_CODES,
        'language': ""en"",
        'redirect_uri': None,
      }
    )
    return response
token = create_link_token()
token
cb_url = naas.callback.add()
uid = uuid.uuid4().hex
iframe = """"""
<head>
  <script src=""https://cdn.plaid.com/link/v2/stable/link-initialize.js""></script>
</head>
<script>
const handler_{uid} = Plaid.create({
  token: '{GENERATED_LINK_TOKEN}',
  onSuccess: (public_token, metadata) => {
        const xhr = new XMLHttpRequest();
        xhr.open(""POST"", ""{CALLBACK_URL}"", true);
        xhr.setRequestHeader('Content-Type', 'application/json');
        xhr.send(JSON.stringify({
            public_token: public_token
        }));
  }
});
handler_{uid}.open();
</script>
""""""
iframe = iframe.replace('{uid}', uid)
iframe = iframe.replace('{CALLBACK_URL}', cb_url.get('url'))
iframe = iframe.replace('{GENERATED_LINK_TOKEN}', token.get('link_token'))
IPython.core.display.display(IPython.core.display.HTML(iframe))
cb_data = naas.callback.get(cb_url.get('uuid'))
cb_data = json.loads(cb_data)
public_token = cb_data.get(""public_token"")
public_token
exchange_response = client.Item.public_token.exchange(public_token)
access_token = exchange_response['access_token']
item_id = exchange_response['item_id']
response = client.Transactions.get(access_token,
                                   start_date=start_transaction,
                                   end_date=end_transaction)
transactions = response['transactions']

while len(transactions) < response['total_transactions']:
    response = client.Transactions.get(access_token,
                                       start_date=start,
                                       end_date=end,
                                       offset=len(transactions)
                                      )
    transactions.extend(response['transactions'])
transaction_df = pd.DataFrame.from_records(transactions)
transaction_df
transaction_df.to_csv('transactions.csv')"
10254,Plaid - Get accounts,"pip install plaid-python
import os
import plaid
import IPython.core.display
import uuid
import naas
import json
import pandas as pd
PLAID_CLIENT_ID = ""*************"" 
PLAID_SECRET = ""*************"" 
PLAID_ENV = 'sandbox'

PLAID_PRODUCTS = ['transactions']
PLAID_COUNTRY_CODES = ['FR']
start_transaction = ""2020-09-01""
end_transaction = ""2020-10-01""
client = plaid.Client(client_id=PLAID_CLIENT_ID,
                      secret=PLAID_SECRET,
                      environment=PLAID_ENV)
def create_link_token():
    response = client.LinkToken.create(
      {
        'user': {
          # This should correspond to a unique id for the current user.
          'client_user_id': 'user-id',
        },
        'client_name': ""Plaid Quickstart"",
        'products': PLAID_PRODUCTS,
        'country_codes': PLAID_COUNTRY_CODES,
        'language': ""en"",
        'redirect_uri': None,
      }
    )
    return response
token = create_link_token()
token
cb_url = naas.callback.add()
uid = uuid.uuid4().hex
iframe = """"""
<head>
  <script src=""https://cdn.plaid.com/link/v2/stable/link-initialize.js""></script>
</head>
<script>
const handler_{uid} = Plaid.create({
  token: '{GENERATED_LINK_TOKEN}',
  onSuccess: (public_token, metadata) => {
        const xhr = new XMLHttpRequest();
        xhr.open(""POST"", ""{CALLBACK_URL}"", true);
        xhr.setRequestHeader('Content-Type', 'application/json');
        xhr.send(JSON.stringify({
            public_token: public_token
        }));
  }
});
handler_{uid}.open();
</script>
""""""
iframe = iframe.replace('{uid}', uid)
iframe = iframe.replace('{CALLBACK_URL}', cb_url.get('url'))
iframe = iframe.replace('{GENERATED_LINK_TOKEN}', token.get('link_token'))
IPython.core.display.display(IPython.core.display.HTML(iframe))
cb_data = naas.callback.get(cb_url.get('uuid'))
cb_data = json.loads(cb_data)
public_token = cb_data.get(""public_token"")
public_token
exchange_response = client.Item.public_token.exchange(public_token)
access_token = exchange_response['access_token']
item_id = exchange_response['item_id']
response = client.Accounts.balance.get(access_token)
accounts = response['accounts']
accounts_df = pd.DataFrame.from_records(accounts)
accounts_df
accounts_df.to_csv('accounts.csv')"
10255,FAO - Consumer price indice,"import requests, zipfile, io
import matplotlib.pyplot as plt
import naas_drivers
import pandas as pd
import plotly.express as px
import csv
import codecs
import plotly.graph_objects as go
filename = ""ConsumerPriceIndices_E_All_Data_(Normalized).csv"" 
zip_file_url = ""http://fenixservices.fao.org/faostat/static/bulkdownloads/ConsumerPriceIndices_E_All_Data_(Normalized).zip""
r = requests.get(zip_file_url, stream=True)
z = zipfile.ZipFile(io.BytesIO(r.content))
z.extractall()

df = pd.read_csv(filename, encoding='latin-1')
df.to_csv(filename, index=False)
df = pd.read_csv(filename)

df.head()
df = df[df['Item Code'] == 23013]
df = df[df.Year == 2020]
dfmax10 = df.groupby(['Area', 'Year']).mean().reset_index().sort_values('Value', ascending=False).reset_index()
dfmin10 = df.groupby(['Area', 'Year']).mean().reset_index().sort_values('Value', ascending=True).reset_index()
dfmax10y = dfmax10['Area'].head(10).iloc[::-1]
dfmax10x = dfmax10['Value'].head(10).iloc[::-1]

fig = go.Figure(go.Bar(
            x=dfmax10x,
            y=dfmax10y,
            orientation='h'))

fig.update_xaxes(type=""log"")
fig.update_layout(title_text='Top 10 of the biggest evolution in 2020')

fig.show()
dfmin10y = dfmin10['Area'].head(10).iloc[::-1]
dfmin10x = dfmin10['Value'].head(10).iloc[::-1]

fig = go.Figure(go.Bar(
            x=dfmin10x,
            y=dfmin10y,
            orientation='h'))

fig.update_layout(title_text='Top 10 of the worst evolution in 2020')
fig.show()"
10256,INPI - Download PDF recap,"import urllib
# Input
SIREN = ""8779417XX""

# Output
PDF_OUTPUT = f""RECAPITULATIF_INPI_{SIREN}.pdf""
def download_pdf(siren, filepath):
    url = f""https://data.inpi.fr/export/companies?format=pdf&ids=[%22{siren}%22]""
    response = urllib.request.urlopen(url)    
    file = open(filepath, 'wb')
    file.write(response.read())
    file.close()
    print(""File saved:"", filepath) 
download_pdf(SIREN, PDF_OUTPUT)"
10257,spaCy - SpaCy Build a sentiment analysis model using Twitter,"%pip install --user requests pandas pyyaml datetime numpy datetime matplotlib wordcloud seaborn spacy
import os
import requests
import pandas as pd
import json
import ast
import yaml

import numpy as np
from datetime import datetime, date

import matplotlib.pyplot as plt
import matplotlib as mpl
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import re
import seaborn as sns
import string
import warnings
import random
import spacy
from spacy.training import Example
from spacy.pipeline.textcat import DEFAULT_SINGLE_TEXTCAT_MODEL
BEARER_TOKEN = ""...""
# Twitter query to fetch tweets. Learn how to build one here: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query
TWITTER_QUERY = '(Putin OR Lukashenka OR Russia OR ""Vladimir Putin"") -is:retweet lang:en'

# Number of tweets to fetch (max 100).
TWITTER_MAX_RESULTS = 100
def create_url(query, max_results=100):
    tweet_fields = ""tweet.fields=created_at,public_metrics,context_annotations,text,possibly_sensitive,geo""
    url = ""https://api.twitter.com/2/tweets/search/recent?max_results={}&query={}&{}"".format(max_results, query, tweet_fields)
    return url

def create_headers(bearer_token):
    headers = {""Authorization"": ""Bearer {}"".format(bearer_token)}
    return headers

def connect_to_endpoint(url, headers):
    response = requests.request(""GET"", url, headers=headers)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()
url= create_url(TWITTER_QUERY, TWITTER_MAX_RESULTS)
bearer_token = BEARER_TOKEN
headers = create_headers(bearer_token)
json_response = connect_to_endpoint(url, headers)
json_response
#Initialize Lists
id_data = []
date_data = [] 
rtwt_data = []
reply_data = []
text_data = []
def get_id():
    for data in json_response[""data""]:
        id_record = data[""id""]
        id_data.append(id_record)

def get_created_at():
    for data in json_response[""data""]:
        date_record = data[""created_at""]
        date_data.append(date_record)

def retweet_count():
    for retweets in json_response[""data""]:
        rtwt_count=retweets[""public_metrics""][""retweet_count""] 
        rtwt_data.append(rtwt_count)

def reply_count():
    for reply in json_response[""data""]:
        reply_count = reply[""public_metrics""][""reply_count""]
        reply_data.append(reply_count)

        
def get_text():
    for data in json_response[""data""]:
        text_record = data[""text""]
        text_data.append(text_record)
        
get_id() 
get_created_at() 
retweet_count()
reply_count() 
get_text()
def get_domain():
    d_id = []
    d_name = []
    d_desc = []
    d_twt_id = []
    
    for j in json_response[""data""]:
        for i in j:
            if i == ""context_annotations"":
                for d in j[""context_annotations""]:
                    d_id.append(d[""domain""][""id""])
                    d_name.append(d[""domain""][""name""])
                    d_desc.append(d[""domain""][""description""])
                    d_twt_id.append(j[""id""])
                    
    domain = {""id"": d_id, ""name"": d_name, ""desc"": d_desc,""tweet id"": d_twt_id}
    return domain


def get_entity():    
    e_id = []
    e_name = []
    e_desc = []
    e_twt_id = []

    for j in json_response[""data""]:
        for i in j:
            if i == ""context_annotations"":
                for d in j[""context_annotations""]:
                    e_id.append(d[""entity""][""id""])
                    e_name.append(d[""entity""][""name""])
                    e_twt_id.append(j[""id""])
                
    entity = {""id"": e_id, ""name"": e_name, ""desc"": e_desc,""tweet id"": e_twt_id}
    return entity
#inspect output
print(get_entity())
print(get_domain())
print(len(id_data), len(date_data), len(rtwt_data), len(reply_data), len(text_data))
#saved tweet fields doe not include entity or domain data
save_data = {""id"": id_data, ""date"": date_data, ""retweet count"": rtwt_data, ""reply count"": reply_data, ""text"": text_data}
df = pd.DataFrame(save_data)
df.to_csv(""pol_tweet_data.csv"")
df
def cleanup_text(file, rem_item):
    r = re.findall(rem_item, file)
    for i in r:
        file = re.sub(i, """", file)
    return file
#remove handles
df[""clean text""] = np.vectorize(cleanup_text)(df[""text""], ""@[\w]*"")
df[""clean text""].head()
len(df[""clean text""])
print(df[""clean text""])
#convert to tokens
nlp = None
try:
    nlp = spacy.load(""en_core_web_lg"") #here you can load a model that you'd previously trained
except:
    ! python -m spacy download en_core_web_lg
    nlp = spacy.load(""en_core_web_lg"")
    
text = str(df[""clean text""])
doc = nlp(text)
tokens_list = [] 
for token in doc:
    tokens_list.append(token)

tokens_list
#Review entities recognized
for ent in doc.ents:
    print(ent.text, ent.label_)
#Add a category for entities of interest, if needed. 
from spacy.matcher import PhraseMatcher 
matcher = PhraseMatcher(nlp.vocab) 

#define politicians as entities 
terms = [""Putin"", ""Zelensky""] 
patterns = [nlp.make_doc(term) for term in terms] 
matcher.add(""politiciansList"", None, *patterns) 

matches = matcher(doc) 

#this prints out the spans where the instances are found and the entity identified
for mid, start, end in matches: 
    print(start, end, doc[start:end])
config = {
    ""threshold"": 0.5,
    ""model"": DEFAULT_SINGLE_TEXTCAT_MODEL }

textcat = nlp.add_pipe(""textcat"", config=config) 
#create training data for your example consisting of examples of positive and negative sentiment
train_data = [(""Helping refugees. This is what kindness looks like."", {""cats"": {""POS"": True}}),
              (""In this time of uncertainty, we have a clear way forward: Help Ukraine defend itself. Support the Ukrainian people. Hold Russia accountable."", {""cats"": {""POS"": True}}),
              (""Priests demand head of Ukrainian Orthodox Church Moscow Patriarchate be brought to church tribunal for position on war."", {""cats"": {""POS"": True}}),
              (""Mayor of the most northern village in Ukraine Hremiach Hanna Havrylina was released after yesterday‚Äôs prisoners‚Äô swap."", {""cats"": {""POS"": True}}),
              (""Look at this female volunteer from Belarus fighting alongside Ukrainians."", {""cats"": {""POS"": True}}),
              (""Russian soldiers: They're animals... Humans don't behave like this. My parents told me about WW2 & the fascists didn't even do such things."", {""cats"": {""NEG"": True}}),
              (""All Russians are evil"", {""cats"": {""NEG"": True}}),
              (""The West is pushing Ukraine toward a conflict."", {""cats"": {""NEG"": True}}),
              (""Cowards"", {""cats"": {""NEG"": True}}),
              (""Russia‚Äôs deployment of combat forces is a mere repositioning of troops on its own territory."", {""cats"": {""NEG"": True}}),
              (""Ukraine and Ukrainian government officials are the aggressor in the Russia-Ukraine relationship."", {""cats"": {""NEG"": True}})] 
textcat.add_label(""POS"")
textcat.add_label(""NEG"")
    
train_examples = [Example.from_dict(nlp.make_doc(text), label) for text,label in train_data] 
textcat.initialize(lambda: train_examples, nlp=nlp)
#Define training example

epochs = 20

#Disable other pipe components & define training loop to incorporate statistical information

with nlp.select_pipes(enable=""textcat""):
    optimizer = nlp.resume_training() #Creates optimizer object
    for i in range(epochs):
        random.shuffle(train_data)
        for text, label in train_data:
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, label) 
            print(nlp.update([example], sgd=optimizer))
#enter an example tweet to test results
doc2 = nlp(""As Russia continues to commit horrific atrocities against the Ukrainian people, we must take additional steps to cut off"")

print(doc2.cats)
#enter another example
doc3 = nlp(""One of the captured Russian soldiers who was sent by Putin to ‚Äúdenazify‚Äù Ukraine"")
print(doc3.cats)
#process each row in clean text column
df[""nlp_proc""] = [nlp(i) for i in df[""clean text""]]
#save positive/negative predictions to cats column
df[""cats""] = [i.cats for i in df[""nlp_proc""]]
#assign value of 1 to positive classification, 0 to negative
sc_val = []

for i in df[""cats""]:
    if i[""POS""] >= .5:
        sc_val.append(1)
    else:
        sc_val.append(0)
#append classification score to dataframe
df[""score""] = sc_val
#check dataframe
df
#print out tweet id, text and score = to review results
for index, i in enumerate(df[""score""]):
    if i == 1:
        print(df[""id""][index], df[""clean text""][index], df[""score""][index])
#wordcloud
wordcloud = WordCloud(stopwords = STOPWORDS, collocations=True).generate(str(tokens_list))

plt.imshow(wordcloud, interpolation=""bilinear"")
plt.axis(""off"")
plt.show()
ax = df.score.value_counts().plot(kind=""bar"", colormap=""Paired"")
plt.show()
from pathlib import Path
output_dir=Path(""spaCy_models"")

def save_model(output_dir):
    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print(""Saved model to"", output_dir)
        
save_model(output_dir)
### To load trained, custom model on new data use:
#nlp = spacy.load(""spaCy_models"")"
10258,D-Tale - Visualize dataframe,"! pip install --user dtale
import pandas
import dtale
import dtale.app as dtale_app

dtale_app.JUPYTER_SERVER_PROXY = True
csv_path = ""https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv""
df = pandas.read_csv(csv_path)
d = dtale.show(df)
d
d._main_url"
10259,Qonto - Get statement barline,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = None
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
# Title of the graph, default = ""Evolution du {date_from} au {date_to}""
title = f""üíµ<b> Qonto - Suivi des encaissements / D√©caissements</b><br>""
# Name of line displayed in legend
line_name = ""Solde""
# Line color
line_color = ""#1ea1f1""
# Name of cash in bar displayed in legend
cashin_name = ""Encaissements""
# Cash in bar color
cashin_color = ""#47dd82""
# Name of cash out bar displayed in legend
cashout_name = ""D√©caissements""
# Cash out bar color
cashout_color = ""#ea484f""
barline = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.barline(date_from=date_from,
                                                                            date_to=date_to,
                                                                            title=title,
                                                                            line_name=line_name,
                                                                            line_color=line_color,
                                                                            cashin_name=cashin_name,
                                                                            cashin_color=cashin_color,
                                                                            cashout_name=cashout_name,
                                                                            cashout_color=cashout_color)
barline
"
10260,Qonto - Get statement,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = ""2022-01-01""
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
df_statement = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.get(date_from=date_from,
                                                                             date_to=date_to)
df_statement"
10261,Qonto - Get statement ranking by category,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = None
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
df_summary = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.summary(
    summary_type=""CATEGORY"",
    date_from=date_from,
    date_to=date_to
)
df_summary
"
10262,Qonto - Get organizations,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
df_organizations = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).organizations.get()
df_organizations"
10263,Qonto - Get cash position trend,"from naas_drivers import qonto
from datetime import datetime
import pandas as pd
import plotly.graph_objects as go
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = None
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
df_statement = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.get(
    to_group=[""IBAN"", ""DATE""],
    date_from=date_from,
    date_to=date_to
)
df_statement
def get_trend(df_statement,
              date_col_name,
              value_col_name):
    
    # Init dataframe
    df = df_statement.copy()
    
    # Format date
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    
    # Fill empty date
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    df = df.reset_index(drop=True)
    for _, row in df.iterrows():
        if _ > 0:
            iban = df.loc[df.index[_-1], ""IBAN""]
            n_1 = df.loc[df.index[_-1], value_col_name]
            n = df.loc[df.index[_], value_col_name]
            if n == 0:
                df.loc[_, value_col_name] = n_1
                df.loc[_, ""IBAN""] = iban
    return df

df_trend = get_trend(df_statement, ""DATE"", ""POSITION"")
df_trend.tail(10)
def create_linechart(df, date, value, var):    
    # Get last value
    df[""VALUE_D""] = df[value].map(""{:,.2f} ‚Ç¨"".format).str.replace("","", "" "")
    df[""VAR_D""] = df[var].map(""{:,.2f} ‚Ç¨"".format).str.replace("","", "" "")
    df.loc[df[var].astype(float) > 0, ""VAR_D""] = ""+"" + df[""VAR_D""]
    df[""TEXT""] = (""<b>Cash position as of "" + df[""DATE""].astype(str) + "" : </b>"" + 
                  df[""VALUE_D""] + ""<br>"" + 
                  df[""VAR_D""] + "" vs yesterday"")
    
    last_value = df.loc[df.index[-1], ""VALUE_D""]
    last_var = df.loc[df.index[-1], ""VAR_D""]
    
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[date],
            y=df[value],
            mode=""lines"",
            hoverinfo=""text"",
            text=df[""TEXT""],
            line=dict(color=""#6b5aed""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""üíµ<b> Qonto - Cash position trend</b><br><span style='font-size: 13px;'>Last position : {last_value} ({last_var} vs yesterday)</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title='Amount',
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""DATE"", ""POSITION"", ""AMOUNT"")
"
10264,Qonto - Releve de compte augmente,"from naas_drivers import qonto
from datetime import datetime, timedelta
import pandas as pd
import naas
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
import naas
QONTO_USER_ID = naas.secret.get(""QONTO_USER_ID"")
QONTO_SECRET_KEY = naas.secret.get(""QONTO_API_KEY"")
# Destinataire
EMAIL_TO = [""florent.ravenel1@gmail.com""]

# Objet de l'email
EMAIL_SUBJECT = f""üèõÔ∏è Qonto - Votre relev√© de compte augment√© du {datetime.now().strftime('%d/%m/%Y')}""
# Date de d√©but au format AAAA-MM-JJ
DATE_FROM = ""2021-01-01""

# Date de fin au format AAAA-MM-JJ (par defaut, c'est la date d'aujoud'hui qui est selectionn√©e)
DATE_TO = datetime.now().strftime(""%Y-%m-%d"")

# Nombre de jours de r√©cup√©ration des derni√®res transactions (doit √™tre un chiffre n√©gatif)
LAST_TRANSACTIONS = -7
naas.scheduler.add(cron=""0 8 * * 1"")
GRAPH_FILE = ""graph_account_statement.html""
GRAPH_IMG = ""graph_account_statement.jpeg""
TABLE_FILE = ""account_statement.xlsx""
# Colonne to consolidate (DATE already included), if empty return only DATE, AMOUNT, POSITION
to_group = [""TRANSACTION_ID"",
            ""LABEL"",
            ""OPERATION_TYPE""]

df_statement = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.get(to_group=to_group,
                                                                             date_from=DATE_FROM,
                                                                             date_to=DATE_TO)
df_statement
barline = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.barline(date_from=DATE_FROM,
                                                                            date_to=DATE_TO)
barline
cash_summary = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.summary(summary_type=""OPERATION_TYPE"",
                                                                                 language=""FR"",
                                                                                 date_from=DATE_FROM,
                                                                                 date_to=DATE_TO)
cash_summary
df_last = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.transactions(date_from=LAST_TRANSACTIONS,
                                                                                 date_to=DATE_TO)
df_last
current_position = round(df_statement['POSITION'].tolist()[-1], 2)
current_position
df_statement.to_excel(TABLE_FILE)
statement_link = naas.asset.add(TABLE_FILE)
# Image
barline.write_image(GRAPH_IMG)
graph_img = naas.asset.add(GRAPH_IMG)

# HTML
barline.write_html(GRAPH_FILE)
params = {""inline"": True}
graph_link = naas.asset.add(GRAPH_FILE, params=params)
email_content = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.email(DATE_FROM,
                                                                                DATE_TO,
                                                                                current_position,
                                                                                graph_img,
                                                                                graph_link,
                                                                                cash_summary,
                                                                                LAST_TRANSACTIONS,
                                                                                df_last,
                                                                                statement_link)
naas.notification.send(EMAIL_TO,
                       EMAIL_SUBJECT,
                       email_content)"
10265,Qonto - Get transactions,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = None
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
df_transactions = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).transactions.get(date_from=date_from,
                                                                                  date_to=date_to)
df_transactions"
10266,Qonto - Get positions,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
df_positions = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).positions.get()
df_positions"
10267,Qonto - Get statement summary by operation type,"from naas_drivers import qonto
QONTO_USER_ID = 'YOUR_USER_ID'
QONTO_SECRET_KEY = 'YOUR_SECRET_KEY'
# Date to start extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01""
date_from = None
# Date to end extraction, format: ""AAAA-MM-JJ"", example: ""2021-01-01"", default = now
date_to = None
df_summary = qonto.connect(QONTO_USER_ID, QONTO_SECRET_KEY).statements.summary(
    summary_type=""OPERATION_TYPE"",
    date_from=date_from,
    date_to=date_to
)
df_summary
"
10268,TikTok - Get user stats,"try:
    from TikTokAPI import TikTokAPI
except:
    !pip install --user PyTikTokAPI
    from TikTokAPI import TikTokAPI
import nest_asyncio
import pandas as pd
nest_asyncio.apply()
# Cookies
cookie = {
  ""s_v_web_id"": ""verify_l0ecjehXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",
  ""tt_webid"": ""1%7CaSy-x8YGNmB_l9qsXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""
}

# Username
username = ""tiktester04""
api = TikTokAPI(cookie=cookie)
def get_user_stats(username):
    #Get user details
    '''
    User detail fields : video count, follower count, following count, heart count
    '''

    user_obj = api.getUserByName(username)
    user_video_count = user_obj[""userInfo""][""stats""][""videoCount""]
    user_follower_count = user_obj[""userInfo""][""stats""][""followerCount""]
    user_following_count = user_obj[""userInfo""][""stats""][""followingCount""]
    user_heart_count = user_obj[""userInfo""][""stats""][""heartCount""]

    user_stats = {
        ""user_video_count"": user_video_count,
        ""user_follower_count"": user_follower_count,
        ""user_following_count"": user_following_count,
        ""user_heart_count"": user_heart_count

    }
    return user_stats
df_stats = get_user_stats(username)
df_stats"
10269,TikTok - Get videos stats,"try:
    from TikTokAPI import TikTokAPI
except:
    !pip install --user PyTikTokAPI
    from TikTokAPI import TikTokAPI
import nest_asyncio
import pandas as pd
nest_asyncio.apply()
# Cookies
cookie = {
  ""s_v_web_id"": ""verify_l0ecjehXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"",
  ""tt_webid"": ""1%7CaSy-x8YGNmB_l9qsXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""
}

# Username
username = ""tiktester04""

#setup count of videos to be retrieved ( preferred count by api is 30)
video_count = 30

#set hashtag to identify and extract videos
user_hastag = ""#teslatok""

#set application type. This is used to distinguish data for application in master data model
APP_TYPE = ""TikTok""
api = TikTokAPI(cookie=cookie)
def get_videos_by_user(username):
    #Get videos by userName
    '''
    Video ID

    '''
    userVideoIdList = []
    #userVideos = api.getVideosByUserName(user_name=username, count=video_count,cursor=0)
    userVideos = api.getVideosByHashTag(user_hastag,count=video_count)
    userVideoList = userVideos[""itemList""]

    for userVideo in userVideoList:
        userVideoIdList.append(userVideo[""video""][""id""])
    
    return userVideoIdList
userVideoIdList = get_videos_by_user(username)
userVideoIdList
def get_video_stats_by_id(userVideoIdList):
    video_stats = {}
    tiktokDF = pd.DataFrame()
    for videoId in userVideoIdList:
        video_stats[videoId] = """"
        videoByID = api.getVideoById(videoId)
        videoTitle = """"
        videoDesc = videoByID[""itemInfo""][""itemStruct""][""desc""]
        videoCreateTime = videoByID[""itemInfo""][""itemStruct""][""createTime""]
        videoLink = videoByID[""itemInfo""][""itemStruct""][""video""][""playAddr""]

        videoStats = videoByID[""itemInfo""][""itemStruct""][""stats""]
        videoLikeCount = videoStats[""diggCount""]
        videoCommentCount = videoStats[""commentCount""]
        videoPlayCount = videoStats[""playCount""]
        videoShareCount = videoStats[""shareCount""]

        videoAuthor = videoByID[""itemInfo""][""itemStruct""][""author""]
        videoAuthorName = videoAuthor[""uniqueId""]
        videoAuthorStats = videoByID[""itemInfo""][""itemStruct""][""authorStats""]
        videoAuthorFollowerCount = videoAuthorStats[""followerCount""]
        videoAuthorFollowingCount = videoAuthorStats[""followingCount""]
        videoAuthorTotalVideoCount = videoAuthorStats[""videoCount""]
        videoAuthorTotalHeartCount = videoAuthorStats[""heartCount""]

        video_stats[videoId] = video_stats_obj
        data=[[videoId,videoCreateTime,videoAuthorName,videoTitle,videoDesc,videoLink,videoPlayCount,videoCommentCount,videoLikeCount,videoShareCount,APP_TYPE]]
        df = pd.DataFrame(data,columns=[""ACTIVITY_ID"",""PUBLISHED_DATE"",""AUTHOR_NAME"",""TITLE"",""TEXT"",""POST_URL"",""VIEWS"",""COMMENTS"",""LIKES"",""SHARES"",""APPLICATION_TYPE""
        ])
        tiktokDF = tiktokDF.append(df)
    return tiktokDF
df_videos = get_video_stats_by_id(userVideoIdList)
df_videos"
10270,Bazimo - Get export Baux,"from naas_drivers import bazimo
import naas
# Credentials
bazimo_email = ""bazimo_email""
bazimo_pwd = ""bazimo_pwd""

# Export name
bazimo_export = ""Baux""

# Export output
csv_output = f""Export_{bazimo_export}.csv""
# Get your data updated everyday 15 min from 9 to 20 during week days.
naas.scheduler.add(cron=""*/15 9-20 * * 1-5"")

#-> Uncomment this line and run the cell to delete your scheduler
# naas.scheduler.delete()
df_bazimo = bazimo.connect(bazimo_email, bazimo_pwd).exports.get(bazimo_export)
df_bazimo
df_bazimo.to_csv(csv_output)
naas.asset.add(csv_output)

#-> Uncomment this line and run the cell to delete your asset
# naas.asset.delete(csv_output)"
10271,Bazimo - Get export Factures,"from naas_drivers import bazimo
import naas
# Credentials
bazimo_email = ""bazimo_email""
bazimo_pwd = ""bazimo_pwd""

# Export name
bazimo_export = ""Factures""

# Export output
csv_output = f""Export_{bazimo_export}.csv""
# Get your data updated everyday 15 min from 9 to 20 during week days.
naas.scheduler.add(cron=""*/15 9-20 * * 1-5"")

#-> Uncomment this line and run the cell to delete your scheduler
# naas.scheduler.delete()
df_bazimo = bazimo.connect(bazimo_email, bazimo_pwd).exports.get(bazimo_export)
df_bazimo
df_bazimo.to_csv(csv_output)
naas.asset.add(csv_output)

#-> Uncomment this line and run the cell to delete your asset
# naas.asset.delete(csv_output)"
10272,Bazimo - Get export Lots,"from naas_drivers import bazimo
import naas
# Credentials
bazimo_email = ""bazimo_email""
bazimo_pwd = ""bazimo_pwd""

# Export name
bazimo_export = ""Lots""

# Export output
csv_output = f""Export_{bazimo_export}.csv""
# Get your data updated everyday 15 min from 9 to 20 during week days.
naas.scheduler.add(cron=""*/15 9-20 * * 1-5"")

#-> Uncomment this line and run the cell to delete your scheduler
# naas.scheduler.delete()
df_bazimo = bazimo.connect(bazimo_email, bazimo_pwd).exports.get(bazimo_export)
df_bazimo
df_bazimo.to_csv(csv_output)
naas.asset.add(csv_output)

#-> Uncomment this line and run the cell to delete your asset
# naas.asset.delete(csv_output)"
10273,Bazimo - Get export Actifs,"from naas_drivers import bazimo
import naas
# Credentials
bazimo_email = ""bazimo_email""
bazimo_pwd = ""bazimo_pwd""

# Export name
bazimo_export = ""Actifs""

# Export output
csv_output = f""Export_{bazimo_export}.csv""
# Get your data updated everyday 15 min from 9 to 20 during week days.
naas.scheduler.add(cron=""*/15 9-20 * * 1-5"")

#-> Uncomment this line and run the cell to delete your scheduler
# naas.scheduler.delete()
df_bazimo = bazimo.connect(bazimo_email, bazimo_pwd).exports.get(bazimo_export)
df_bazimo
df_bazimo.to_csv(csv_output)
naas.asset.add(csv_output)

#-> Uncomment this line and run the cell to delete your asset
# naas.asset.delete(csv_output)"
10274,Bazimo - Get export Locataires,"from naas_drivers import bazimo
import naas
# Credentials
bazimo_email = ""bazimo_email""
bazimo_pwd = ""bazimo_pwd""

# Export name
bazimo_export = ""Locataires""

# Export output
csv_output = f""Export_{bazimo_export}.csv""
# Get your data updated everyday 15 min from 9 to 20 during week days.
naas.scheduler.add(cron=""*/15 9-20 * * 1-5"")

#-> Uncomment this line and run the cell to delete your scheduler
# naas.scheduler.delete()
df_bazimo = bazimo.connect(bazimo_email, bazimo_pwd).exports.get(bazimo_export)
df_bazimo
df_bazimo.to_csv(csv_output)
naas.asset.add(csv_output)

#-> Uncomment this line and run the cell to delete your asset
# naas.asset.delete(csv_output)"
10275,Integromat - Trigger workflow,"from naas_drivers import integromat
URL = ""https://hook.integromat.com/7edtlwmn8foer0r9i9ainjvsz3vxmwos""

DATA = {""first_name"": ""Bryan"",
        ""last_name"":""Helmig"", ""age"": 27 }
result = integromat.connect(URL).send(DATA)
result"
10276,Canny - Create,"import requests
import json
import pandas as pd
canny_api = ""CANNY_API_KEY""                          # api key of canny
post_title = ""Post title""                            # Enter post title                    
post_body = ""Post body using canny api""              # Enter post body
api_key = {
""apiKey"":canny_api          
}
limit = {
""limit"":""100""                          
}
response = requests.get(""https://canny.io/api/v1/boards/list"")
response = requests.post(""https://canny.io/api/v1/boards/list"", api_key)
post_details = response.json()
db = post_details['boards']
df = pd.DataFrame(columns = db[0].keys()) 
for i in range(len(db)):
    df = df.append(db[i], ignore_index=True)
df = df[['name','id']]
board_list = df.rename(columns={'name': 'BOARD_NAME', 'id': 'BOARD_ID'})
board_list
board_name = ""Requests""      #Enter board name
for i in range(len(board_list)):
    if board_list['BOARD_NAME'][i] == board_name:
        board_id = board_list['BOARD_ID'][i]
board_id
board_id = {
""boardID"":board_id                          
}
response = requests.get(""https://canny.io/api/v1/posts/list"")
data = {**api_key, **board_id, **limit}
response = requests.post(""https://canny.io/api/v1/posts/list"", data)
post_details = response.json()
# post_details['posts']
author_list = pd.DataFrame()
for i in range(len(post_details['posts'])):
    author_list = author_list.append(post_details['posts'][i]['author'], ignore_index=True)
author_list.drop_duplicates(subset =""email"", keep = False, inplace = True)
author_list = author_list[['name','id']]
author_list = author_list.rename(columns={'name': 'AUTHOR_NAME', 'id': 'AUTHOR_ID'})
author_list
author_name = ""Sanjay Sabu""  #Enter author name
for i in author_list['AUTHOR_NAME'].index:
    if author_list['AUTHOR_NAME'][i] == author_name:
        author_id = author_list['AUTHOR_ID'][i]
author_id = {
""authorID"":author_id                          
}
post_title = {
""title"":post_title                                    
}
post_body = {
""details"":post_body                    
}
data = {**api_key, **author_id, **board_id, **post_body, **post_title}
response = requests.post(""https://canny.io/api/v1/posts/create"", data)"
10277,Canny - Github issue update,"pip install PyGithub
import requests
import json
from github import Github
# For Github 
gihub_personal_token = ""YOUR_GITHUB_TOKEN""                        # Settings/Developer settings/Personal access tokens
github_repo = ""optimusprime2021/api-tester""                       # Github repository name

# For Canny
canny_post_url = ""https://canny.io/api/v1/posts/list""             # Canny post url
canny_apikey = ""CANNY_API_KEY""                                    # Canny api key
response = requests.get(canny_post_url)
data = {""apiKey"":canny_apikey,""id"":"""",""limit"":""100""}
response = requests.post(canny_post_url,data)
post_details = response.json()
if response.status_code == 200:
    print(""Successfully connected to Canny"")
elif response.status_code == 404:
    print(""Couldn't connect to Canny, Please check the credentials"")
    exit()
import pandas as pd
pd.set_option('mode.chained_assignment', None)
dd = post_details['posts']
df = pd.DataFrame(columns = dd[0].keys()) 
for i in range(len(dd)):
    df = df.append(dd[i], ignore_index=True)
# df

board = []
category = []
tags = []
for i in range(len(df)):
    board.append(df['board'][i]['name'])
    if not df['category'][i]:
        category.append('Not assigned')
    else:
        category.append(df['category'][i]['name'])    
    if not df['tags'][i]:
        tags.append('Not assigned')
    else:
        tags.append(df['tags'][i][0]['name'])
        
        
df = df[['title','status','details','url']]
df['board'] = board
df['category'] = category
df['tags'] = tags
df = df[(df[""tags""] == ""Awesome-notebooks"")]          # tag name
df
## add url to dataframe
issues = []
g = Github(gihub_personal_token)
repo = g.get_repo(github_repo)
open_issues = repo.get_issues(state='open')
for issue in open_issues:
    issues.append(issue.title)
repo = g.get_repo(github_repo)
for i in df.index:
    if df['title'][i] not in issues:
        repo.create_issue(title=df['title'][i], body=df['details'][i]+""\n canny url: ""+df['url'][i])
# repo = g.get_repo(github_repo)
# open_issues = repo.get_issues(state='open')
# for issue in open_issues:
#     issue.edit(state='closed')"
10278,Canny - Read,"import requests
import json
import pandas as pd
canny_api = ""CANNY_API_KEY""   # api key of canny
class canny:
    def __init__(self,api_key):
        self.api_key = api_key

    def read(self):
        canny_api = self.api_key
        response = requests.get(""https://canny.io/api/v1/posts/list"")
        api_key = {
        ""apiKey"":canny_api          
        }
        board_id = {
        ""id"":""""                          
        }
        limit = {
        ""limit"":""100""                          
        }
        data = {**api_key, **board_id, **limit}
        response = requests.post(""https://canny.io/api/v1/posts/list"",data)
        post_details = response.json()
        pd.set_option('mode.chained_assignment', None)
        dd = post_details['posts']
        df = pd.DataFrame(columns = dd[0].keys()) 
        for i in range(len(dd)):
            df = df.append(dd[i], ignore_index=True)
        df = df.rename(columns={'details': 'POST_DETAIL', 'status': 'STATUS', 'title': 'POST_NAME','board': 'BOARD','category': 'CATEGORY','id': 'BOARD_ID'})        
        board = []
        category = []
        tags = []
        eta = []
        created = []
        for i in range(len(df)):
            board.append(df['BOARD'][i]['name'])
            created.append(df['BOARD'][i]['created'])
            if not df['CATEGORY'][i]:
                category.append('Not assigned')
            else:
                category.append(df['CATEGORY'][i]['name'])    
            if not df['tags'][i]:
                tags.append('Not assigned')
            else:
                tags.append(df['tags'][i][0]['name'])
            if not df['eta'][i]:
                eta.append('Not assigned')
            else:
                eta.append(df['eta'][i])  
        df1 = df[['POST_NAME','POST_DETAIL','STATUS','BOARD_ID']]
        df1['BOARD'] = board
        df1['CREATED'] = created
        df1['ETA'] = eta
        df1['CATEGORY'] = category
        df1['TAGS'] = tags
        return df1
canny(canny_api).read()
canny(canny_api).read().to_csv('naas_canny.csv') "
10279,OpenWeatherMap - Get City Weather,"import requests
OPENWEATHER_KEY = '**********'  # get your key from here https://home.openweathermap.org/api_keys (it takes couples of minutes)
CITY = ""Paris""
def get_weather_info(city):
    url = f""http://api.openweathermap.org/data/2.5/weather?q={city}&appid={OPENWEATHER_KEY}""
    response = requests.get(url)
    return response.json()

def format_weather_data(data):
    return {
        ""temp"": f'{round(int(data[""main""][""temp""]) - 273.15, 1)}¬∞',
        ""city"": data[""name""],
    }
    
def run(city):
    data = get_weather_info(city)
    return format_weather_data(data)
run(CITY)"
10280,OpenWeatherMap - Send daily email with predictions,"import requests
import markdown2
import time
import pandas as pd
import naas
from naas_drivers import plotly, prediction
OPENWEATHER_KEY = '***************'  # get your key from here https://home.openweathermap.org/api_keys (it takes couples of minutes)
city = 'rouen'
country_code = 'fr' # if you don't want to specify a country code, let ''
# Output paths image and html
output_image = f'{city}.png'
output_html = f'{city}.html'
email_to = [""template@naas.ai""]
email_from = None
subject = f'{city} predictions as of today'
# naas.scheduler.add(cron='0 8 * * *')
# naas.scheduler.delete()
%%writefile message.md
Hey

The *CITY* temperature on the last 5 days

In +2 days, basic ML models predict the following temperature: 

- *linear*: LINEAR

    
<img href=link_html target=""_blank"" src=link_image style=""width:640px; height:360px;"" /><br>
[Open dynamic chart](link_html)<br>

    
Have a nice day.
<br>

PS: You can [send the email again](link_webhook) if you need a fresh update.<br>
<div><strong>Full Name</strong></div>
<div>Open source lover | <a href=""http://www.naas.ai/"" target=""_blank"">Naas</a></div>
naas.dependency.add(""message.md"")
def get_geoloc(city: str, country_code: str = ''):
    """""" Get the geoloc of a city, country
    
    :param city: name of the city
    :type city: str
    :param country_code: Please use ISO 3166 country codes, default to ''
    :type country_code: str
    """"""
    url = f'http://api.openweathermap.org/geo/1.0/direct?q={city},,{country_code}&appid={OPENWEATHER_KEY}'
    return requests.get(url).json()

def get_lat_lon(city: str, country_code: str = ''):
    """""" Get the geoloc of a city, country
    
    :param city: name of the city
    :type city: str
    :param country_code: Please use ISO 3166 country codes, default to ''
    :type country_code: str
    """"""
    geoloc = get_geoloc(city, country_code)
    
    if len(geoloc) == 0:
        return None, None
    
    return geoloc[0]['lat'], geoloc[0]['lon']

# get_lat_lon('paris')
# get_lat_lon('paris', 'us')
def get_historical_weather(city: str, country_code: str = '', nbr_days_before_now: int = 0):
    """"""Get historical weather data. For free API, maximum history is 5 days before now
    
    :param city: name of the city
    :type city: str
    :param country_code: Please use ISO 3166 country codes, default to ''
    :type country_code: str 
    :param nbr_hours_before_now: number of hour before now
    """"""
    unix_dt = int(time.time() - 60 * 60 * 24 * nbr_days_before_now)
    lat, lon = get_lat_lon(city, country_code)
    if lat is None:
        return None
    url = f'https://api.openweathermap.org/data/2.5/onecall/timemachine?lat={lat}&lon={lon}&dt={unix_dt}&appid={OPENWEATHER_KEY}&units=metric'
    return requests.get(url).json()

def weather_data_to_df(city: str, country_code: str = '', nbr_days_before_now: int = 0) -> pd.DataFrame:
    data = get_historical_weather(city, country_code, nbr_days_before_now) 
    df = pd.DataFrame(data['hourly'])
    df['date_time'] = pd.to_datetime(df['dt'], unit='s')
    df['city'] = city
    df['country_code'] = country_code
    
    df_explode_weather = pd.concat([df.drop(['weather', 'dt'], axis=1), df['weather'].str[0].apply(pd.Series)], axis=1)
    # df_explode_weather.set_index('date_time', inplace=True)
    return df_explode_weather
df_histo_weather = pd.concat([weather_data_to_df(city, country_code, _) for _ in range(6)], ignore_index=True)
df_histo_weather = df_histo_weather.sort_values(by='date_time').reset_index(drop=True).rename(columns={""date_time"": ""Date""})
df_histo_weather
df_predict = prediction.get(dataset=df_histo_weather,
                            date_column='Date',
                            column=""temp"",
                            data_points=5,
                            prediction_type=""all"")

df_predict
chart = plotly.linechart(df_predict,
                         x='Date',
                         y=['temp', 'ARIMA', ""LINEAR"", ""SVR"", ""COMPOUND""],
                         showlegend=True,
                         title=f'Temp in {city} last 5 days')
chart.write_html(output_html)
chart.write_image(output_image, width=1200)
link_image = naas.asset.add(output_image)
link_html = naas.asset.add(output_html, {'inline': True})
link_webhook = naas.webhook.add()
markdown_file = ""message.md""
content = open(markdown_file, ""r"").read()
md = markdown2.markdown(content)
md
post = md.replace(""DATANOW"", str(DATANOW))
post = post.replace(""CITY"", str(city))
post = post.replace(""LINEAR"", str(LINEAR))
post = post.replace(""link_image"", str(link_image))
post = post.replace(""link_html"", str(link_html))
post = post.replace(""link_webhook"", str(link_webhook))
post
content = post
naas.notification.send(email_to=email_to,
                       subject=subject,
                       html=content,
                       files=files,
                       email_from=email_from)"
10281,Instagram - Post image and caption,"try:
    from instabot import Bot
except:
    pip install instabot --user
    from instabot import Bot
import naas
# Credentials
username = ""USERNAME""
password = ""PASSWORD""

# Instragram outputs
image_path = ""demo.jpg""
caption = ""Naas is doing this.""
bot = Bot()
bot.login(username=username,  
          password=password) 
bot.upload_photo(image_path,
                 caption=caption) "
10282,Instagram - Get stats from posts,"import requests
import json
import datetime
import pandas as pd
access_token = """"
client_id = """"
client_secret = """"
page_id = """"
instagram_account_id = """"
ig_username = ""naaslife""
params = dict()
params['access_token'] = access_token        
params['client_id'] = client_id
params['client_secret'] = client_secret
params['graph_domain'] = 'https://graph.facebook.com'
params['graph_version'] = 'v13.0'
params['endpoint_base'] = params['graph_domain'] + '/' + params['graph_version'] + '/'
params['page_id'] = page_id           
params['instagram_account_id'] = instagram_account_id
params['ig_username'] = ig_username
# Define Endpoint Parameters
endpointParams = dict()
endpointParams['input_token'] = params['access_token']
endpointParams['access_token'] = params['access_token']

# Define URL
url = params['graph_domain'] + '/debug_token'
# Requests Data
try:
    data = requests.get(url, endpointParams)
    access_token_data = json.loads(data.content)
    
except Exception as err:
    print(err)

print(""Token Expires: "", datetime.datetime.fromtimestamp(
    access_token_data['data']['expires_at']))
# Define URL
url = params['endpoint_base'] + params['instagram_account_id'] + '/media'

# Define Endpoint Parameters
endpointParams = dict()
endpointParams['fields'] = 'id,caption,media_type,media_url,permalink,thumbnail_url,timestamp,username,like_count,comments_count'
endpointParams['access_token'] = params['access_token']

# Requests Data
data = requests.get(url, endpointParams)
basic_insight = json.loads(data.content)
df = pd.DataFrame(basic_insight['data'])
df.columns = ['id', 'Caption', 'Media_Type', 'Media_URL',
              'Permalink', 'Timestamp', 'Username', 'Likes', 'Comments']
df.head()"
10283,Jupyter - Get server uptime,"from naas_drivers import jupyter
from os import environ
# Jupyter token stored in your env
JUPYTER_TOKEN = environ.get('JUPYTERHUB_API_TOKEN')

# Username => email address linked to your jupyter account
username = ""hello@naas.ai""
data = jupyter.connect(JUPYTER_TOKEN).get_server_uptime(username)
data"
10284,Jupyter - Restart server,"from naas_drivers import jupyter
from os import environ
# Jupyter token stored in your env
JUPYTER_TOKEN = environ.get('JUPYTERHUB_API_TOKEN')

# Username => email address linked to your jupyter account
username = ""wsr@naas.ai""
jupyter.connect(JUPYTER_TOKEN).restart_user(username)
"
10285,Jupyter - Get user information,"from naas_drivers import jupyter
from os import environ
# Jupyter token
JUPYTER_TOKEN = environ.get('JUPYTERHUB_API_TOKEN')
data = jupyter.connect(JUPYTER_TOKEN).get_me()
data"
10286,Jupyter - Get user terminal,"from naas_drivers import jupyter
from os import environ
# Jupyter token stored in your env
JUPYTER_TOKEN = environ.get('JUPYTERHUB_API_TOKEN')

# Username => email address linked to your jupyter account
username = ""hello@naas.ai""
data = jupyter.connect(JUPYTER_TOKEN).get_user_terminal(username)
data"
10287,Jupyter - Get user session,"from naas_drivers import jupyter
from os import environ
# Jupyter token stored in your env
JUPYTER_TOKEN = environ.get('JUPYTERHUB_API_TOKEN')

# Username => email address linked to your jupyter account
username = ""hello@naas.ai""
data = jupyter.connect(JUPYTER_TOKEN).get_user_session(username)
data"
10288,Elasticsearch - Connect to server,"from elasticsearchconnector import ElasticsearchConnector
instance = ElasticsearchConnector(""sample_credentials.json"")
# parameters = {'index':'< Name of the index >','type':' < Document name > '}
parameters = {'index':'students','type':'engineering'}
# data = { < Key value pairs > }
data = {""Name"": ""Poul"", ""Age"":20, ""address"": ""New york""}
result = instance.save_data(parameters,data)
# parameters = {'index':'< Name of the index >','type':' < Document name > '}
parameters = {'index':'students','type':'engineering'}
# Single search
q1 = {""query"": {""match"": {'Name':'Poul'}}}
result = instance.search_data(parameters,[q1],search_type='search')
print(result)
# Multiple search
q1 = {""query"": {""match"": {'Name':'Poul'}}}
q2 = {""query"": {""match"": {'Age':27}}}
result = instance.search_data(parameters,[q1,q2],search_type='msearch')
print(result)"
10289,Gmail - Read mailbox,"from naas_drivers import email
username = ""*********@gmail.com""
to = ""*********@gmail.com""
password = ""*********""
smtp_server = ""imap.gmail.com""
box = ""INBOX""
emails = email.connect(username, 
        password, 
        username, 
        smtp_server)
df = emails.get(criteria=""unseen"")
df
uid_list = df['uid'].tolist() 
uid_list
%%time
uid = uid_list
flag = ""DELETED""
# possible value for flag:
# flag = 'SEEN'
# flag = 'ANSWERED'
# flag = 'FLAGGED'
# flag = 'DELETED'
# flag = 'DRAFT'
# flag = 'RECENT'
attachments = emails.set_flag(uid, flag, True)"
10290,Gmail - Send emails from Gsheet classic,"import naas_drivers
from naas_drivers import gsheet
from naas_drivers import html
spreadsheet_id = ""1swGTMX6d_N8-AVRueBEd8C0J6OlvO218iDSVMootWZk""
data = naas_drivers.gsheet.connect(spreadsheet_id).get(sheet_name=""Sheet1"")
your_email = ""jeremy.ravenel@cashstory.com""
firstname_list = data['FIRST NAME']
email_list = data['EMAIL']
import naas
url_image = naas.assets.add(""2020.gif"")
email_content = naas_drivers.html.generate( 
        display='iframe',
        title='üéÖ Merry Christmas',
        heading= '& Happy new year {first_name} üçæ',
        image = f""{url_image}"",
        text_1= ""Even if 2020 has been extremely difficult year, let's make 2021 better!"",
        text_2= ""Keep smiling,"",
        text_3= ""Keep laughing,"",
        text_4= ""Spread love ‚ù§Ô∏è"",
        
)
for i in range(len(data)):
    subject = ""Merry Christmas & spread love for 2021 ‚ù§Ô∏è""
    content = email_content.replace(""{first_name}"",firstname_list[i])
    naas.notifications.send(email_to=email_list[i], subject=subject, html=content, email_from=your_email)"
10291,Gmail - Schedule mailbox cleaning,"import naas
from naas_drivers import email
import pandas as pd
import numpy as np
import plotly.express as px
username = ""naas.sanjay22@gmail.com""
password = ""atsuwkylwfhucugw""
smtp_server = ""imap.gmail.com""
box = ""INBOX""
naas.scheduler.add(recurrence=""0 9 * * *"") # Scheduler set for 9 am
emails = naas_drivers.email.connect(username, 
        password, 
        username, 
        smtp_server)
dataframe = emails.get(criteria=""seen"")
dataframe
sender_name = []
sender_email = []
for df in dataframe[""from""]:
    sender_name.append(df['name'])
    sender_email.append(df['email'])
result = pd.DataFrame(columns = ['SENDER_NAME','SENDER_EMAIL','COUNT','PERCENTAGE'])
name_unique = np.unique(sender_name)
email_unique = np.unique(sender_email)
total_email = len(emails.get(criteria=""seen"")) + len(emails.get(criteria=""unseen""))
c = 0
for i in np.unique(sender_name):
    new_row = {'SENDER_NAME':i,'SENDER_EMAIL':sender_email[c],'COUNT':sender_name.count(i),'PERCENTAGE':round(((sender_name.count(i))/total_email)*100)}
    result = result.append(new_row, ignore_index=True)
    c+=1
result
fig = px.bar(x=result['COUNT'], y=result['SENDER_NAME'], orientation='h')
fig.show()
d_email = ""notifications@naas.ai""  # email id to be deleted
data_from = dataframe['from']
data_uid = dataframe['uid']
uid = []
for i in range(len(dataframe)):
    if data_from[i]['email'] == d_email:
        uid.append(data_uid[i])
print(uid)
for i in uid:
    attachments = emails.set_flag(i, ""DELETED"", True)
dataframe = emails.get(criteria=""seen"")
dataframe"
10292,Gmail - Send emails from Gsheet specific,"import naas
from naas_drivers import gsheet
from naas_drivers import html
spreadsheet_id = ""1_VAF0kLPfnZxjA7HsF4F3YdZLi_V_wmZh0nHUP6DWPg""
data = gsheet.connect(spreadsheet_id).get(sheet_name=""Sheet1"")
your_email = ""jeremy.ravenel@cashstory.com""
firstname_list = data['FIRST NAME']
email_list = data['EMAIL']
specific_message_list = data['SPECIFIC MESSAGE']
url_image = naas.assets.add(""2020.gif"")
email_content = html.generate( 
        display='iframe',
        title='üéÖ Merry Christmas',
        heading= '& Happy new year {first_name} üçæ',
        image = f""{url_image}"",
        text_1= '{specific message}',
        text_2= ""Even if 2020 has been extremely difficult year, let's make 2021 better!"",
        text_3= ""Keep smiling,"",
        text_4= ""Keep laughing,"",
        text_5= ""Spread love ‚ù§Ô∏è"",
        
)
for i in range(len(data)):
    subject = ""Merry Christmas & spread love for 2021 ‚ù§Ô∏è""
    content = email_content.replace(""{first_name}"",firstname_list[i]).replace(""{specific message}"",specific_message_list[i])
    naas.notifications.send(email_to=email_list[i], subject=subject, html=content, email_from=your_email)"
10293,Gmail - Automate response from keywords in mailbox,"import naas
from naas_drivers import email
from re import search
username = ""**********@gmail.com""
to = ""**********@gmail.com""
password = ""**********""
smtp_server = ""imap.gmail.com""
box = ""INBOX""
emails = email.connect(username, 
        password, 
        username, 
        smtp_server)
dataframe = emails.get(criteria=""ALL"")
df
for df in dataframe[""text""]:
    text = df.lower()
    if search(""sales report"", text): 
        email_to = ""naas.sanjay22@gmail.com""
        subject = ""Sales Report""
        content = ""Hi \n,Here I am attaching the sales report as per your request\n.With Regards\n,NAAS Team""
        files = [""Excel-Sales_Feb2020.csv""]
        naas.notifications.send(email_to=email_to, subject=subject, html=content, files=files)"
10294,Gmail - Clean mailbox,"import naas
from naas_drivers import email
import pandas as pd
import numpy as np
import plotly.express as px
username = ""**********@gmail.com""
password = ""**********""
smtp_server = ""imap.gmail.com""
box = ""INBOX""
emails = naas_drivers.email.connect(username, 
        password, 
        username, 
        smtp_server)
dataframe = emails.get(criteria=""ALL"")
dataframe
sender_name = []
sender_email = []
for df in dataframe[""from""]:
    sender_name.append(df['name'])
    sender_email.append(df['email'])
result = pd.DataFrame(columns = ['SENDER_NAME','SENDER_EMAIL','COUNT','PERCENTAGE'])
indexes = np.unique(sender_name, return_index=True)[1]
[sender_name[index] for index in sorted(indexes)]

indexes = np.unique(sender_email, return_index=True)[1]
[sender_email[index] for index in sorted(indexes)]
total_email = len(emails.get(criteria=""ALL""))
c = 0
for i in sender_email:
    new_row = {'SENDER_NAME':sender_name[c],'SENDER_EMAIL':i,'COUNT':sender_email.count(i),'PERCENTAGE':round(((sender_email.count(i))/total_email)*100)}
    result = result.append(new_row, ignore_index=True)
    c+=1
result = result.drop_duplicates()
result.sort_values(by=['COUNT'], inplace=True, ascending=False)
result
fig = px.bar(x=result['COUNT'], y=result['SENDER_EMAIL'], orientation='h')
fig.show()
%%time
uid = [21]   #uid of mails to be deleted
uid = map(str, uid)  
flag = ""DELETED""
for i in uid:
    attachments = emails.set_flag(i, flag, True)
d_email = ""notifications@naas.ai""  # email id to be deleted
data_from = dataframe['from']
data_uid = dataframe['uid']
uid = []
for i in range(len(dataframe)):
    if data_from[i]['email'] == d_email:
        uid.append(data_uid[i])
for i in uid:
    attachments = emails.set_flag(i, ""DELETED"", True)
dataframe = emails.get(criteria=""ALL"")
dataframe"
10295,Thinkific - Get users,"from naas_drivers import thinkific
api_key = ""api_key""
data = thinkific.connect(api_key).users.get(uid=""uid"")
data"
10296,Thinkific - Send users,"from naas_drivers import thinkific
api_key = ""api_key""
thinkific.connect(api_key)
thinkific.users.send(
    email=""bob@cashstory.com""
)"
10297,Pandas - ISO Date Conversion,"import pandas as pd
from dateutil.parser import parse
dict1 = {
    ""Name"": [""Peter"",""Dolly"",""Maggie"",""David"",""Isabelle""],
    ""Date"":[""20/2/2021"",""19/8/2014"",""8/9/2000"",""4/3/2013"",""14/7/1995""],
    ""Second Date"":[""August 20,2011"",""September 16,1993"",""January 23,2009"",""October 17,2019"",""March 4,2021""]
}
data=pd.DataFrame(dict1)
data
# Converting the first Date into ISO Format 
data[""Date""]=pd.to_datetime(data[""Date""]).dt.strftime('%Y-%m-%d')
# Converting the second date into ISO Format
data[""Second Date""]=data[""Second Date""].apply(lambda i: parse(i))
data"
10298,Pandas - Create dataframe from dict,"import pandas as pd
my_dict = {""LABEL"": 1995, ""VALUE"": 219}
df = pd.DataFrame([my_dict])
df
df"
10299,Pandas - Transform dataframe to dict,"from naas_drivers import yahoofinance
ticker = ""TSLA""
date_from = -100
date_to = 'today'
df_yahoo = yahoofinance.get(ticker,
                            date_from=-5)
df_yahoo
dict_yahoo = df_yahoo.to_dict(orient=""records"")
dict_yahoo"
10300,Pandas - Format number to string,"from naas_drivers import yahoofinance
ticker = ""TSLA""
date_from = -100
date_to = 'today'
df_yahoo = yahoofinance.get(ticker,
                            date_from=-5)
df_yahoo
df_yahoo['VALUE'] = df_yahoo['Close'].map(""‚Ç¨ {:,.2f}"".format).str.replace("","", "" "")
df_yahoo
"
10301,Pandas - Merge Dataframes,"import pandas as pd
import numpy as np
# Creating values to be used as datasets
dict1 = {
    ""student_id"": [1,2,3,4,5,6,7,8,9,10],
    ""student_name"": [""Peter"",""Dolly"",""Maggie"",""David"",""Isabelle"",""Harry"",""Akin"",""Abbey"",""Victoria"",""Sam""],
    ""student_course"": np.random.choice([""Biology"",""Physics"",""Chemistry""], size=10)
}
# Create dataframe
df_1 = pd.DataFrame(dict1)
df_1
# Creating values to be used as datasets
dict2 = {
    ""student_id"": np.random.choice([1,2,3,4,5,6,7,8,9,10], size=100),
    ""student_grade"": np.random.choice([""A"",""B"",""C"",""D"",""E"",""F""], size=100),
    ""professors"": np.random.choice([""Mark Levinson"",""Angela Marge"",""Bonnie James"",""Klaus Michealson""], size=100),
}
# Create dataframe
df_2 = pd.DataFrame(dict2)  # OR Data2=pd.read_csv(filepath)
df_2
df = pd.merge(df_1, df_2)
df
df = pd.merge(df_1, df_2, on=""student_id"")
df
df = pd.merge(df_1, df_2, on=""student_id"", how='left')
df
df_1 = df_1.rename(columns={""student_id"": ""id""}) # Renamed student_id to id so as to give this example
df_1
df = pd.merge(df_1, df_2, left_on=""id"", right_on=""student_id"")
df
df_1.set_index(""id"") # this will make id the new index for df_1
df = pd.merge(df_1, df_2, left_index=True, right_on=""student_id"")#the new index will be from index of df_2 where they joined
df
df_2.set_index(""student_id"") # making student_id the index of Data2
df = pd.merge(df_1, df_2, left_index=True, right_index=True) # new index will be from the left index unlike when joining only one index
df"
10302,Pandas - Convert datetime series,"import pandas as pd
dict1 = {
    ""Name"": [""Peter"",""Dolly"",""Maggie"",""David"",""Isabelle""],
    ""Date"": [""20/2/2021"",""19/8/2014"",""8/9/2000"",""4/3/2013"",""14/7/1995""],
    ""Second Date"": [""August 20,2011"",""September 16,1993"",""January 23,2009"",""October 17,2019"",""March 4,2021""]
}
df = pd.DataFrame(dict1)
df
# Your date string format
current_format = ""%d/%m/%Y""

# New date format you want to use
new_format = ""%Y-W%U""
df[""New_Date""] = pd.to_datetime(df[""Date""], format=current_format).dt.strftime(new_format)
df
df"
10303,Pandas - Create Pivot Table,"import pandas as pd
df = pd.DataFrame(
    {
        ""item"": [""apple"", ""apple"", ""apple"", ""apple"", ""apple""],
        ""size"": [""small"", ""small"", ""large"", ""large"", ""large""],
        ""location"": [""Walmart"", ""Aldi"", ""Walmart"", ""Aldi"", ""Aldi""],
        ""price"": [3, 2, 4, 3, 2.5],
    }
)
df
pivot = pd.pivot_table(
    df, values=""price"", index=[""item"", ""size""], columns=[""location""], aggfunc=""mean""
)
pivot"
10304,LinkedIn - Update metrics from posts in Notion content calendar,"import naas
from naas_drivers import linkedin, notion, emailbuilder
from datetime import datetime
import pandas as pd
import plotly.express as px
import os
import requests
# LinkedIn cookies
LI_AT = ""ENTER_YOUR_COOKIE_HERE"" # EXAMPLE : ""AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2""
JSESSIONID = ""ENTER_YOUR_JSESSIONID_HERE"" # EXAMPLE : ""ajax:8379907400220387585""

# LinkedIn profile url
PROFILE_URL = ""ENTER_YOUR_LINKEDIN_PROFILE_HERE"" # EXAMPLE ""https://www.linkedin.com/in/myprofile/""

# The first execution all posts will be retrieved.
# Then, you can use the parameter below to setup the number of posts you want to retrieved from LinkedIn API everytime this notebook is run.
NO_POSTS_RETRIEVED = 10
# Notion parameters
NOTION_TOKEN = ""ENTER_YOUR_NOTION_TOKEN_HERE"" # EXAMPLE : ""secret_eaLtxxxxxxxzuBPQvParsFxxxxxxx""
NOTION_DATABASE_URL = ""ENTER_YOUR_NOTION_DATABASE_URL_HERE"" # EXAMPLE : ""https://www.notion.so/naas-official/fc64df2aae7f4796963d14edec816xxxxx""
# Custom path of your CSV with the profile URL
profile_id = PROFILE_URL.split(""https://www.linkedin.com/in/"")[-1].split(""/"")[0]
csv_output = f""LINKEDIN_POSTS_{profile_id}.csv""
# Email
EMAIL = ""ENTER_YOUR_EMAIL"" # EXAMPLE : ""florent@naas.ai""
# the default settings below will make the notebook run everyday at 8:00
# for information on changing this setting, please check https://crontab.guru/ for information on the required CRON syntax 
naas.scheduler.add(cron=""0 8 * * *"")

# to de-schedule this notebook, simply run the following command: 
# naas.scheduler.delete()
def email_linkedin_limit(email):
    content = {
       ""header_naas"": (""<a href='https://www.naas.ai/'>""
                       ""<img align='center' width='30%' target='_blank' style='border-radius:5px;'""
                       ""src='https://landen.imgix.net/jtci2pxwjczr/assets/5ice39g4.png?w=160'""
                       ""alt='Please provide more information.'/>""
                       ""</a>""),
        ""txt_0"": emailbuilder.text(f""Hi there,<br><br>""
                                   ""Your LinkedIn cookies needs to be renewed.<br><br>""
                                   ""Please go to naas and update them in your notebook 'Setup LinkedIn'.<br>""),
        ""button"": emailbuilder.button(f""https://app.naas.ai/user/{email}/"", ""Go to Naas""),
        ""signature"": ""Naas Team"",
        ""footer"": emailbuilder.footer_company(naas=True)
    }
    email_content = emailbuilder.generate(display='iframe', **content)
    return email_content

# email_content = email_linkedin_limit(EMAIL)
def read_csv(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_posts = read_csv(csv_output)
df_posts
def get_last_posts(df_posts,
                   csv_output,
                   key=""POST_URL"",
                   no_posts=10,
                   min_updated_time=60):
    # Init output
    df_new = pd.DataFrame()
    
    try:
        # Init df posts is empty then return entire database
        if len(df_posts) > 0:
            if ""DATE_EXTRACT"" in df_posts.columns:
                last_update_date = df_posts[""DATE_EXTRACT""].max()
                time_last_update = datetime.now() - datetime.strptime(last_update_date, ""%Y-%m-%d %H:%M:%S"")
                minute_last_update = time_last_update.total_seconds() / 60
                if minute_last_update > min_updated_time:
                    # If df posts not empty get the last X posts (new and already existing)
                    df_new = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                                        limit=no_posts,
                                                                                        sleep=False)
                else:
                    print(f""üõë Nothing to update. Last update done {round(minute_last_update, 0)} minutes ago."")
                    return df_new
        else:
            df_new = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                                limit=-1)

        # Concat, save database in CSV and dependency in production
        df = pd.concat([df_new, df_posts]).drop_duplicates(key, keep=""first"")
        df[""DATE_EXTRACT""] = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
        df.to_csv(csv_output, index=False)
        naas.dependency.add(csv_output)
        
        # Return only last post retrieved
        return df_new.reset_index(drop=True)
    except Exception as e:
        print(""‚ùå Error"", e)
        if e.response.status_code == 302:
            email_content = email_linkedin_limit(EMAIL)
            naas.notification.send(email_to=EMAIL,
                                   subject=""‚ö†Ô∏è naas.ai - Update your Linkedin cookies"",
                                   html=email_content)
            raise(e)

df_update = get_last_posts(df_posts,
                           csv_output,
                           no_posts=NO_POSTS_RETRIEVED)
df_update
def create_polls_graph(uid, title, data):
    # Create dataframe
    df = pd.melt(pd.DataFrame([data]))
    df = df.sort_values(by=""value"")
    voters = df.value.sum()
    
    # Create fig
    fig = px.bar(df,
                 y=""variable"",
                 x=""value"",
                 orientation='h',
                 title=f""{title}<br><span style='font-size: 13px;'>Total amount of votes: {voters}</span>"",
                 text=""value"",
                 labels={""variable"": ""Poll options"",
                         ""value"": ""Poll results""}
                 )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        plot_bgcolor=""#ffffff"",
        width=600,
        height=400,
        font=dict(family=""Arial"", size=14, color=""black""),
        paper_bgcolor=""white"",
        xaxis_title=None,
        xaxis_showticklabels=False,
        yaxis_title=None,
        margin_pad=10,
    )
    fig.write_html(f""{uid}.html"")
    asset = naas.asset.add(f""{uid}.html"", params={""inline"": True})
    return asset

def update_poll_graph(row):
    poll_graph = None
    
    # Add polls
    poll_id = row.POLL_ID
    poll_question = row.POLL_QUESTION
    poll_results = row.POLL_RESULTS
    if poll_id and poll_question and poll_results:
        poll_graph = create_polls_graph(poll_id, poll_question, poll_results)
    return poll_graph
def update_dynamic_properties(page, row):
    # Page properties : dynamic
    page.number(""Engagment score"", float(row.ENGAGEMENT_SCORE))
    page.number(""Views"", int(row.VIEWS))
    page.number(""Likes"", int(row.LIKES))
    page.number(""Comments"", int(row.COMMENTS))
    page.number(""Shares"", int(row.SHARES))
    return page
def update_content_notion(df, database_url):
    # Decode database id
    database_id = database_url.split(""/"")[-1].split(""?v="")[0]
    
    # Get pages from notion database
    pages = notion.connect(NOTION_TOKEN).database.query(database_id, query={})
    
    # Manage dataframe empty
    if len(df) == 0:
        print(f""üõë Nothing to update in Notion."")
        return
    
    # Loop in data
    df.COMPANY_MENTION = df.COMPANY_MENTION.fillna("""")
    df.PROFILE_MENTION = df.PROFILE_MENTION.fillna("""")
    for i, row in df.iterrows():
        title = row.TITLE
        content_title = row.CONTENT_TITLE
        if title is None and content_title is not None:
            title = f""Repost - {content_title}""
        elif title is None and content_title is None:
            title = ""Repost""
        post_url = row.POST_URL
        print(post_url)
        
        # Create or update page
        page_new = True
        for page in pages:
            page_temp = page.df()
            page_id = page_temp.loc[page_temp.Name == ""Content URL"", ""Value""].values
            if page_id == post_url:
                page_new = False
                break
        try:
            if page_new:
                # Create new page in notion
                page = notion.Page.new(database_id=database_id).create()

                # Page properties : static
                page.date(""Publication Date"", row.PUBLISHED_DATE)
                page.title(""Name"", title)
                page.select(""Content type"", row.CONTENT)
                page.select(""Platform"", ""LinkedIn"")
                page.select(""Status"", ""Published ‚ú®"")
                page.select(""Author"", row.AUTHOR_NAME)
                profile_mention = row.COMPANY_MENTION
                if profile_mention is not None:
                    if len(profile_mention) > 2:
                        page.rich_text(""Profile mention"", profile_mention)
                company_mention = row.COMPANY_MENTION
                if company_mention is not None:
                    if len(company_mention) > 2:
                        page.rich_text(""Company mention"", company_mention)
                page.number(""Nb tags"", int(row.TAGS_COUNT))
                tags = row.TAGS
                if tags is None:
                    tags = """"
                else:
                    if len(tags) < 2:
                        tags = """"
                page.rich_text(""Tags"", tags)
                page.number(""Nb emojis"", int(row.EMOJIS_COUNT))
                emojis = row.EMOJIS
                if emojis is None:
                    emojis = """"
                else:
                    if len(emojis) < 2:
                        emojis = """"
                page.rich_text(""Emojis"", emojis)
                page.number(""Nb links"", int(row.LINKS_COUNT))
                links = row.LINKS
                if links is not None:
                    if len(links) > 2:
                        page.link(""Links"", links)
                page.number(""Nb characters"", int(row.CHARACTER_COUNT))
                page.link(""Content URL"", post_url)
                
                # Page blocks text
                page.heading_1(""Text"")
                text = row.TEXT
                if text is not None:
                    split_text = text.split(""\n"")
                    for t in split_text:
                        page.paragraph(t)
                    
                # Page blocks content
                image_url = row.IMAGE_URL
                content_url = row.CONTENT_URL
                poll_question = row.POLL_QUESTION
                if image_url or content_title or content_url or poll_question:
                    page.heading_1(""Content"")
                
                # Add image in content section
                if image_url:
                    page.heading_2(""Image"")
                    page.paragraph(image_url)
                    
                # Add post in content section
                if content_title:
                    page.heading_2(""Media"")
                    page.heading_3(content_title)
                
                if content_url:
                    page.paragraph(content_url)
                
                # Add poll graph in content section
                if poll_question:
                    page.heading_3(""Poll"")
                    poll_graph = update_poll_graph(row)
                    if poll_graph:
                        page.embed(poll_graph)
                
                # Page properties : dynamic
                page = update_dynamic_properties(page, row)
                
                # Create page in Notion
                page.update()
                print(f""‚úÖ Page '{title}' created in Notion."", '\n')
            else:
                # Update poll graph
                update_poll_graph(row)
                
                # Page properties : dynamic
                page = update_dynamic_properties(page, row)
                
                # Update page
                page.update()
                print(f""üìà Post stats updated in notion for page '{title}'."", '\n')
        except Exception as e:
            print(f""‚ùå Error creating page '{title}' in Notion"", e)
            print(row)
update_content_notion(df_update, NOTION_DATABASE_URL)"
10305,LinkedIn - Ignore invitation received,"from naas_drivers import linkedin
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""
df_invitation = linkedin.connect(LI_AT, JSESSIONID).invitation.get_received()
df_invitation
# Value in column ""INVITATION_ID""
invitation_id = ""691596420000000000""

# Value in column ""SHARED_SECRET""
shared_secret = ""6xubdsfs""

# If ""INVITATION_TYPE"" is ""Profile"" then False else True
is_generic = False
result = linkedin.connect(LI_AT, JSESSIONID).invitation.ignore(invitation_id, shared_secret, is_generic)
result"
10306,LinkedIn - Get contact from profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
PROFILE_URL = ""PROFILE_URL""
df = linkedin.connect(LI_AT, JSESSIONID).profile.get_contact(PROFILE_URL)
df"
10307,LinkedIn - Linkedin Follow total content views,"from naas_drivers import linkedin
import naas
import pandas as pd
from datetime import datetime
import plotly.graph_objects as go
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# Enter profile URL
PROFILE_URL = ""PROFILE_URL""
# Outputs
name_output = ""LinkedIn_Total_content_views""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
# Schedule your notebook everyday at 9:00 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
# Get posts feed from CSV stored in your local (Returns empty if CSV does not exist)
def get_past_feeds(csv_output):
    try:
        df = pd.read_csv(csv_output)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_posts_feed = get_past_feeds(csv_output)
df_posts_feed
def get_posts(df):
    # Get last post URL in dataframe
    if len(df) == 0:
        last_post_url = None
    else:
        last_post_url = df.POST_URL[0]
    # Get new posts since last url (this part is important to optimize script performance)
    until = {}
    if last_post_url:
        until = {""POST_URL"": last_post_url}

    df_posts_feed = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL, until=until, limit=-1)

    # Merge dataframe
    merge_df = df.append(df_posts_feed, ignore_index=False)
    merge_df.sort_values(by = 'PUBLISHED_DATE', ascending = False, inplace=True)
    
    # Keeps/updates the latest views count value for that day
    merge_df.drop_duplicates('POST_URL', keep = 'last', inplace=True)
    return merge_df

merged_df = get_posts(df_posts_feed)
merged_df
# Create dataframe with number of LinkedIn views cumulated by date with daily variation
# -> Empty date must be fullfiled with last value

def get_trend(posts_df):
    
    df = posts_df.copy()
    date_col_name='PUBLISHED_DATE'
    value_col_name=""VIEWS""
    date_order='asc'
    
    # Format date
    for idx, item in enumerate(df['PUBLISHED_DATE']):
        df.loc[idx, 'PUBLISHED_DATE'] = item.split('+')[0]
    
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""sum""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    
    # Calc sum cum
    df[""value_cum""] = df.agg({value_col_name: ""cumsum""})
    df.drop(columns='VIEWS', inplace=True)
    return df.reset_index(drop=True)

df_trend = get_trend(merged_df)
df_trend
def create_linechart(df, label, value, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            mode=""lines"",
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=title,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, label=""PUBLISHED_DATE"", value=""value_cum"", title='Total Content Views')
# Save your dataframe in CSV
merged_df.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)"
10308,LinkedIn - Get company followers,"from naas_drivers import linkedin
import pandas as pd
import naas
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Company URL
COMPANY_URL = ""https://www.linkedin.com/company/naas-ai/""
# Inputs
csv_input = ""LinkedIn_company_followers.csv""
# Get company followers from CSV stored in your local (Returns empty if CSV does not exist)
def get_company_followers(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_followers = get_company_followers(csv_input)
df_followers
def get_new_followers(df, input_path):
    # Get all profiles
    profiles = []
    if len(df) > 0:
        profiles = df.PROFILE_ID.unique()
    start = 0
    while True:
        tmp_df = linkedin.connect(LI_AT, JSESSIONID).company.get_followers(COMPANY_URL,
                                                                           start=start,
                                                                           limit=1,
                                                                           sleep=False)
        profile_id = None
        if ""PROFILE_ID"" in tmp_df.columns:
            profile_id = tmp_df.loc[0, ""PROFILE_ID""]
        if profile_id in profiles:
            break
        else:
            df = pd.concat([tmp_df, df])
            df.to_csv(input_path, index=False)
            start += 1
    return df.reset_index(drop=True)

merged_df = get_new_followers(df_followers, csv_input)
merged_df
merged_df"
10309,LinkedIn - Accept all invitations and send first message,"from naas_drivers import linkedin
import naas
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# First message
FIRST_MESSAGE = ""Hello, Nice to connect!""
# Schedule your notebook every hour
naas.scheduler.add(cron=""0 * * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
df_invitation = linkedin.connect(LI_AT, JSESSIONID).invitation.get_received()
df_invitation
def accept_new_contact(df):
    df_accept = pd.DataFrame()
    
    # Loop
    for index, row in df.iterrows():
        fullname = row.FULLNAME
        status = row.INVITATION_STATUS
        invitation_id = row.INVITATION_ID
        shared_secret = row.SHARED_SECRET
        if status == ""PENDING"":
            print(fullname)
            tmp_df = linkedin.connect(LI_AT, JSESSIONID).invitation.accept(invitation_id, shared_secret)
            df_accept = pd.concat([df_accept, tmp_df])
    return df_accept

df_accept = accept_new_contact(df_invitation)
df_accept
def send_first_message(df):
    # Loop
    for index, row in df.iterrows():
        fullname = row.FULLNAME
        profile_id = row.PROFILE_ID
        print(fullname)
        linkedin.connect(LI_AT, JSESSIONID).message.send(FIRST_MESSAGE, profile_id)

send_first_message(df_accept)
df_accept"
10310,LinkedIn - Generate leads from posts,"from naas_drivers import linkedin, hubspot
import pandas as pd
import numpy as np
import naas
from datetime import datetime, timedelta
import requests
import json
# Lindekin cookies
LI_AT = ""AQEFALsBAAAAAAbjCtIAAAF_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""
JSESSIONID = ""ajax:42778xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx""

# Enter your profile URL
PROFILE_URL = ""<YOUR_LINKEDIN_PROFILE_URL>""
# HubSpot API key
HS_API_TOKEN = ""<YOUR_HUBSPOT_API_TOKEN>""

# Your company located after ""reports-dashboard/"" when you are connected in HubSpot https://app.hubspot.com/reports-dashboard/2474088/view/244930
HS_COMPANY_ID = ""<YOUR_HUBSPOT_COMPANY_ID>""

#Hubspot contact URL
HS_CONTACT_URL = ""https://app.hubspot.com/contacts/""+HS_COMPANY_ID+""/contact/""

# Contact owner => contact id to whom task needs to be assigned
HS_OWNER_ID = ""<YOUR_HUBSPOT_API_TOKEN>"" #remove double quotes from owner id (to be added as integer value)

# Time delay to set due date for tasks in days
time_delay = 10
    
# Calc timestamp
tstampobj = datetime.now() + timedelta(days=time_delay)
tstamp = tstampobj.timestamp() * 1000
     
hs = hubspot.connect(HS_API_TOKEN)
SEND_EMAIL_TO = ""<YOUR_EMAIL_ID>""
df_posts = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL, count=100)
df_posts.head()
def get_likes(df_posts):
    DF_all_post_likes = pd.DataFrame()
    for index, row in df_posts.iterrows():
        df = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(row['POST_URL'])
        DF_all_post_likes = DF_all_post_likes.append(df)
get_likes(df_posts)
def count_likes(df):
    to_group = [""PROFILE_URN"",
                ""PROFILE_ID"",
                ""FIRSTNAME"",
                ""LASTNAME""]
    #df = df.groupby(to_group, as_index=False).agg({""PROFILE_ID"": ""count""}).reset_index(name='count')
    #df = df.sort_values(by=""PROFILE_ID"", ascending=False)
    df2 = df.groupby(['PROFILE_ID']).size().sort_values(ascending=False).reset_index(name='LIKE_COUNT')
    return df2

df_counts = count_likes(DF_all_post_likes)
df_counts
max_likes = df_counts[""LIKE_COUNT""].max()
cluster_1 = round(max_likes * 0.1, 0)
cluster_2 = round(max_likes * 0.5, 0)
cluster_3 = round(max_likes * 0.8, 0)
conditions = [
    (df_counts[""LIKE_COUNT""] <= cluster_1),
    (df_counts[""LIKE_COUNT""] > cluster_1) & (df_counts[""LIKE_COUNT""] <= cluster_2),
    (df_counts[""LIKE_COUNT""] > cluster_2) & (df_counts[""LIKE_COUNT""] <= cluster_3),
    (df_counts[""LIKE_COUNT""] > cluster_3)
    ]

values = ['L4', 'L3', 'L2', 'L1']
df_counts['POTENTIAL_LEAD'] = np.select(conditions, values)
df_counts
df_leads = df_counts.loc[df_counts['POTENTIAL_LEAD'].isin([""L1"", ""L2""])]
df_leads
leads_list = [];

r_count = 1

for index, row in df_leads.iterrows():
    profileid = row['PROFILE_ID']
    profileurl = ""https://www.linkedin.com/in/""+profileid+""/""
       
    PROFILECONTACTS = linkedin.connect(LI_AT, JSESSIONID).profile.get_contact(profileurl)
    PROFILEIDENTITY = linkedin.connect(LI_AT, JSESSIONID).profile.get_identity(profileurl)
        
    profileemail = PROFILECONTACTS.at[0,'EMAIL']
    profilephoneno = PROFILECONTACTS.at[0,'PHONENUMBER']
    profilename = PROFILEIDENTITY.at[0,'FIRSTNAME'] + "" ""+ PROFILEIDENTITY.at[0,'LASTNAME']
    profilefirstname = PROFILEIDENTITY.at[0,'FIRSTNAME']
    profilelastname = PROFILEIDENTITY.at[0,'LASTNAME']
    profileoccupation = PROFILEIDENTITY.at[0,'OCCUPATION']
    leads_list.append([profilename, profilefirstname, profilelastname, profileemail, profilephoneno, profileoccupation, profileurl])
    r_count = r_count + 1
#to store the resulting output of create contact method
contact_id = """"
HS_CONTACT_ID_LIST = []
for i in leads_list:
        profilename = i[0]
        profilefirstname = i[1]
        profilelastname = i[2]
        profileemail = i[3]
        profilephoneno = i[4]
        profileoccupation = i[5]
        profileurl = i[6]
        # With send method
        data = {""properties"": 
                {
                    ""linkedinbio"": profileurl,
                    ""firstname"": profilefirstname,
                    ""lastname"": profilelastname,
                    ""jobtitle"": profileoccupation,
                    ""email"": profileemail,
                    ""phone"": profilephoneno,
                     ""hubspot_owner_id"": 111111086,
                }
               }
        #write data to CRM ( create new contact in CRM)
       
        contact_id = hs.contacts.send(data)
        HS_CONTACT_ID_LIST.append([contact_id, profileurl])
table_header = f'<table style=""border-collapse:collapse;border-spacing:0;font-family:Arial""><thead><tr><th style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;text-align:left"">Contact ID</th><th style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;text-align:left"">Hubspot URL</th><th style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;text-align:left"">LinkedIN URL</th></tr></thead><tbody>'
table_body = ''
table_footer = '</tbody></table>'

for i in HS_CONTACT_ID_LIST:
    hs_contact_url = HS_CONTACT_URL + i[0]
    linkedin_url = i[1]
    tablerow = '<tr><td style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;"">'+i[0]+'</td><td style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;"">'+hs_contact_url+'</td><td style=""border-color:black;border-style:solid;border-width:1px;padding:6px 10px;"">'+linkedin_url+'</td></tr>'
    table_body = table_body + tablerow

table = table_header + table_body + table_footer

email_body = f'<p style=""font-family:Arial"">Hi there,</p><br/><p style=""font-family:Arial"">Following new task(s) has been created for you :</p><br/>'+table
def create_task(owner_id,
                tstamp,
                contact_id,contact_props,contact_linkedin_url,hs_contact_url,engagement=""TASK""):
    """""" owner_id=HS_OWNER_ID, tstamp=tstamp, subject=subject, body=body, status=status, engagement=""TASK""
    Engagement type = TASK | NOTE | EMAIL | MEETING | CALL 
    """"""
   
    payload = json.dumps({
            ""engagement"": {
                ""active"": 'true',
                ""ownerId"": 111111086,
                ""type"": ""TASK"",
                ""timestamp"": tstamp
            },
            ""associations"": {
                ""contactIds"": [1551],
                ""companyIds"": [],
                ""dealIds"": [],
                ""ownerIds"": [],
            },

            ""metadata"": {
                ""body"": ""Hi there, you need to contact following user & task is already assigned to you.<br/>"" + ""Name :"" +contact_props['firstname']+ contact_props['lastname'] + "" Contact URL : "" + hs_contact_url,
                ""subject"": ""Task created for Contact ID :""+ contact_id,
                ""status"": ""NOT_STARTED"",
                ""forObjectType"": ""CONTACT""
            }
        });
    url = ""https://api.hubapi.com/engagements/v1/engagements""
    params = {""hapikey"": HS_API_TOKEN}
    headers = {'Content-Type': ""application/json""}
    # Post requests
    res = requests.post(url,data=payload,headers=headers,params=params)
        
    # Check requests
    try:
        res.raise_for_status()
    except requests.HTTPError as e:
        raise (e)
    res_json = res.json()

    # Fetch the task id of the current task created
    task_id = res_json.get(""engagement"").get(""id"")
    print(""üéÜ Task created successfully: "", task_id)
        
    return res_json
TASK_ID_LIST = []
for i in HS_CONTACT_ID_LIST:
    contact_id = i[0]
    contact = hs.contacts.get(contact_id)
    contact_props = contact.get('properties')
    contact_linkedin_url = i[1]
    hs_contact_url = HS_CONTACT_URL + contact_id
    #print(hs_contact_url)
    result = create_task(owner_id=HS_OWNER_ID, 
                tstamp=tstamp, 
                contact_id=contact_id ,
                contact_props = contact_props,
                contact_linkedin_url=contact_linkedin_url,
                hs_contact_url = hs_contact_url, 
                engagement=""TASK"")
    TASK_ID_LIST.append(result.get(""engagement"").get(""id""))
if not TASK_ID_LIST:
    print(""No tasks created"")
else:
    email_to = SEND_EMAIL_TO #to send the report
    subject = ""LinkedIN Leads Alert""
    content = email_body

    naas.notification.send(email_to=email_to, subject=subject, html=content)
"
10311,LinkedIn - Get posts stats from profile,"from naas_drivers import linkedin
import pandas as pd
from datetime import datetime
import naas
# LinkedIn cookies
LI_AT = ""ENTER_YOUR_COOKIE_HERE"" # EXAMPLE : ""AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2""
JSESSIONID = ""ENTER_YOUR_JSESSIONID_HERE"" # EXAMPLE : ""ajax:8379907400220387585""

# LinkedIn profile url
PROFILE_URL = ""ENTER_YOUR_LINKEDIN_PROFILE_HERE"" # EXAMPLE ""https://www.linkedin.com/in/myprofile/""

# The first execution all posts will be retrieved.
# Then, you can use the parameter below to setup the number of posts you want to retrieved from LinkedIn API everytime this notebook is run.
NO_POSTS_RETRIEVED = 10
# Custom path of your CSV with the profile URL
profile_id = PROFILE_URL.split(""https://www.linkedin.com/in/"")[-1].split(""/"")[0]
csv_output = f""LINKEDIN_POSTS_{profile_id}.csv""
# the default settings below will make the notebook run everyday at 8:00
# for information on changing this setting, please check https://crontab.guru/ for information on the required CRON syntax 
naas.scheduler.add(cron=""0 8 * * *"")

# to de-schedule this notebook, simply run the following command: 
# naas.scheduler.delete()
def read_csv(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_posts = read_csv(csv_output)
df_posts
def update_last_posts(df_posts, key=""POST_URL"", no_posts=10):
    # Init output
    df_update = pd.DataFrame()
    
    # Init df posts is empty then return entire database
    if len(df_posts) > 0:
        # If df posts not empty get the last X posts (new and already existing)
        df_update = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                               limit=no_posts,
                                                                               sleep=False)
    else:
        df_update = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                               limit=-1)
    # Concat and add extract date
    df = pd.concat([df_update, df_posts]).drop_duplicates(key, keep=""first"")
    df[""DATE_EXTRACT""] = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")        
    # Return only last post retrieved
    return df.reset_index(drop=True)
    
df_update = update_last_posts(df_posts,
                              no_posts=NO_POSTS_RETRIEVED)
df_update
# Save dataframe in CSV
df_update.to_csv(csv_output, index=False)

# Send CSV to production (It could be used with other scripts)
naas.dependency.add(csv_output)"
10312,LinkedIn - Send comments from post to gsheet,"from naas_drivers import linkedin, gsheet
import random
import time
import pandas as pd
from datetime import datetime
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# Post url
POST_URL = ""POST_URL""
# Spreadsheet id
SPREADSHEET_ID = ""SPREADSHEET_ID""

# Sheet names
SHEET_POST_COMMENTS = ""POST_COMMENTS""
SHEET_MY_NETWORK = ""MY_NETWORK""
SHEET_NOT_MY_NETWORK = ""NOT_MY_NETWORK""
DATETIME_FORMAT = ""%Y-%m-%d %H:%M:%S""
df_posts = linkedin.connect(LI_AT, JSESSIONID).post.get_comments(POST_URL)
df_posts[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
df_network = pd.DataFrame()

for _, row in df_posts.iterrows():
    profile_id = row.PROFILE_ID
    # Get network information to know distance between you and people who likes the post
    tmp_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile_id)
    # Concat dataframe
    df_network = pd.concat([df_network, tmp_network], axis=0)
    # Time sleep in made to mimic human behavior, here it is randomly done between 2 and 5 seconds
    time.sleep(random.randint(2, 5))

df_network.head(5)
df_all = pd.merge(df_posts, df_network, on=[""PROFILE_URN"", ""PROFILE_ID""], how=""left"")
df_all = df_all.sort_values(by=[""FOLLOWERS_COUNT""], ascending=False)
df_all = df_all[df_all[""DISTANCE""] != ""SELF""].reset_index(drop=True)
df_all.head(5)
# My network
my_network = df_all[df_all[""DISTANCE""] == ""DISTANCE_1""].reset_index(drop=True)
my_network[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
my_network.head(5)
# Not in my network
not_my_network = df_all[df_all[""DISTANCE""] != ""DISTANCE_1""].reset_index(drop=True)
not_my_network[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
not_my_network.head(5)
gsheet.connect(SPREADSHEET_ID).send(df_posts, sheet_name=SHEET_POST_COMMENTS, append=False)
gsheet.connect(SPREADSHEET_ID).send(my_network, sheet_name=SHEET_MY_NETWORK, append=False)
gsheet.connect(SPREADSHEET_ID).send(not_my_network, sheet_name=SHEET_NOT_MY_NETWORK, append=False)"
10313,LinkedIn - Send connections from network to gsheet,"from naas_drivers import linkedin, gsheet
import naas
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""
# Spreadsheet URL
SPREADSHEET_URL = ""https://docs.google.com/spreadsheets/d/XXXXXXXXXXXXXXXXXXXX""

# Sheet name
SHEET_NAME = ""LK_CONNECTIONS""
naas.scheduler.add(cron=""0 8 * * *"")

#-> To delete your scheduler, please uncomment the line below and execute this cell
# naas.scheduler.delete()
df_gsheet = gsheet.connect(SPREADSHEET_URL).get(sheet_name=SHEET_NAME)
df_gsheet
def get_new_connections(df_gsheet, key=""PROFILE_URN""):
    profiles = []
    if len(df_gsheet) > 0:
        profiles = df_gsheet[key].unique()
    else:
        df = linkedin.connect(LI_AT, JSESSIONID).network.get_connections(limit=-1)
        return df
    
    # Get new
    df_new = pd.DataFrame()
    update = True
    while update:
        start = 0
        df = linkedin.connect(LI_AT, JSESSIONID).network.get_connections(start=start, count=100, limit=100)
        new_profiles = df[key].unique()
        for i, p in enumerate(new_profiles):
            if p in profiles:
                update = False
                df = df[:i]
                break
        start += 100
        df_new = pd.concat([df_new, df])
    return df_new

df_new = get_new_connections(df_gsheet, key=""PROFILE_URN"")
df_new
gsheet.connect(SPREADSHEET_URL).send(df_new,
                                     sheet_name=SHEET_NAME,
                                     append=True)"
10314,LinkedIn - Send message to profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
CONTENT = ""Hello !""
RECIPIENTS_URL = ""LINKEDIN_URL""
result = linkedin.connect(LI_AT, JSESSIONID)
result = linkedin.message.send(content=CONTENT,
                               recipients_url=RECIPIENTS_URL)"
10315,LinkedIn - Send likes from post to gsheet,"from naas_drivers import linkedin, gsheet
import random
import time
import pandas as pd
from datetime import datetime
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# Post url
POST_URL = ""POST_URL""
# Spreadsheet id
SPREADSHEET_ID = ""SPREADSHEET_ID""

# Sheet names
SHEET_POST_LIKES = ""POST_LIKES""
SHEET_MY_NETWORK = ""MY_NETWORK""
SHEET_NOT_MY_NETWORK = ""NOT_MY_NETWORK""
DATETIME_FORMAT = ""%Y-%m-%d %H:%M:%S""
df_posts = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(POST_URL)
df_posts[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
df_network = pd.DataFrame()

for _, row in df_posts.iterrows():
    profile_id = row.PROFILE_ID
    # Get network information to know distance between you and people who likes the post
    tmp_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile_id)
    # Concat dataframe
    df_network = pd.concat([df_network, tmp_network], axis=0)
    # Time sleep in made to mimic human behavior, here it is randomly done between 2 and 5 seconds
    time.sleep(random.randint(2, 5))

df_network.head(5)
df_all = pd.merge(df_posts, df_network, on=[""PROFILE_URN"", ""PROFILE_ID""], how=""left"")
df_all = df_all.sort_values(by=[""FOLLOWERS_COUNT""], ascending=False)
df_all = df_all[df_all[""DISTANCE""] != ""SELF""].reset_index(drop=True)
df_all.head(5)
# My network
my_network = df_all[df_all[""DISTANCE""] == ""DISTANCE_1""].reset_index(drop=True)
my_network[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
my_network.head(5)
# Not in my network
not_my_network = df_all[df_all[""DISTANCE""] != ""DISTANCE_1""].reset_index(drop=True)
not_my_network[""DATE_EXTRACT""] = datetime.now().strftime(DATETIME_FORMAT)
not_my_network.head(5)
gsheet.connect(SPREADSHEET_ID).send(df_posts, sheet_name=SHEET_POST_LIKES, append=False)
gsheet.connect(SPREADSHEET_ID).send(my_network, sheet_name=SHEET_MY_NETWORK, append=False)
gsheet.connect(SPREADSHEET_ID).send(not_my_network, sheet_name=SHEET_NOT_MY_NETWORK, append=False)"
10316,LinkedIn - Get followers from network,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
df = linkedin.connect(LI_AT, JSESSIONID).network.get_followers(limit=-1)
df"
10317,LinkedIn - Get likes from post,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
POST_URL = ""POST_URL""
df = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(POST_URL)
df"
10318,LinkedIn - Send invitation to company followers,"from naas_drivers import linkedin
import pandas as pd
from datetime import datetime
import naas
import plotly.graph_objects as go
import os
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Company URL
COMPANY_URL = ""https://www.linkedin.com/company/naas-ai/""

# LinkedIn limit invitations up to 100 per week (Becareful !)
LIMIT = 10
# Inputs
csv_input = ""LinkedIn_company_followers.csv""

# Outputs
csv_contact = ""LINKEDIN_EXISTING_CONTACT.csv"" # CSV to manage and remove profile already in your contact
csv_not_valid = ""LINKEDIN_NOT_VALID.csv"" # CSV to manage URL not valid
csv_invitation = ""LINKEDIN_INVITATIONS_SENT.csv"" # CSV to store invitations sent
# Schedule your notebook everyday at 9:00 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
# Get company followers from CSV stored in your local (Returns empty if CSV does not exist)
def get_company_followers(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_followers = get_company_followers(csv_input)
df_followers
def get_new_followers(df, input_path):
    # Get all profiles
    profiles = []
    if len(df) > 0:
        profiles = df.PROFILE_ID.unique()
    start = 0
    while True:
        tmp_df = linkedin.connect(LI_AT, JSESSIONID).company.get_followers(COMPANY_URL,
                                                                           start=start,
                                                                           limit=1,
                                                                           sleep=False)
        profile_id = None
        if ""PROFILE_ID"" in tmp_df.columns:
            profile_id = tmp_df.loc[0, ""PROFILE_ID""]
        if profile_id in profiles:
            break
        else:
            df = pd.concat([tmp_df, df])
            df.to_csv(input_path, index=False)
            start += 1
    return df.reset_index(drop=True)

merged_df = get_new_followers(df_followers, csv_input)
merged_df
df_lk_invitations = linkedin.connect(LI_AT, JSESSIONID).invitation.get_sent()
df_lk_invitations
def get_csv(output_path):
    df = pd.DataFrame()
    if os.path.exists(output_path):
        df = pd.read_csv(output_path).drop_duplicates()
    return df
df_contacts = get_csv(csv_contact)
df_contacts
df_csv_invitations = get_csv(csv_invitation)
df_csv_invitations
def get_new_invitations(df,
                        df_lk_invitations,
                        df_csv_invitations,
                        df_contacts):
    
    # Get list of pending LinkedIn invitations
    pending_lk_invitations = []
    if len(df_lk_invitations) > 0:
        pending_lk_invitations = df_lk_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending LinkedIn invitations :"", len(pending_lk_invitations))
    
    # Get list of CSV invitations
    pending_csv_invitations = []
    if len(df_csv_invitations) > 0:
        pending_csv_invitations = df_csv_invitations[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Pending CSV invitations :"", len(pending_csv_invitations))
    
    # Get profile already in network
    contacts = []
    if len(df_contacts) > 0:
        contacts = df_contacts[""PUBLIC_ID""].unique().tolist()
    print(""‚ùå Already in network :"", len(contacts))
    
    # Remove pending invitations / already in network / not valid profile from dataframe 
    exclude = (pending_lk_invitations + pending_csv_invitations + contacts)
    df = df[~df[""PUBLIC_ID""].isin(exclude)].dropna().reset_index(drop=True)
    print(""‚û°Ô∏è New invitation:"", len(df))
    return df

df_new_invitations = get_new_invitations(merged_df,
                                         df_lk_invitations,
                                         df_csv_invitations,
                                         df_contacts)
df_new_invitations
def send_invitation(df,
                    df_contacts=None,
                    df_csv_invitations=None):
    # Check if new invitations to perform
    if len(df) == 0:
        print(""ü§ô No new invitations to send"")
        return df
    
    # Setup variables
    if df_contacts is None:
        df_contacts = pd.DataFrame()
    if df_csv_invitations is None:
        df_csv_invitations = pd.DataFrame()
        
    # Loop
    count = 1
    df.PROFILE_ID = df.PROFILE_ID.fillna(0)
    for index, row in df.iterrows():
        df_network = pd.DataFrame()
        profile = row[""PUBLIC_ID""]
        print(f""‚û°Ô∏è Checking :"", profile)
        
        # Get distance with profile
        if profile != 0:
            df_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile)
            
        # Check if profile is already in your network
        if len(df_network) > 0:
            distance = df_network.loc[0, ""DISTANCE""]
            # If not profile in network...
            if distance not in [""SELF"", ""DISTANCE_1""]:
                # => send invitation
                try:
                    linkedin.connect(LI_AT, JSESSIONID).invitation.send(recipient_url=profile)
                    print(count, ""- üôå Invitation successfully sent"")
                    df_csv_invitations = pd.concat([df_csv_invitations, df_network])
                    df_csv_invitations.to_csv(csv_invitation, index=False)
                except Exception as e:
                    print(""‚ùå Invitation not sent"", e)
                count += 1
            else:
                # If profile already in your network => append network result to CSV existing contact to not check it again
                df_contacts = pd.concat([df_contacts, df_network])
                df_contacts.to_csv(csv_contact, index=False)
                print(f""üëç Already in my network, üíæ saved in CSV"")
            
        # Manage LinkedIn limit
        if count > LIMIT:
            print(""‚ö†Ô∏è LinkedIn invitation limit reached"", LIMIT)
            return df_csv_invitations
    return df_csv_invitations
        
df_csv_invitations = send_invitation(df_new_invitations,
                                     df_contacts,
                                     df_csv_invitations)
if len(merged_df) > 0:
    merged_df.to_csv(csv_input, index=False)
    naas.dependency.add(csv_input)
df_csv_invitations
"
10319,LinkedIn - Get conversations,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
df = linkedin.connect(LI_AT, JSESSIONID).message.get_conversations()
df"
10320,LinkedIn - Get identity from profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
PROFILE_URL = ""PROFILE_URL""
df = linkedin.connect(LI_AT, JSESSIONID).profile.get_identity(PROFILE_URL)
df"
10321,LinkedIn - Get stats from post,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
POST_URL = ""POST_URL""
df = linkedin.connect(LI_AT, JSESSIONID).post.get_stats(POST_URL)
df"
10322,LinkedIn - Send weekly post engagement metrics by email,"from naas_drivers import linkedin
import naas
from dateutil.parser import parse
import matplotlib.pyplot as plt
try:
    import seaborn as sns
except:
    !pip install seaborn --user
    import seaborn as sns
import pandas as pd
from datetime import datetime, date
import random
import time
# LinkedIn cookies
LI_AT = ""ENTER_YOUR_COOKIE_HERE"" # EXAMPLE : ""AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2""
JSESSIONID = ""ENTER_YOUR_JSESSIONID_HERE"" # EXAMPLE : ""ajax:8379907400220387585""

# LinkedIn profile url
PROFILE_URL = ""ENTER_YOUR_LINKEDIN_PROFILE_HERE"" # EXAMPLE ""https://www.linkedin.com/in/myprofile/""

# The first execution all posts will be retrieved.
# Then, you can use the parameter below to setup the number of posts you want to retrieved from LinkedIn API everytime this notebook is run.
NO_POSTS_RETRIEVED = 10
EMAIL_TO = ""ENTER_RECIPIENT_EMAIL_HERE"" # you will receive weekly summary at this email 
EMAIL_FROM = None # summary will have this email as sender. Only available for your naas email, otherwise you will receive this email from notification@naas.ai
EMAIL_SUBJECT = 'LinkedIn Metrics' # subject of your email
# Custom Path of your CSV with profile URL
profile_id = PROFILE_URL.split(""https://www.linkedin.com/in/"")[-1].split(""/"")[0]
csv_output = f""LINKEDIN_POSTS_{profile_id}.csv""
# Change your remote timezone if needed. By default remote timezone is ""Europe/Paris""
# naas.set_remote_timezone(""Europe/Lisbon"")

# the default settings below will make the notebook run at 12:00 each Saturday
# for information on changing this setting, please check https://crontab.guru/ for information on the required CRON syntax 
naas.scheduler.add(cron=""0 12 * * 6"")
# this notebook will run each week until de-scheduled
# to de-schedule this notebook, simply run the following command: 
# naas.scheduler.delete()
def get_posts(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_posts = get_posts(csv_output)
df_posts
def update_last_posts(df_posts,
                      csv_output,
                      key=""POST_URL"",
                      no_posts=10,
                      min_updated_time=60):
    # Init output
    df_new = pd.DataFrame()
    
    # Init df posts is empty then return entire database
    if len(df_posts) > 0:
        if ""DATE_EXTRACT"" in df_posts.columns:
            last_update_date = df_posts[""DATE_EXTRACT""].max()
            time_last_update = datetime.now() - datetime.strptime(last_update_date, ""%Y-%m-%d %H:%M:%S"")
            minute_last_update = time_last_update.total_seconds() / 60
            if minute_last_update > min_updated_time:
                # If df posts not empty get the last X posts (new and already existing)
                df_new = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                                    limit=no_posts,
                                                                                    sleep=False)
            else:
                print(f""üõë Nothing to update. Last update done {round(minute_last_update, 0)} minutes ago."")
    else:
        df_new = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL,
                                                                            limit=-1)

    # Concat, save database in CSV and dependency in production
    df = pd.concat([df_new, df_posts]).drop_duplicates(key, keep=""first"")
    df[""DATE_EXTRACT""] = datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")
    df.to_csv(csv_output, index=False)
    naas.dependency.add(csv_output)

    # Return only last post retrieved
    return df.reset_index(drop=True)

df_posts = update_last_posts(df_posts,
                             csv_output,
                             no_posts=NO_POSTS_RETRIEVED)
df_posts
def get_iso_year(row) :
    return parse(row.PUBLISHED_DATE).isocalendar()[0]

def get_iso_week(row) :
    return parse(row.PUBLISHED_DATE).isocalendar()[1]

def get_iso_day(row) :
    return parse(row.PUBLISHED_DATE).isoweekday()

def get_iso_day_string(row) :
    return datetime.strptime(str(parse(row.PUBLISHED_DATE).isoweekday()), ""%d"").strftime(""%A"")

# week of the year
df_posts['ISO_YEAR'] = df_posts.apply(get_iso_year, axis=1)
# week of the year
df_posts['ISO_WEEK'] = df_posts.apply(get_iso_week, axis=1)
# day of the year
df_posts['DAY'] = df_posts.apply(get_iso_day, axis=1)
# day of the week
df_posts['DAY OF WEEK'] = df_posts.apply(get_iso_day_string, axis=1)

# all posts made this week
df_this_week = df_posts[(df_posts['ISO_WEEK'] == date.today().isocalendar()[1]) &
                        (df_posts['ISO_YEAR'] == date.today().isocalendar()[0])].sort_values('DAY')
df_this_week.head()
# if we didn't post this week, send a simple email of encouragement and exit
if(df_this_week.empty):
    post = """"""\
    <h2>It looks like you took a week off from posting...</h2>
    <h4>Good for you, you'll crush it next week!</h4>""""""
    naas.notification.send(email_to=EMAIL_TO,
                           subject=EMAIL_SUBJECT,
                           html=post,
                           email_from=EMAIL_FROM)
    raise SystemExit(""No posts this week, basic email sent"")
likes = df_this_week.LIKES.sum()
views = df_this_week.VIEWS.sum()
shares = df_this_week.SHARES.sum()
comments = df_this_week.COMMENTS.sum()

# preview of what will be send by email:
print(""This week's cumulative statistics:"")
print(""\n\tüëÄ Impressions\t"", views, ""\n\tüëç Likes\t"", likes, ""\n\tüí¨ Comments\t"", comments, ""\n\t‚è© Shares\t"", shares)
top_post = df_this_week.sort_values(""VIEWS"", ascending=False).iloc[0]

# get information on most viewed post
top_post_text = top_post[""TEXT""]
top_post_url = top_post[""POST_URL""]
top_post_views = top_post[""VIEWS""]
top_post_likes = top_post[""LIKES""]
top_post_comments = top_post[""COMMENTS""]
top_post_day = top_post[""DAY OF WEEK""]
top_post_text = top_post_text.rstrip()[0:128] + "" (...)""

# preview of what will be send by email:
print(""‚úèÔ∏è Your best post this week got üëÄ x"", top_post_views, ""and üëç x"", top_post_likes)
print(""You posted it on"", top_post_day,""\n"")
print(""This is what it said:\n\n"", top_post_text)
print(""See your post on LinkedIn : "", top_post_url)
sns.set_style(""darkgrid"")
fig, axs = plt.subplots(ncols=2)
fig.tight_layout()
sns.lineplot(x=""DAY OF WEEK"", y=""VIEWS"", data=df_this_week, ax=axs[0])
sns.lineplot(x=""DAY OF WEEK"", y=""LIKES"", data=df_this_week, palette=""deep"", ax=axs[1])

# save image to production as a public asset :
output_image = 'week.png'
fig.savefig(output_image)
link_image = naas.asset.add(output_image)
def get_likes(df_posts):
    df_out = pd.DataFrame()
    for index, row in df_posts.iterrows():
        df = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(row['POST_URL'])
        df_out = df_out.append(df)
        # aditional pause to protect your LinkedIn account 
        time.sleep(random.uniform(3, 6))
    return df_out

# count likes for each profile 
def count_likes(df):
    to_group = [""PROFILE_ID"",
                ""FIRSTNAME"",
                ""LASTNAME""]
    df2 = df.groupby(['PROFILE_ID']).size().sort_values(ascending=False).reset_index(name='LIKE_COUNT')
    return df2
df_likes = get_likes(df_this_week)
df_counts = count_likes(df_likes)

top_fan = df_likes[df_likes[""PROFILE_ID""] == df_counts[""PROFILE_ID""][0]]

fan_slug = top_fan[""PROFILE_ID""].iloc[0]
fan_first_name = top_fan[""FIRSTNAME""].iloc[0]
fan_last_name = top_fan[""LASTNAME""].iloc[0]
fan_name = f""{fan_first_name} {fan_last_name}""  
fan_likes = df_counts[""LIKE_COUNT""][0]
fan_url = f""https://www.linkedin.com/in/{fan_slug}""

# preview of what will be send by email 
print(""‚ù§Ô∏è This week"", fan_name, ""was your biggest fan! They left you"", fan_likes, ""likes"")
print(""Go say hi on their profile:"", fan_url)
# You can edit the basic HTML below to change the look and feel of the email
# but be sure to pay attention to the variable names (indicated by $)
# so you don't unexpectedly break anything

html = """"""\
<h2>Here's your weekly LinkedIn report!</h2>
<h4>üöÄ You did great this week !</h4>

&emsp;üëÄ $VIEWS impressions<br>
&emsp;üëç $LIKES likes<br>
&emsp;üí¨ $COMMENTS comments<br> 
&emsp;‚è© $SHARES shares<br>

<h4>‚ù§Ô∏è This week <i>$FAN_NAME</i> was your biggest fan!</h4>
&emsp;They left you üëç x $FAN_LIKES<br><br>
&emsp;<a href=""$FAN_URL"">Go say hi on their profile</a><br>

<h4>‚úèÔ∏è This week your best post got üëÄ x $BEST_VIEWS and üëç x $BEST_LIKES</h4>
&emsp;You posted it on $BEST_DAY, and this is what it said: <br>
<br>
&emsp;&emsp;<i>$BEST_TEXT</i>
<br><br>
&emsp;<a href=""$BEST_URL"">See your post on LinkedIn</a>

<h4>üìà Here, have some charts (we're out of cookies)</h4>
<img src=""$CHART"" style=""width:640px; height:360px;"" /><br>
""""""
html = html.replace(""$VIEWS"", str(views))
html = html.replace(""$LIKES"", str(likes))
html = html.replace(""$COMMENTS"", str(comments))
html = html.replace(""$SHARES"", str(shares))
html = html.replace(""$FAN_NAME"", str(fan_name))
html = html.replace(""$FAN_LIKES"", str(fan_likes))
html = html.replace(""$FAN_URL"", str(fan_url))
html = html.replace(""$BEST_VIEWS"", str(top_post_views))
html = html.replace(""$BEST_LIKES"", str(top_post_likes))
html = html.replace(""$BEST_DAY"", str(top_post_day))
html = html.replace(""$BEST_TEXT"", str(top_post_text))
html = html.replace(""$BEST_URL"", str(top_post_url))
html = html.replace(""$CHART"", str(link_image))
post = html

# preview final email
print(post)
# sends the email
naas.notification.send(email_to=EMAIL_TO,
                       subject=EMAIL_SUBJECT,
                       html=post,
                       email_from=EMAIL_FROM)"
10323,LinkedIn - Send posts feed to gsheet,"from naas_drivers import linkedin, gsheet
import naas
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# Linkedin profile url
PROFILE_URL = ""https://www.linkedin.com/in/xxxxxx/""

# Number of posts updated in Gsheet (This avoid to requests the entire database)
LIMIT = 10
# Spreadsheet URL
SPREADSHEET_URL = ""https://docs.google.com/spreadsheets/d/XXXXXXXXXXXXXXXXXXXX""

# Sheet name
SHEET_NAME = ""LK_POSTS_FEED""
naas.scheduler.add(cron=""0 8 * * *"")

#-> To delete your scheduler, please uncomment the line below and execute this cell
# naas.scheduler.delete()
df_gsheet = gsheet.connect(SPREADSHEET_URL).get(sheet_name=SHEET_NAME)
df_gsheet
def get_new_posts(df_gsheet, key, limit=LIMIT, sleep=False):
    posts = []
    if len(df_gsheet) > 0:
        posts = df_gsheet[key].unique()
    else:
        df_posts_feed = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL, limit=-1, sleep=sleep)
        return df_posts_feed
    
    # Get new
    df_posts_feed = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL, limit=LIMIT, sleep=sleep)
    df_new = pd.concat([df_posts_feed, df_gsheet]).drop_duplicates(key, keep=""first"")
    return df_new

df_new = get_new_posts(df_gsheet, ""POST_URL"", limit=LIMIT)
df_new
gsheet.connect(SPREADSHEET_URL).send(df_new,
                                     sheet_name=SHEET_NAME,
                                     append=False)"
10324,LinkedIn - Follow company followers,"from naas_drivers import linkedin
import pandas as pd
from datetime import datetime
import naas
import plotly.graph_objects as go
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Company URL
COMPANY_URL = ""https://www.linkedin.com/company/naas-ai/""
# Inputs
csv_input = ""LinkedIn_company_followers.csv""

# Outputs
company_name = COMPANY_URL.strip().split(""company/"")[-1].split(""/"")[0]
title = f""{company_name} : LinkedIn company followers""
name_output = ""LinkedIn_company_followers_trend""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
# Schedule your notebook everyday at 9:00 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
# Get company followers from CSV stored in your local (Returns empty if CSV does not exist)
def get_company_followers(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_followers = get_company_followers(csv_input)
df_followers
def get_new_followers(df, input_path):
    # Get all profiles
    profiles = []
    if len(df) > 0:
        profiles = df.PROFILE_ID.unique()
    start = 0
    while True:
        tmp_df = linkedin.connect(LI_AT, JSESSIONID).company.get_followers(COMPANY_URL,
                                                                           start=start,
                                                                           limit=1,
                                                                           sleep=False)
        profile_id = None
        if ""PROFILE_ID"" in tmp_df.columns:
            profile_id = tmp_df.loc[0, ""PROFILE_ID""]
        if profile_id in profiles:
            break
        else:
            df = pd.concat([tmp_df, df])
            df.to_csv(input_path, index=False)
            start += 1
    return df.reset_index(drop=True)

merged_df = get_new_followers(df_followers, csv_input)
merged_df
def get_trend(df,
              date_col_name=None,
              value_col_name=None,
              date_order='asc'):
    
    # Format date
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq=""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    
    # Calc sum cum
    df[""VALUE_CUM""] = df.agg({value_col_name: ""cumsum""})
    
    df[""TEXT""] = (df['VALUE_CUM'].astype(str) + "" as of "" + df[date_col_name].dt.strftime(""%Y-%m-%d"") +
                  "" (+"" + df[value_col_name].astype(str) + "" vs yesterday)"")
    return df.reset_index(drop=True)

df_trend = get_trend(merged_df, ""FOLLOWED_AT"", ""PROFILE_ID"")
df_trend
def create_linechart(df, label, value, text, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            text=df[text],
            hoverinfo=""text"",
            mode=""lines"",
            stackgroup=""one""
        )
    )
    fig.update_layout(
        title=f""<b>{title}</b><br><span style='font-size: 13px;'>{df.loc[df.index[-1], 'TEXT']}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""FOLLOWED_AT"", ""VALUE_CUM"", ""TEXT"", title)
df_trend.to_csv(csv_output, index=False)
fig.write_html(html_output)
fig.write_image(image_output)
naas.asset.add(csv_output)

#-> to remove your outputs, uncomment the lines and execute the cell
# naas.asset.delete(csv_output)
naas.asset.add(html_output, params={""inline"": True})

#-> to remove your outputs, uncomment the lines and execute the cell
# naas.asset.delete(html_output)
naas.asset.add(image_output, params={""inline"": True})

#-> to remove your outputs, uncomment the lines and execute the cell
# naas.asset.delete(image_output)"
10325,LinkedIn - Get info from company,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
COMPANY_URL = ""https://www.linkedin.com/company/naas-ai"" #or universal name = ""tesla"" or id = ""8819091""
df = linkedin.connect(LI_AT, JSESSIONID).company.get_info(COMPANY_URL)
df"
10326,LinkedIn - Send invitation to profile,"from naas_drivers import linkedin
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Profile URL you want to send the invitation to
RECIPIENT_URL = ""LINKEDIN_URL"" # https://www.linkedin.com/in/****/
result = linkedin.invitation.send(recipient_url=RECIPIENT_URL)
result"
10327,LinkedIn - Get connections from network,"from naas_drivers import linkedin
import pandas as pd
from datetime import datetime
import naas
import plotly.graph_objects as go
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
# Outputs
title = ""LinkedIn - Connections""
name_output = ""LinkedIn_connections""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
df_connections = linkedin.connect(LI_AT, JSESSIONID).network.get_connections(limit=-1)
df_connections
def get_trend(df,
              date_col_name='CREATED_AT',
              value_col_name=""PROFILE_ID"",
              date_order='asc'):
    
    # Format date
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    
    # Calc sum cum
    df[""VALUE_CUM""] = df.agg({value_col_name: ""cumsum""})
    
    df[""TEXT""] = (df['VALUE_CUM'].astype(str) + "" as of "" + df[date_col_name].dt.strftime(""%Y-%m-%d"") +
                  "" (+"" + df[value_col_name].astype(str) + "" vs yesterday)"")
    return df.reset_index(drop=True)

df_trend = get_trend(df_connections)
df_trend
def create_linechart(df, label, value, text, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            text=df[text],
            hoverinfo=""text"",
            mode=""lines"",
#             line=dict(color=""black""),
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=f""<b>{title}</b><br><span style='font-size: 13px;'>{df.loc[df.index[-1], 'TEXT']}</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Date"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title='No. of connections',
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, ""CREATED_AT"", ""VALUE_CUM"", ""TEXT"", title)
df_connections.to_csv(csv_output, index=False)
fig.write_html(html_output)
fig.write_image(image_output)
naas.asset.add(csv_output)
naas.asset.add(html_output, params={""inline"": True})
naas.asset.add(image_output, params={""inline"": True})

#-> to remove your outputs, uncomment the lines and execute the cell
# naas.asset.delete(csv_output)
# naas.asset.delete(html_output)
# naas.asset.delete(image_output)"
10328,LinkedIn - Get invitations sent,"from naas_drivers import linkedin
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""
df_invitation = linkedin.connect(LI_AT, JSESSIONID).invitation.get_sent()
df_invitation"
10329,LinkedIn - Get polls from post,"from naas_drivers import linkedin
import plotly.express as px
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
POST_URL = ""POST_URL""
df = linkedin.connect(LI_AT, JSESSIONID).post.get_polls(POST_URL)
print(""üìù Nb of poll results"", len(df))
df.head(5)
df.to_csv(""POLL.csv"", index=False)
print(""üíæ Poll results saved in csv"")
def create_polls_graph(df):
    poll_id = df.POLL_ID.unique()[0]
    title = df.POLL_QUESTION.unique()[0]
    
    # Create dataframe
    df = df.groupby([""POLL_RESULT""], as_index=False).agg({""PROFILE_ID"": ""count""})
    df[""VALUE""] = df[""PROFILE_ID""] / df[""PROFILE_ID""].sum() * 100
    df[""VALUE_D""] = df[""VALUE""].map('{:.0f}%'.format)
    
    # Count voters
    voters = df.PROFILE_ID.sum()
    
    # Create fig
    fig = px.bar(df,
                 y=""POLL_RESULT"",
                 x=""PROFILE_ID"",
                 orientation='h',
                 title=f""{title}<br><span style='font-size: 13px;'>Total amount of votes: {voters}</span>"",
                 text=""VALUE_D"",
                 labels={
                     ""POLL_RESULT"": ""Options"",
                     ""PROFILE_ID"": ""Nb of votes"",
                     ""VALUE_D"": ""% of votes""
                 },
                 )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        plot_bgcolor=""#ffffff"",
        width=600,
        height=400,
        font=dict(family=""Arial"", size=14, color=""black""),
        paper_bgcolor=""white"",
        xaxis_title=None,
        xaxis_showticklabels=False,
        yaxis_title=None,
        margin_pad=10,
    )
    fig.write_html(f""{poll_id}.html"")
    fig.show()
    asset = naas.asset.add(f""{poll_id}.html"")
    return asset

create_polls_graph(df)"
10330,LinkedIn - Get comments from post,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
POST_URL = ""POST_URL""
df = linkedin.connect(LI_AT, JSESSIONID).post.get_comments(POST_URL)
df"
10331,LinkedIn - Accept invitation received,"from naas_drivers import linkedin
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""
df_invitation = linkedin.connect(LI_AT, JSESSIONID).invitation.get_received()
df_invitation
# Value in column ""INVITATION_ID""
invitation_id = ""691596420000000000""

# Value in column ""SHARED_SECRET""
shared_secret = ""6xubdsfs""

# If ""INVITATION_TYPE"" is ""Profile"" then False else True
is_generic = False
result = linkedin.connect(LI_AT, JSESSIONID).invitation.accept(invitation_id, shared_secret, is_generic)
result"
10332,LinkedIn - Send message to profile from post likes,"from naas_drivers import linkedin
import pandas as pd
import naas
import time
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Post url
POST_URL = ""POST_URL""

# Message
MESSAGE = ""Hi there, thanks for liking my post !""
# Post likes
csv_post_likes = f""LINKEDIN_POST_LIKES_{POST_URL.split('activity-')[1].split('-')[0]}.csv""
df_post_likes = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(POST_URL)
print(""üëç Post likes :"", len(df_post_likes))
df_post_likes.tail(1)
def get_connections(df):
    for index, row in df.iterrows():
        df_network = pd.DataFrame()
        profile = row[""PUBLIC_ID""]
        print(f""‚û°Ô∏è Checking :"", profile)
        
        # Get distance with profile
        if profile != 0:
            df_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile)
            
        # Check if profile is already in your network
        if len(df_network) > 0:
            distance = df_network.loc[0, ""DISTANCE""]
            df.loc[index, ""DISTANCE""] = distance
            df.to_csv(csv_post_likes, index=False)
    return df

df_profile = get_connections(df_post_likes)
df_profile
def send_message(df):
    df = df[df.DISTANCE == ""DISTANCE_1""].reset_index(drop=True)
    for index, row in df.iterrows():
        fullname = row[""FULLNAME""]
        firstname = row[""FIRSTNAME""]
        profile_id = row[""PROFILE_ID""]
        message_custom = MESSAGE.replace(""Hi there,"", f""Hi {firstname},"")
        print(f""‚û°Ô∏è Sending to :"", fullname)
        
        # Get distance with profile
        try:
            linkedin.message.send(content=message_custom, recipients_url=profile_id)
        except Exception as e:
            print(""‚ùå Message not sent"", e)
        time.sleep(3)
    return df

df_message = send_message(df_profile)
df_message"
10333,LinkedIn - Get network from profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
PROFILE_URL = ""PROFILE_URL""
df = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(PROFILE_URL)
df"
10334,LinkedIn - Linkedin Follow number of content published,"from naas_drivers import linkedin
import naas
import pandas as pd
from datetime import datetime
import plotly.graph_objects as go
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""

# Enter profile URL
PROFILE_URL = ""PROFILE_URL""
# Outputs
name_output = ""LinkedIn_Total_content_published""
csv_output = f""{name_output}.csv""
html_output = f""{name_output}.html""
image_output = f""{name_output}.png""
# Schedule your notebook everyday at 9:00 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
# Get posts feed from CSV stored in your local (Returns empty if CSV does not exist)
def get_past_feeds(csv_output):
    try:
        df = pd.read_csv(csv_output)
    except FileNotFoundError as e:
        # Empty dataframe returned
        return pd.DataFrame()
    return df

df_posts_feed = get_past_feeds(csv_output)
df_posts_feed
def get_posts(df):
    # Get last post URL in dataframe
    if len(df) == 0:
        last_post_url = None
    else:
        last_post_url = df.POST_URL[0]
    # Get new posts since last url (this part is important to optimize script performance)
    until = {}
    if last_post_url:
        until = {""POST_URL"": last_post_url}

    df_posts_feed = linkedin.connect(LI_AT, JSESSIONID).profile.get_posts_feed(PROFILE_URL, until=until, limit=-1)

    # Merge dataframe
    merge_df = df.append(df_posts_feed, ignore_index=False)
    merge_df.sort_values(by = 'PUBLISHED_DATE', ascending = False, inplace=True)
    
    # Keeps/updates the latest views count value for that day
    merge_df.drop_duplicates('POST_URL', keep = 'last', inplace=True)
    
    return merge_df

merged_df = get_posts(df_posts_feed)
merged_df
# Create dataframe with number of LinkedIn views cumulated by date with daily variation
# -> Empty date must be fullfiled with last value

def get_trend(posts_df):
    
    # taking activity_id instead of content_id as the
    # content_id seems to be None for some type of content.
    
    df = posts_df.copy()
    date_col_name='PUBLISHED_DATE'
    value_col_name=""ACTIVITY_ID""
    date_order='asc'
    
    # Format date
    for idx, item in enumerate(df['PUBLISHED_DATE']):
        df.loc[idx, 'PUBLISHED_DATE'] = item.split('+')[0]
    
    df[date_col_name] = pd.to_datetime(df[date_col_name]).dt.strftime(""%Y-%m-%d"")
    df = df.groupby(date_col_name, as_index=False).agg({value_col_name: ""count""})
    d = datetime.now().date()
    d2 = df.loc[df.index[0], date_col_name]
    idx = pd.date_range(d2, d, freq = ""D"")
    
    df.set_index(date_col_name, drop=True, inplace=True)
    df.index = pd.DatetimeIndex(df.index)
    df = df.reindex(idx, fill_value=0)
    df[date_col_name] = pd.DatetimeIndex(df.index)
    
    # Calc sum cum
    df[""value_cum""] = df.agg({value_col_name: ""cumsum""})
    df.drop(columns='ACTIVITY_ID', inplace=True)
    return df.reset_index(drop=True)

df_trend = get_trend(merged_df)
df_trend
def create_linechart(df, label, value, title):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            mode=""lines"",
        )
    )
    fig.update_traces(marker_color='black')
    fig.update_layout(
        title=title,
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df_trend, label=""PUBLISHED_DATE"", value=""value_cum"", title='Total Content Published')
# Save your dataframe in CSV
merged_df.to_csv(csv_output, index=False)

# Share output with naas
naas.asset.add(csv_output)

#-> Uncomment the line below to remove your asset
# naas.asset.delete(csv_output)
# Save your graph in HTML
fig.write_html(html_output)

# Share output with naas
naas.asset.add(html_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(html_output)
# Save your graph in PNG
fig.write_image(image_output)

# Share output with naas
naas.asset.add(image_output, params={""inline"": True})

#-> Uncomment the line below to remove your asset
# naas.asset.delete(image_output)
"
10335,LinkedIn - Get messages from profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
CONVERSATION_URL = ""CONVERSATION_URL""
df = linkedin.connect(LI_AT, JSESSIONID).message.get_messages(CONVERSATION_URL)
df"
10336,LinkedIn - Send company followers to Google Sheets,"from naas_drivers import linkedin, gsheet
import pandas as pd
import naas
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Company URL
COMPANY_URL = ""https://www.linkedin.com/company/naas-ai/""
# Enter spreadsheet URL and sheet name
SPREADSHEET_URL = ""https://docs.google.com/spreadsheets/d/1SoF6qIOeAKStOIx9FrMhcElN7XuiKiqPutZy823BgsY/edit#gid=0""
SHEET_NAME = ""NAAS_FOLLOWERS""
# Schedule your notebook everyday at 9:00 AM
naas.scheduler.add(cron=""0 9 * * *"")

#-> Uncomment the line below to remove your scheduler
# naas.scheduler.delete()
df_gsheet = gsheet.connect(SPREADSHEET_URL).get(SHEET_NAME)
df_gsheet
def get_new_followers(df):
    # Get all profiles
    profiles = []
    if len(df) > 0:
        profiles = df.PROFILE_ID.unique()
    start = 0
    while True:
        tmp_df = linkedin.connect(LI_AT, JSESSIONID).company.get_followers(COMPANY_URL,
                                                                           start=start,
                                                                           limit=1,
                                                                           sleep=False)
        profile_id = None
        if ""PROFILE_ID"" in tmp_df.columns:
            profile_id = tmp_df.loc[0, ""PROFILE_ID""]
        if profile_id in profiles:
            break
        else:
            df = pd.concat([tmp_df, df])
            start += 1
    return df.reset_index(drop=True)

df_followers = get_new_followers(df_gsheet)
df_followers
gsheet.connect(SPREADSHEET_URL).send(data=df_followers,
                                     sheet_name=SHEET_NAME,
                                     append=False)
"
10337,LinkedIn - Send invitation to profile from post likes,"from naas_drivers import linkedin
import pandas as pd
import naas
import time
# Credentials
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585

# Post url
POST_URL = ""POST_URL""
# Post likes
csv_post_likes = f""LINKEDIN_POST_LIKES_{POST_URL.split('activity-')[1].split('-')[0]}.csv""
df_post_likes = linkedin.connect(LI_AT, JSESSIONID).post.get_likes(POST_URL)
print(""üëç Post likes :"", len(df_post_likes))
df_post_likes.tail(1)
def get_connections(df):
    for index, row in df.iterrows():
        df_network = pd.DataFrame()
        profile = row[""PUBLIC_ID""]
        print(f""‚û°Ô∏è Checking :"", profile)
        
        # Get distance with profile
        if profile != 0:
            df_network = linkedin.connect(LI_AT, JSESSIONID).profile.get_network(profile)
            
        # Check if profile is already in your network
        if len(df_network) > 0:
            distance = df_network.loc[0, ""DISTANCE""]
            df.loc[index, ""DISTANCE""] = distance
            df.to_csv(csv_post_likes, index=False)
    return df

df_profile = get_connections(df_post_likes)
df_profile
def send_invitation(df):
    df = df[~df.DISTANCE.isin([""SELF"", ""DISTANCE_1""])].reset_index(drop=True)
    for index, row in df.iterrows():
        fullname = row[""FULLNAME""]
        profile_id = row[""PROFILE_ID""]
        print(f""‚û°Ô∏è Sending to :"", fullname)
        
        # Get distance with profile
        try:
            linkedin.invitation.send(recipient_url=profile_id)
        except Exception as e:
            print(""‚ùå Invitation not sent"", e)
        time.sleep(3)
    return df

df_invitation = send_invitation(df_profile)
df_invitation"
10338,LinkedIn - Get guests from event,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
EVENT_URL = ""https://www.linkedin.com/events/6762355783188525056/""
df = linkedin.connect(LI_AT, JSESSIONID).event.get_guests(EVENT_URL)
df"
10339,LinkedIn - Get resume from profile,"from naas_drivers import linkedin
LI_AT = 'YOUR_COOKIE_LI_AT'  # EXAMPLE AQFAzQN_PLPR4wAAAXc-FCKmgiMit5FLdY1af3-2
JSESSIONID = 'YOUR_COOKIE_JSESSIONID'  # EXAMPLE ajax:8379907400220387585
PROFILE_URL = ""PROFILE_URL""
df = linkedin.connect(LI_AT, JSESSIONID).profile.get_resume(PROFILE_URL)
df"
10340,LinkedIn - Get invitations received,"from naas_drivers import linkedin
import pandas as pd
# Lindekin cookies
LI_AT = ""AQEDARCNSioDe6wmAAABfqF-HR4AAAF-xYqhHlYAtSu7EZZEpFer0UZF-GLuz2DNSz4asOOyCRxPGFjenv37irMObYYgxxxxxxx""
JSESSIONID = ""ajax:12XXXXXXXXXXXXXXXXX""
df_invitation = linkedin.connect(LI_AT, JSESSIONID).invitation.get_received()
df_invitation
"
10341,Remoteok - Post daily jobs on slack,"import pandas as pd
import requests
from datetime import datetime
import time
from naas_drivers import gsheet, slack
import naas
SLACK_TOKEN = ""xoxb-1481042297777-3085654341191-xxxxxxxxxxxxxxxxxxxxxxxxx""
SLACK_CHANNEL = ""05_jobs""
spreadsheet_id = ""1EBefhkbmqaXMZLRCiafabf6xxxxxxxxxxxxxxxxxxx""
sheet_name = ""REMOTEOK_POSTS""
categories = ['machine learning',
              'data science',
              'nlp',
              'deep learning',
              'computer vision',
              'data',
              'natural language processing',
              'data engineer']
date_from  = -10 ### this is 10 days from now => must be negative
naas.scheduler.add(recurrence=""0 9 * * *"")
# # naas.scheduler.delete() # Uncomment this line to delete your scheduler if needed
try:
    df_jobs_log = gsheet.connect(spreadsheet_id).get(sheet_name=sheet_name)
except KeyError as e:
    print('Gsheet is empty!!')
    df_jobs_log = pd.DataFrame()
REMOTEOK_API = ""https://remoteok.com/api""
REMOTEOK_DATETIME = ""%Y-%m-%dT%H:%M:%S""
NAAS_DATETIME = ""%Y-%m-%d %H:%M:%S""

def get_jobs(remoteok_url, categories):
    df = pd.DataFrame()
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
        }
    index=0
    for tag in categories:
        url = remoteok_url + f""?tag={tag}""
        res = requests.get(url, headers=headers)
        try:
            res.raise_for_status()
        except requests.HTTPError as e:
            return e
        
        job_details = res.json()
        
        if len(job_details)==1:
            continue
        else:
            for idx, job in enumerate(job_details):
                if idx!=0:
                    date = job['date'].split('+')[0]
                    publication_time = datetime.strptime(date, REMOTEOK_DATETIME).timestamp()
                    required_time = time.time() + date_from* 24 * 60 * 60  ### time in seconds
                    
                    if publication_time >= required_time:
                        df.loc[index, 'URL'] = job.get('url')
                        df.loc[index, 'TITLE'] = job.get('position')
                        df.loc[index, 'COMPANY'] = job.get('company')
                        df.loc[index, 'TAGS'] = "", "".join(job.get('tags'))
                        df.loc[index, 'LOCATION'] = job.get('location')
                        df.loc[index, 'PUBLICATION_DATE'] = datetime.fromtimestamp(publication_time).strftime(NAAS_DATETIME)
                        index+=1
    
    df = df.drop_duplicates(subset = 'URL', keep='first')
    df = df.sort_values(by='PUBLICATION_DATE', ascending=False)
    return df

df_jobs = get_jobs(REMOTEOK_API, categories)
df_jobs.head()
def remove_duplicates(df1, df2):
    # Get jobs log
    jobs_log = df1.URL.unique()
    
    # Exclude jobs already log from jobs
    df2 = df2[~df2.URL.isin(jobs_log)]
    return df2.sort_values(by=""PUBLICATION_DATE"")

df_new_jobs = remove_duplicates(df_jobs_log, df_jobs)
df_new_jobs
gsheet.connect(spreadsheet_id).send(sheet_name=sheet_name,
                                    data=df_new_jobs,
                                    append=True)
if len(df_new_jobs) > 0:
    for _, row in df_new_jobs.iterrows():
        url = row.URL
        slack.connect(SLACK_TOKEN).send(SLACK_CHANNEL, f""<{url}>"")
else:
    print(""Nothing to be published in Slack !"")"
10342,Remoteok - Get jobs from categories,"import pandas as pd
import requests
from datetime import datetime
import time
categories = ['machine learning',
              'data science',
              'nlp',
              'deep learning',
              'computer vision',
              'data',
              'natural language processing',
              'data engineer']
date_from  = -30 ### this is 30 days from now => must be negative
csv_output = ""REMOTIVE_JOBS.csv""
REMOTEOK_API = ""https://remoteok.com/api""
REMOTEOK_DATETIME = ""%Y-%m-%dT%H:%M:%S""
NAAS_DATETIME = ""%Y-%m-%d %H:%M:%S""

def get_jobs(remoteok_url, categories):
    df = pd.DataFrame()
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',
        }
    index=0
    for tag in categories:
        url = remoteok_url + f""?tag={tag}""
        res = requests.get(url, headers=headers)
        try:
            res.raise_for_status()
        except requests.HTTPError as e:
            return e
        
        job_details = res.json()
        
        if len(job_details)==1:
            continue
        else:
            for idx, job in enumerate(job_details):
                if idx!=0:
                    date = job['date'].split('+')[0]
                    publication_time = datetime.strptime(date, REMOTEOK_DATETIME).timestamp()
                    required_time = time.time() + date_from* 24 * 60 * 60  ### time in seconds
                    
                    if publication_time >= required_time:
                        df.loc[index, 'URL'] = job.get('url')
                        df.loc[index, 'TITLE'] = job.get('position')
                        df.loc[index, 'COMPANY'] = job.get('company')
                        df.loc[index, 'TAGS'] = "", "".join(job.get('tags'))
                        df.loc[index, 'LOCATION'] = job.get('location')
                        df.loc[index, 'PUBLICATION_DATE'] = datetime.fromtimestamp(publication_time).strftime(NAAS_DATETIME)
                        index+=1
                        
    df = df.sort_values(by='PUBLICATION_DATE', ascending=False)
    return df

df_jobs = get_jobs(REMOTEOK_API, categories)
df_jobs.head(5)
df_jobs.to_csv(csv_output, index=False)"
10343,Twilio - Send SMS,"try:
    from twilio.rest import Client
except:
    ! pip install --user twilio
    from twilio.rest import Client
# Credentials
account_sid = """"    # From Twilio Console
auth_token = """"     # From Twilio Console

# Message
from_number = """"    # Buy a new number if not available preferably starting with ""+1"" country code
to_number = """"      # Could send messages only to verified Caller ID's
message = """"        # Message to be Sent
client = Client(account_sid, auth_token)  # Set Client
client.api.account.messages.create(to=to_number,
                                   from_=from_number,
                                   body=message)"
10344,Stripe - Get charges,"try:
    import stripe
except:
    !pip install stripe
    import stripe
import pandas as pd
api_key = ""sk_test_4eC39HqLyjWDarjtT1zdp7dc""
stripe.api_key = api_key
charges = stripe.Charge.list(limit=30)
pd.DataFrame(charges.get('data'))"
10345,Stripe - Get balances,"try:
    import stripe
except:
    !pip install stripe
    import stripe
import pandas as pd
api_key = ""sk_test_4eC39HqLyjWDarjtT1zdp7dc""
stripe.api_key = api_key
balance = stripe.BalanceTransaction.list(limit=30)
pd.DataFrame(balance.get('data'))"
10346,Stripe - Get customers,"try:
    import stripe
except:
    !pip install stripe
    import stripe
import pandas as pd
api_key = ""sk_test_4eC39HqLyjWDarjtT1zdp7dc""
stripe.api_key = api_key
customers = stripe.Customer.list(limit=30)
pd.DataFrame(customers.get('data'))"
10347,Societe.com - Get verif.com,"import pandas as pd
data = pd.read_html(""https://www.verif.com/societe/CASHSTORY-880612569/"", encoding=""UTF-8"")
print(len(data))
data[0]"
10348,Societe.com - Get company details,"import pandas as pd
data = pd.read_html(""https://www.societe.com/societe/cashstory-880612569.html"")
data[0]
"
10349,NASA - Artic sea ice,"import pandas as pd
import plotly.graph_objects as go
import naas
# Input
data_input = ""https://climate.nasa.gov/system/internal_resources/details/original/2485_Sept_Arctic_extent_1979-2021.xlsx""

# Output
image_output = ""Artic_sea_ice_extent.png""
html_output = ""Artic_sea_ice_extent.png""
df = pd.read_excel(data_input)
df.tail(10)
def create_linechart(df, label, value):
    # Init
    fig = go.Figure()
    
    # Create fig
    fig.add_trace(
        go.Scatter(
            x=df[label],
            y=df[value],
            hoverinfo=""text"",
            hovertext=df[label].astype(str) + ""<br>"" + df[value].astype(str) + "" million square km"",
            mode=""lines+text"",
        )
    )
    fig.update_layout(
        title=f""<b>Arctic Sea Ice Extent</b><br><span style='font-size: 13px;'>ANNUAL SEPTEMBER MINIMUM EXTENT</span>"",
        title_font=dict(family=""Arial"", size=18, color=""black""),
        plot_bgcolor=""#ffffff"",
        width=1200,
        height=800,
        paper_bgcolor=""white"",
        xaxis_title=""Year"",
        xaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        yaxis_title='million square km',
        yaxis_title_font=dict(family=""Arial"", size=11, color=""black""),
        margin_pad=10,
    )
    fig.show()
    return fig

fig = create_linechart(df, ""year"", ""extent"")
fig.write_html(html_output)
fig.write_image(image_output)
naas.asset.add(html_output, params={""inline"": True})
naas.asset.add(image_output)

#-> Uncomment lines below to remove your assets
# naas.asset.delete(html_output)
# naas.asset.delete(image_output)"
10350,NASA - Global temperature,"import pandas
import plotly.graph_objects as go
url_nasa_termperatures = ""https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt""
df = pandas.read_csv(url_nasa_termperatures, sep=r'     ', skiprows=5, names=[""Year"", ""Delta"", ""Detla (Smoothed)""], engine=""python"")

df.tail(10)
fig = go.Figure(layout_title=""<b>Land-Ocean Temperature Index (¬∞C)</b>"")
fig.add_trace(go.Scatter(
    x = df[""Year""],
    y = df[""Delta""],
    name=""Delta"",
))

fig.add_trace(go.Scatter(
    x = df[""Year""],
    y = df[""Detla (Smoothed)""],
    name=""Delta (Smoothed)"", 
))

fig.update_layout(
    autosize=False,
    width=1300,
    height=700,
    plot_bgcolor='rgb(250,250,250)',
    xaxis = dict(
    tickmode = 'linear',
    tick0 = 2,
    dtick = 5,
    ),
)
fig.update_yaxes(title_text=""Temperature anomaly (¬∞C)"")
fig.update_xaxes(title_text=""Year"", tickangle=60)
fig.add_hline(y=0.0)
# To add maximum Temperature Delta line : uncomment the following line
# fig.add_hline(y=2.0)
fig.update_layout(title_x=0.5)

fig.show()
fig.add_vrect(x0=""1910"", x1=""1911"", annotation_text=""2nd IR <br> 1910"", annotation_position=""top left"",
annotation=dict(font_size=20, font_family=""Times New Roman""),
fillcolor=""black"", opacity=0.45, line_width=0)

fig.add_vrect(x0=""1970"", x1=""1971"", annotation_text=""3rd IR <br> 1970"", annotation_position=""top left"",
annotation=dict(font_size=20, font_family=""Times New Roman""),
fillcolor=""yellow"", opacity=0.45, line_width=0)

fig.add_vrect(x0=""2000"", x1=""2001"", annotation_text=""4th IR <br> 2000"", annotation_position=""top left"",
annotation=dict(font_size=20, font_family=""Times New Roman""),
fillcolor=""green"", opacity=0.45, line_width=0)

fig.update_layout(title_x=0.5, title_text=""<b>Land-Ocean Temperature Index (¬∞C)</b> <br> Focused on Industrial Revolution Dates (IR)"")

fig.show()"
10351,NASA - Sea level,"import pandas
import plotly.graph_objects as go
uri_nasa_sea_level = ""nasa-sea-level-data.txt""
df = pandas.read_csv(uri_nasa_sea_level, engine=""python"", comment='HDR',delim_whitespace=True, names=[""A"",""B"",""Year + Fraction"",""D"",""E"",""F"",""G"", ""H"",""I"",""J"",""K"",""Smoothed GMSL (mm)"",])

df.head(10)
new_df = pandas.DataFrame(df, columns=['Year + Fraction', 'Smoothed GMSL (mm)'])

dates = []
values = []

ref = 0

for i, row in new_df.iterrows():
    #date formating
    date_split = str(row['Year + Fraction']).split('.')
    year = date_split[0]
    fraction = '0.' + date_split[1]
    float_fraction = float(fraction)
    date = year + ""-1-1""
    date_delta = 365 * float_fraction
    value = pandas.to_datetime(date) + pandas.to_timedelta(date_delta, unit='D')
    dates.append(value)
    
    #value formating
    #to stay inline with the graph visible on nasa's website, we need to have 0 as our first value
    if i == 0:
       ref = row['Smoothed GMSL (mm)'] 
    
    val = row['Smoothed GMSL (mm)'] - ref 
    values.append(val)
    
    
    

new_df['Date'] = dates
new_df['Value'] = values
    
new_df.head()
fig = go.Figure(layout_title=""<b>Sea Level variation since 1993 (mm)</b>"")
fig.add_trace(go.Scatter(
    x = new_df[""Date""],
    y = new_df[""Value""],
    name=""Delta"",
))

fig.update_layout(
    autosize=False,
    width=1300,
    height=700,
    plot_bgcolor='rgb(250,250,250)',
)


fig.add_annotation(y=6, x='2020-1-1',
            text=""Data source: Satellite sea level observations.<br> Credit: NASA's Goddard Space Flight Center"",
            showarrow=False,
            )

fig.update_yaxes(title_text=""Sea Height Variation (mm)"")
fig.update_xaxes(title_text=""Year"", tickangle=60)
fig.add_hline(y=0.0)
fig.update_layout(title_x=0.5)

fig.show()"
